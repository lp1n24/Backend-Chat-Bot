source,page,type,text
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,1,page,"How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings Shuaichen Chang and Eric Fosler-Lussier The Ohio State University {chang.1692, fosler-lussier.1}@osu.edu Abstract Large language models (LLMs) with in-context learning have demonstrated remarkable ca- pability in the text-to-SQL task. Previous research has prompted LLMs with various demonstration-retrieval strategies and interme- diate reasoning steps to enhance the perfor- mance of LLMs. However, those works of- ten employ varied strategies when constructing the prompt text for text-to-SQL inputs, such as databases and demonstration examples. This leads to a lack of comparability in both the prompt constructions and their primary contri- butions. Furthermore, selecting an effective prompt construction has emerged as a persis- tent problem for future research. To address this limitation, we comprehensively investigate the impact of prompt constructions across var- ious settings and provide insights into prompt constructions for future text-to-SQL studies. 1 1 Introduction Text-to-SQL models enable users to query databases using natural language questions (NLQs) without having to develop the underlying SQL query. Over the past few decades, neural models with supervised learning have achieved impressive performance on the text-to-SQL task, which are usually trained on a large training set and then eval- uated on test examples (Wang et al., 2019; Yu et al., 2021; Rubin and Berant, 2021; Scholak et al., 2021; Gan et al., 2021; Li et al., 2023a). Recently, large language models (LLMs) have demonstrated strong capabilities for in-context learning on many language understanding and gen- eration tasks (Brown et al., 2020; Chen et al., 2021a; Chowdhery et al., 2022), including on the text-to-SQL task (Rajkumar et al., 2022; Chang et al., 2023; Liu et al., 2023). Instead of train- ing a text-to-SQL model on a large training set, 1The code for the paper is available at https://github. com/shuaichenchang/prompt-text-to-sql . Database CREATE TABLE Highschooler ( ID int primary key , name text , grade int ); /* 3 example rows : SELECT * FROM Highschooler LIMIT 3; ID name grade 1510 Jordan 9 1689 Gabriel 9 1381 Tiffany 9 */ Task Instruction -- Using valid SQLite , answer the following questions for the tables provided above . Demonstration Question : What is Kyle 's id? SELECT ID FROM Highschooler WHERE name = "" Kyle ""; Test Question Question : How many high schoolers are there ? SELECT Figure 1: An example of prompt text for 1-shot single- domain text-to-SQL using a snippet of the database Network_1 with a question from the Spider dataset (Yu et al., 2018). in-context learning allows LLMs to convert a test NLQ into a SQL query using a prompt text. This prompt text includes essential components such as the test database and question. These are accompanied by zero or a few demonstrations: NLQ-SQL pairs corresponding to either the test database (single-domain) or different databases (cross-domain). Figure 1 provides an example of a prompt text for a one-shot single-domain task. Previous research has augmented the text-to- SQL capability of LLMs with demonstration- retrieval strategies (Poesia et al., 2022; Shi et al., 2022), intermediate reasoning steps (Cheng et al., 2022; Chen et al., 2023; Pourreza and Rafiei, 2023), and self-debugging ability (Chen et al., 2023; Pour- reza and Rafiei, 2023). However, those studies of- ten employ different prompt strategies that include various key components of text-to-SQL: database arXiv:2305.11853v3 [cs.CL] 27 Nov 2023"
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,71,caption,Figure 1: An example of prompt text for 1-shot single-
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,2,page,"schema and content, and demonstration examples. The difference in prompt constructions makes it difficult to directly compare two studies on their main contribution, and the outcomes of different studies may change based on future revelations in prompt engineering. In this paper, we evaluate various strategies for prompt construction in three commonly employed text-to-SQL settings: zero-shot, single-domain, and cross-domain. We assess LLMs on text-to- SQL, considering various database prompt con- structions in all three settings. Additionally, in the cross-domain scenario, we investigate the strategy for constructing demonstrations. Through our eval- uation, we aim to gain insights into the effective- ness of these prompt construction strategies. Our findings can be summarized as follows: • Table relationship and table content play a crucial role in effectively prompting LLMs. However, it is essential to carefully consider their repre- sentation in the prompt, as LLMs are sensitive to the specific presentation in the zero-shot and cross-domain settings. • In-domain demonstration examples can mitigate LLMs’ sensitivity to different representations of database knowledge but they cannot replace table content knowledge. • The length of the prompt has a significant impact on the LLMs’ performance in the cross-domain setting. We discovered a preferred prompt length that leads to improved performance. 2 In-context Learning for Text-to-SQL In the text-to-SQL task, a database and a natural language question (NLQ) are provided as input for generating an output SQL query. Traditional supervised learning approaches train models on specific text-to-SQL datasets. However, in-context learning allows pretrained large language models (LLMs) to perform text-to-SQL by providing either zero or a few training examples (NLQ-SQL pairs) as demonstrations. This section introduces three widely used settings for in-context learning in text- to-SQL. Prompt examples in these settings can be found in Appendix A.1. Zero-shot Text-to-SQL This setting evaluates the text-to-SQL capability of pretrained LLMs to directly infer the NLQ-SQL relationship from a table without any demonstration examples. The input includes a task instruction and a test question with its corresponding database. Zero-shot text- to-SQL is used to directly assess the text-to-SQL capability of LLMs (Rajkumar et al., 2022; Chang et al., 2023; Liu et al., 2023). Single-domain Few-shot Text-to-SQL This set- ting is designed for applications or domains where it is easy to construct examples, such as booking flights (Price, 1990; Dahl et al., 1994) and querying geographic information (Zelle and Mooney, 1996). It tests the ability of LLMs to adapt with a few in-domain demonstration examples, which are col- lected from the same database as the test question. The goal is to evaluate how well the LLMs can per- form text-to-SQL with minimal in-domain training data (Rajkumar et al., 2022). Cross-domain Few-shot Text-to-SQL This set- ting evaluates the generalization capability of mod- els to new domains by learning from out-of-domain demonstrations. In this scenario, the demonstra- tion NLQ-SQL pairs correspond to one or multiple demonstration databases that are different from the test database. Cross-domain few-shot text-to-SQL assesses how well LLMs can apply their learned knowledge from demonstrations to new databases (Poesia et al., 2022; Chen et al., 2023). 3 Prompt Construction A text-to-SQL prompt typically comprises four components: a task instruction, a test database, a test NLQ, and optional demonstrations, as il- lustrated in Figure 1. While the task instruction and test NLQ are easily presented in natural lan- guage, there are various strategies for representing the databases and incorporating demonstrations. In this section, we explore different prompt construc- tions for databases and demonstrations. 3.1 Database Prompt A relational database consists of the database schema and database content. The database schema encompasses the schemas (headers) of tables and the relationship among tables, and database content refers to the data stored in the tables. Database Schema Figure 2 illustrates various prompt constructions for the database schema that have been utilized in previous studies: (1) Table(Columns) (Liu et al., 2023) lists each table along with its columns inside parentheses to repre- sent the table schemas; (2) Columns=[] (Pourreza and Rafiei, 2023) represents each table along with"
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,3,page,"Table(Columns) (Liu et al., 2023) Highschooler (ID , name , grade ); Friend ( student_id , friend_id ); Columns=[] (Pourreza and Rafiei, 2023) Table Highschooler , Columns = [ID , name , grade ]; Table Friend , Columns = [ student_id , friend_id ]; +FK (Pourreza and Rafiei, 2023) Foreign_keys = [ Friend . student_id = Highschooler .ID , Friend . friend_id = Highschooler .ID ]; CreateTable (Rajkumar et al., 2022) CREATE TABLE Highschooler ( ID int primary key , name text , grade int ); CREATE TABLE Friend ( student_id int , friend_id int , primary key ( student_id , friend_id ), foreign key ( student_id ) references Highschooler (ID), foreign key ( friend_id ) references Highschooler (ID) ); Figure 2: Examples of the different database schema constructions for a snippet of database Network_1 in Spider. a list of its columns using an equation-like notation; (3) +ForeignKey (Pourreza and Rafiei, 2023) fur- ther adds foreign keys to indicate the relationships between tables; (4) CreateTable (Rajkumar et al., 2022) employed the “Create Table” statement to display the table schemas and relationships. To ensure consistency in the prompt text and accommodate the case-insensitivity of SQL key- words and the database schema, we unify the space and line break in the prompt text and convert all words to lowercase, except for the database content. This normalization process helps to standardize the prompt text. An example is shown in Figure 4. Database content Previous research shows that being aware of database content can improve model performance by exposing models to the specific format of values in each column (Wang et al., 2019; Lin et al., 2020; Scholak et al., 2021; Rajkumar et al., 2022). For instance, the phrase “American student” could be converted to “ WHERE country = ‘USA’” or “ WHERE country = ‘The United States of America’” depending on the contents of the country column. Figure 3 summarizes different approaches used to construct prompts for showcasing the content of a database. (1) InsertRow (Chen et al., 2023): InsertRow (Chen et al., 2023) INSERT INTO Highschooler (ID , name , grade ) VALUES (1510 , "" Jordan "", 9); INSERT INTO Highschooler (ID , name , grade ) VALUES (1689 , "" Gabriel "", 9); INSERT INTO Highschooler (ID , name , grade ) VALUES (1381 , "" Tiffany "", 9); SelectRow (Rajkumar et al., 2022) /* 3 example rows : SELECT * FROM Highschooler LIMIT 3; ID name grade 1510 Jordan 9 1689 Gabriel 9 1381 Tiffany 9 */ SelectCol (Ours) /* Columns in Highschooler and 3 distinct examples in each column : ID: 1025 , 1101 , 1247 name : "" Jordan "", "" Gabriel "", "" Tiffany "" grade : 9, 10 , 11 */ Figure 3: Examples of the different database content constructions for showing 3 cell values in each column for the Highschool table in Figure 2. This method displaysR rows of each table by utiliz- ing R “INSERT INTO” statements. (2) SelectRow (Rajkumar et al., 2022): This approach employs the “SELECT * FROM Table LIMIT R” query to display the first R rows of each table. (3) SelectCol: In- stead of presenting table content in a row-wise man- ner, an alternative method is to use a column-wise format. As there may be duplicated content across different rows, presenting the content column-wise ensures the provision of distinct values within each column to expose LLMs to a broader range of content. We propose using the query “ SELECT DISTINCT [Column] FROM [Table] LIMIT R ” to list R distinct cell values in each column. 3.2 Demonstration Prompt In few-shot settings, LLMs are provided with demonstrations within the prompt text. In the single-domain few-shot setting, we incorporate a few pairs of NLQs and SQLs as demonstrations inserted between the test database and question, following previous work (Rajkumar et al., 2022). In the cross-domain few-shot setting, we use both out-of-domain NLQ-SQL pairs (demonstration ex- amples) and corresponding databases (demonstra- tion databases) placed before the test database and question. Prior research in the N-shot setting either uses one demonstration database with N examples (Pourreza and Rafiei, 2023) or employs N demon- stration databases, each with a single NLQ-SQL"
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,28,caption,Figure 2: Examples of the different database schema
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,54,caption,Figure 3 summarizes different approaches used
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,81,caption,Figure 3: Examples of the different database content
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,4,page,"Unnormalized database and SQL -- Database Schema CREATE TABLE Highschooler ( ID int primary key , name text , grade int ); -- SQL Query SELECT count ( * ) FROM Highschooler WHERE Name = "" Kyle ""; Normalized database and SQL -- Database Schema create table highschooler ( id int primary key , name text , grade int ); -- SQL Query select count (*) from highschooler where name = 'Kyle '; Figure 4: An example of the normalization for database and SQL prompts. pair (Poesia et al., 2022; Chen et al., 2023). In con- trast, we consider a more general scenario where the demonstrations comprise M databases, each with K NLQ-SQL pairs, with M × K = N. We list the examples of 4-shot single-domain and cross- domain demonstrations in Appendix A.1. Additionally, we normalize demonstration SQL queries by first parsing the SQL queries and unify- ing their format, such as using lowercase for SQL keywords and database schema and unifying the space around punctuation. Figure 4 provides an example of SQL normalization. 4 Experiments Data & Evaluation For our experiments, we uti- lize the Spider dataset (Yu et al., 2018), a cross- domain benchmark for the text-to-SQL task. We conduct our experiments on the development set of Spider (Spider-dev) as the test set is not publicly available. Spider-dev consists of 20 databases with 1034 pairs of NLQ and SQL in total. We evaluate models with execution accuracy (EX) which com- pares the execution results of a predicted SQL and a gold SQL. In the cross-domain setting, we use the train- ing set of Spider to select demonstration exam- ples. As a few databases contain long schema that may cause the prompt to exceed the token limits of LLMs, we only use the databases with fewer than 1000 tokens when constructing the CreateTable prompt. This results in a total of 130 databases being used as demonstration databases in the cross- domain setting. Models We used GPT-3 Codex (Chen et al., 2021a) and ChatGPT due to their demonstrated performance and prevalence in the field.2 Experiment Setup For the zero-shot setting, we construct each prompt text with a task instruction, a test database, and a test question. We include R = 3 table rows in the database prompt, which has been discovered as the optimal number in previ- ous work (Rajkumar et al., 2022). For the few-shot settings, we incorporate N demonstration exam- ples in addition to the zero-shot prompt text. In the single-domain text-to-SQL scenario, we use a leave-one-out split, as some databases in Spider-dev contain a small number of examples. When evaluating one example, we regard all other examples from the same database as the training set and randomly retrieve N examples from them. Since Spider contains multiple NLQs correspond- ing to the same SQL query, we require that the train- ing set does not contain examples with the same SQL template as the test example, again following previous work (Finegan-Dollak et al., 2018). In the cross-domain scenario, we randomly se- lect M demonstration databases, each with K NLQ-SQL pairs (M × K = N) from the Spider training set. Incorporating multiple demonstration databases in a prompt text significantly increases its length. Hence, we only use Codex for the cross- domain experiments, due to its higher token limit of 8K, surpassing the 4K limit of ChatGPT. In both single-domain and cross-domain settings, we com- pare different prompt construction methods using the same few-shot examples to make a fair compar- ison. We repeat our experiments three times and present the average results. 5 Results In this section, we present our empirical findings in the areas of zero-shot, single-domain, and cross- domain text-to-SQL. Through our experiments, we aim to answer a few crucial research questions in each setting and provide insightful strategies for future studies on effective prompting. 5.1 Zero-shot Text-to-SQL In the zero-shot setting, we focus on comparing different prompt constructions for databases. Table 2We employ the Code-davinci-002 version of Codex across all settings. In zero-shot and single-domain setups, we uti- lize the gpt-3.5-turbo-0301 version of ChatGPT. For cross- domain experiments involving ChatGPT-16K, we turned to gpt-3.5-turbo-16k-0613 due to its extended context length."
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,20,caption,Figure 4: An example of the normalization for database
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,5,page,"Codex ChatGPT Database Prompt Construction # Tokens (U|N) EX (U|N) # Tokens (U|N) EX (U|N) Table Schema Table(Columns) 148 | 147 69.0 | 71.9 118 | 115 68.8 | 70.5 Columns=[] 169 | 167 70.2 | 71.8 137 | 135 68.3 | 69.1 +Relationship Columns=[]+ForeignKey 226 | 223 72.3 | 73.1 178 | 174 72.9 | 71.2 CreateTable 474 | 356 71.8 | 73.1 339 | 254 70.7 | 71.7 +Relationship+Content CreateTable+InsertRow 3 1089 | 1013 70.9 | 71.9 964 | 872 71.8 | 71.8 CreateTable+SelectRow 3 820 | 770 73.3 | 74.1 761 | 674 71.8 | 72.1 CreateTable+SelectCol 3 958 | 831 75.0 | 75.7 799 | 712 73.3 | 73.6 Table 1: Zero-shot results of Codex and ChatGPT using different database prompt constructions. Table Schema (upper part) contains prompts that solely include the schema of tables, while +Relationship (middle part) incor- porates foreign keys as the table relationships and +Relationship+Content (lower part) adds table content as well. # Tokens is the average token counts in the prompts and EX represents the execution accuracy of SQLs. U|N represents the results of unnormalized prompts and normalized prompts, respectively. The underlines highlight the lower number of tokens and higher accuracies when comparing unnormalized and normalized prompts and the highest accuracy achieved among all prompts is highlighted in bold. 1 shows the average prompt length and execution accuracy of Codex and ChatGPT using various database prompt constructions. Q1: How does normalized database prompt perform compared to unnormalized ones? Nor- malized schemas are found to have a reduced to- ken count in comparison to unnormalized schemas across all database constructions. The normaliza- tion also tends to yield slightly better performance. As for Codex, normalized schemas show improve- ment in all prompts. For ChatGPT, normalized schemas either improve accuracy or achieve the same accuracy or achieve the same level of accu- racy as unnormalized schemas in 6 out of 7 schema constructions. The tests of statistical significance are presented in Appendix A.2. Q2: What database knowledge is crucial for effectively prompting LLMs? Our experiments indicate that table relationships and content are im- portant. The Columns=[] prompt includes only the table schema, while the Columns=[]+ForeignKey prompt contains the additional relationship among tables shown as foreign keys. Including such information improves the performance of both Codex (71.8 -> 73.1) and ChatGPT (69.1 -> 71.2). Moveover, exposing LLMs to database content with the SelectRow and SelectCol prompts fur- ther enhances the performance of both Codex and ChatGPT, while the InsertRow prompt does not seem to be beneficial. We believe that database content is valuable, but its representation needs to be carefully chosen. Q3: How does Codex perform compared to ChatGPT? While we do not focus on comparing different LLMs on the text-to-SQL tasks in this paper, it is worth noting that Codex consistently outperforms ChatGPT on zero-shot text-to-SQL using various prompt constructions. Based on all the findings above, we would recom- mend using Codex in conjunction with normalized CreateTableSelectCol prompt construction for zero-shot text-to-SQL.3 5.2 Single-domain Text-to-SQL In the zero-shot text-to-SQL setting, we discovered that the prompt constructions of databases impact the performance of LLMs. This discovery naturally raises the question of whether the introduction of in-domain demonstrations affects the performance of LLMs to different database prompts. Q1: Does the use of in-domain demonstrations enhance LLM’s performance? Figure 5 depicts the performance of Codex and ChatGPT using dif- ferent database prompt constructions with respect to different numbers of in-domain demonstration examples. For all database prompts, the perfor- mance of LLMs experiences a notable improve- ment when in-domain examples are presented. Fur- thermore, the performance continues to enhance as the number of in-domain examples increases. Q2: What database knowledge is important when presenting in-domain demonstrations? While we have observed that the presence of table 3To simplify our experiments and ensure consistent prompts, we adopt normalization for single-domain and cross- domain experiments."
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,10,caption,Table 1: Zero-shot results of Codex and ChatGPT using different database prompt constructions. Table Schema
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,6,page,"0 5 10 15 70 72 74 76 78 80 82 Table Table(Columns) Columns=[] Table+RS Columns=[]+ForeignKey CreateTable Table+RS+Cont CreateTable+InsertRow CreateTable+SelectRow CreateTable+SelectCol # Shots Execution Accuracy (a) Codex 0 5 10 15 70 72 74 76 78 80 Table Table(Columns) Columns=[] Table+RS Columns=[]+ForeignKey CreateTable Table+RS+Cont CreateTable+InsertRow CreateTable+SelectRow CreateTable+SelectCol # Shots Execution Accuracy (b) ChatGPT Figure 5: Execution accuracy of Codex and ChatGPT for single-domain text-to-SQL with 1, 4, 8, and 16 in- domain examples. RS and Cont correspond to table relationship and table content, respectively. Detailed results can be found in Table 4 and 5. relationships and table content enhanced LLMs’ performance in the zero-shot scenario, it is not clear whether they are still important in the single- domain setting. A hypothesis is that table relation- ship and table content knowledge can be acquired from in-domain examples as they may appear in SQL clauses JOIN and WHERE. For table relationships, we compare two database prompt constructions Columns=[] and Columns=[]+ForeignKey. Both construct the ta- ble schema in the same way while the latter in- cludes foreign keys as table relationships. In the zero-shot scenario, Columns=[]+ForeignKey out- performs Columns=[] by 1.3 and 2.1 for Codex and ChatGPT, respectively. However, as increasing the number of in-domain examples, we notice a gradual reduction in the performance gap between these two prompts. With the utilization of 16 in- domain examples, the gap completely disappears for Codex, while ChatGPT exhibits a marginal dif- ference of only 0.5%. For table content, we compare CreateTable with CreateTable+SelectCol. Both contain the same prompts for presenting the table schema and relationship, while the latter addi- tionally includes table content. In the zero-shot scenario, CreateTable+SelectCol outperforms CreateTable by 2.0% for Codex and 1.7% for ChatGPT. As we proceed to increase the number of in-domain examples, we observe that the per- formance gap between these two prompts does not exhibit a significant reduction. Even with 16 in- domain examples, the gap still persists at 1.3 for Codex and 1.9 for ChatGPT. These results indicate LLMs are able to quickly learn table relationships from a small number of in- domain demonstrations, however, it is more chal- lenging to obtain table content knowledge from demonstration examples. Consequently, the inclu- sion of table content remains crucial for achieving satisfactory performance in the single-domain text- to-SQL scenario. Q3: Can in-domain demonstrations alleviate the sensitivity of LLMs to the representation of ta- ble content? In the zero-shot setting, we observe that LLMs are sensitive to how the table content is presented. Specifically, SelectCol 3 outperforms InsertRow 3 by a substantial margin of 3.8 for Codex and 1.8 for ChatGPT. However, as we ex- pose LLMs to in-domain demonstrations, LLMs be- come less sensitive to the specific representation of table content. The performance disparities among the three table content prompts become marginal. Notably, with only 4 examples, the performance difference between SelectCol 3 and InsertRow 3 diminishes to 0.3 for Codex and 0.2 for ChatGPT. To summarize, in single-domain text-to-SQL, we recommend incorporating a greater number of in-domain examples whenever feasible. It is also essential to ensure the presence of table content in conjunction with the table schema while the specific choice of table content construction is less crucial compared to the zero-shot scenario. 5.3 Cross-domain Text-to-SQL In this section, we present the results to answer a series of questions regarding the demonstration and database prompt construction. 5.3.1 Impact of Demonstration Prompt To investigate the impact of the number of databases and examples per database in demon- strations, we conduct experiments encompassing various combinations. Specifically, our demonstra- tions are composed of M demonstration databases, each containing K NLQ-SQL pairs. We consider scenarios with up to 8 databases and 16 examples per database as long as the combination does not"
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,39,caption,Figure 5: Execution accuracy of Codex and ChatGPT
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,7,page,"1 2 4 6 8 # DBs 168421 # Examples/DB 76.0 76.0 74.5 75.7 75.9 74.2 76.2 75.7 76.3 75.0 75.0 75.2 76.1 74.7 73.8 73.0 73.3 73.9 73.3 72.5 72.5 73.0 73.5 74.0 74.5 75.0 75.5 76.0 Figure 6: A heat map of Codex’s execution accuracy us- ing CreateTable+SelectRow 3 for different numbers of databases and examples per database in the demon- stration. Darker color indicates higher accuracy. 1,1 2,1 4,1 6,1 8,1 1,2 2,2 4,2 6,2 8,2 1,4 2,4 4,4 6,4 1,8 2,8 4,8 1,16 2,16 4,16 2000 3000 4000 5000 6000 7000 72 73 74 75 76 1 example/DB 2 examples/DB 4 examples/DB 8 examples/DB 16 examples/DB # Prompt Tokens Execution Accuracy Figure 7: Execution accuracy of Codex in relation to the length of prompts. Each dot on the graph represents a specific demonstration prompt construction, with the m, kdenoting the number of databases and examples per database used in the prompt. The lines represent second-degree polynomial trendlines fitted to the results. exceed the prompt length limit. We opt to use the database prompt CreateTable+SelectRow 3 as it contains fewer tokens compared to InsertRow and SelectCol while encompassing all valuable database knowledge. We present the experiments with Codex in this section. Experiments involv- ing ChatGPT-16K can be found in Appendix A.4 which show similar results as Codex. Q1: Does increasing demonstration examples enhance LLMs’ performance? Figure 2 presents the accuracy of Codex corresponding to different combinations of the number of databases and the number of examples per database used as demon- strations. We analyze the results from two perspec- tives. Firstly, for a fixed number of databases, we observe an initial improvement in Codex’s perfor- mance as the number of examples per database increases. However, this improvement plateaus or declines once 4 examples per database are provided. Surprisingly, when using 4 databases, employing 8 or 16 examples per database leads to a significant decrease in the Codex’s performance compared to using 2 or 4 examples per database. Secondly, for a fixed number of examples per database, we ob- serve an initial increase in Codex’s performance as the number of databases increases, however, this improvement is followed by a significant decrease once the number of databases reaches a certain threshold (either 4 or 6). Q2: Why does increasing the number of databases decrease LLMs’ performance? As depicted in Figure 2, presenting more databases does not always lead to improved performance. In fact, there is a significant decline in performance, once it surpasses a threshold. We hypothesize that this phenomenon is attributed to the length of the prompt text. To test this hypothesis, we analyze the results in relation to the prompt length. Figure 7 shows the relationship between the accuracy of different demonstration prompts and their prompt lengths. Notably, the performance of Codex exhibits an inverted-U shape as the prompt length increases for each number of examples per database. Additionally, we observe a substantial drop in performance once the prompt text length exceeds approximately 5500 tokens. Similarly, Fig- ure 9 shows that the performance of ChatGPT-16K starts to decrease when prompt text length exceeds 11K tokens. Based on these observations, we con- jecture that LLMs may have a sweet spot in terms of prompt length, potentially influenced by factors such as their model architecture or training data. This indicates that even though LLMs are capable of handling long contexts, they may not necessarily perform better with excessively long prompts. 5.3.2 Impact of Database Prompt Since incorporating demonstration databases may cause a decrease in Codex’s performance, we fo- cus our database prompt experiments on using one demonstration database in combination with vary- ing quantities of demonstration examples. Table 2 presents the execution accuracy of Codex using different database prompts. Q3: Do different database prompts show similar trends with the number of demonstration exam- ples? We observe an initial performance increase for all database prompts. However, once more than 4 examples are provided, the improvement starts to level off, indicating that the different database prompts exhibit similar trends in relation to the number of demonstration examples."
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,18,caption,Figure 6: A heat map of Codex’s execution accuracy us-
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,53,caption,Figure 7: Execution accuracy of Codex in relation to
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,97,caption,Figure 7 shows the relationship between the
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,8,page,"Database Prompt Construction 0-shot 1-shot 2-shot 4-shot 8-shot 16-shot Table Schema Table(Columns) 71.9 72.0 73.0 73.2 72.8 73.9 Columns=[] 71.8 71.9 73.6 74.2 73.7 74.4 +Relationship Columns=[]+ForeignKey 73.1 73.3 74.5 74.9 74.9 75.2 CreateTable 73.1 72.1 73.4 73.7 74.1 75.1 +Relationship+Content CreateTable+InsertRow 3 71.9 72.2 74.1 74.9 74.9 74.8 CreateTable+SelectRow 3 74.1 73.0 75.0 76.2 75.7 76.0 CreateTable+SelectCol 3 75.7 74.4 75.5 76.5 76.8 76.5 Table 2: Cross-domain results of Codex using different database prompt constructions. Only one demonstration database is included in a prompt, N-shot represents N examples corresponding to the demonstration database. The best and second-best results for each shot are highlighted in bold and underlined. Q4: Can out-of-domain demonstrations allevi- ate the sensitivity of LLMs to database prompts? First, we observe that incorporating table relation- ships and content in the prompts remains crucial for effectively prompting Codex in the cross-domain setting. This is not surprising, as Codex cannot di- rectly learn knowledge specific to the test database from the out-of-domain demonstrations. Further- more, we find that Codex continues to exhibit sensi- tivity to the representation of table content. Despite having demonstration databases that mirror the con- struction of the test database, Codex still displays a preference forSelectRow and SelectCol when presenting table content, compared to InsertCol. In conclusion, while out-of-domain demonstra- tions enhance LLMs’ capabilities in text-to-SQL, they do not provide database-specific knowledge. Consequently, careful construction of database prompts remains crucial, aligning with the observa- tions made in the zero-shot setting. 6 Related Work LLMs for Text-to-SQL In recent years, there has been significant progress in leveraging LLMs for the text-to-SQL task. Various methods have been proposed to enhance the capabilities of LLMs. For example, Rubin et al. (2021); Poesia et al. (2022) have demonstrated the effectiveness of similarity-based demonstration retrieval in the cross-domain setting. Additionally, Levy et al. (2022) have highlighted the advantages of incor- porating diverse demonstrations for compositional generalization. Furthermore, Pourreza and Rafiei (2023) and Chen et al. (2023) incorporate interme- diate steps in prompts and unlock LLMs’ capability of self-correcting their predictions. In contrast to these approaches, our focus lies in conducting a comprehensive evaluation of prompt representations across different text-to-SQL set- tings. While there are similar motivations to the work by Rajkumar et al. (2022), which analyzes the performance of CodeX on Spider for the zero-shot setting and on two databases for the single-domain setting, we aim to provide more general findings by evaluating across a wider range of databases and considering all three text-to-SQL settings. Table Representation Encoding structured databases with neural models has been a persistent challenge. To encode database schema, graph neural networks are utilized to represent the rela- tionships among tables (Bogin et al., 2019; Chen et al., 2021b). Alternatively, other studies (Guo et al., 2019; Lin et al., 2020; Shaw et al., 2020) have converted table schemas into a sequence to effectively leverage pretrained language models, such as BERT (Devlin et al., 2018) and T5 (Raffel et al., 2020). In such cases, table relationships can be encoded as meta-data features (Lin et al., 2020) or used as a guide for attention mechanism (Wang et al., 2019; Cao et al., 2021; Li et al., 2023b). To incorporate table content into neural mod- els, prior supervised methods provide question- specific table content by identifying the relevant table content mentioned in the question through string matching (Lin et al., 2020; Shaw et al., 2020). However, Chang et al. (2023) have revealed the vulnerability of string matching to perturbations. Given that LLMs with in-context learning support longer input sequences compared to supervised methods, we follow previous work to provide table content without explicitly considering the questions (Rajkumar et al., 2022; Chen et al., 2023)."
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,9,caption,Table 2: Cross-domain results of Codex using different database prompt constructions. Only one demonstration
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,9,page,"7 Conclusions In this paper, we investigate effective prompt- ing strategies in the text-to-SQL task. We thor- oughly compare various prompt construction strate- gies for databases and demonstrations in the zero- shot, single-domain, and cross-domain text-to-SQL. Through our investigation, we uncover the critical database knowledge and optimal representations for effective prompting. Additionally, an interest- ing finding is the existence of a sweet spot in terms of prompt length for Codex in the cross-domain setting. Overall, we believe that our findings will provide valuable guidance for future research in the field of text-to-SQL with LLMs. Limitation We conducted our experiments using 20 databases from the Spider dataset, with the goal of providing general findings for text-to-SQL prompt construc- tions. However, our findings may not always be applicable to a specific database, particularly if the database is significantly different from the Spider databases. For the single-domain and cross-domain text-to-SQL scenarios, we conduct our experiments multiple times, each involving randomly selecting demonstrations with different random seeds, how- ever, we did not investigate the effectiveness of prompt constructions with different demonstration- retrieval strategies or intermediate reasoning steps. Ethics Statement We acknowledge the importance of the ACL Ethics Policy and agree with it. In this paper, we use Ope- nAI Codex and ChatGPT as our language models 4. Codex is currently free for research purposes, the cost of ChatGPT is around $200. The code for the paper is included in the supplementary ma- terials and will be publicly released to facilitate reproducibility. 4API is available at https://openai.com/api/. References Ben Bogin, Matt Gardner, and Jonathan Berant. 2019. Representing schema structure with graph neural networks for text-to-sql parsing. arXiv preprint arXiv:1905.06241. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901. Ruisheng Cao, Lu Chen, Zhi Chen, Yanbin Zhao, Su Zhu, and Kai Yu. 2021. Lgesql: line graph en- hanced text-to-sql model with mixed local and non- local relations. arXiv preprint arXiv:2106.01093. Shuaichen Chang, Jun Wang, Mingwen Dong, Lin Pan, Henghui Zhu, Alexander Hanbo Li, Wuwei Lan, Sheng Zhang, Jiarong Jiang, Joseph Lilien, et al. 2023. Dr. spider: A diagnostic evaluation bench- mark towards text-to-sql robustness. arXiv preprint arXiv:2301.08881. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021a. Evaluating large lan- guage models trained on code. arXiv preprint arXiv:2107.03374. Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128. Zhi Chen, Lu Chen, Yanbin Zhao, Ruisheng Cao, Zi- han Xu, Su Zhu, and Kai Yu. 2021b. Shadowgnn: Graph projection neural network for text-to-sql parser. arXiv preprint arXiv:2104.04689. Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, et al. 2022. Binding language models in symbolic languages. arXiv preprint arXiv:2210.02875. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Deborah A. Dahl, Madeleine Bates, Michael Brown, William Fisher, Kate Hunicke-Smith, David Pallett, Christine Pao, Alexander Rudnicky, and Elizabeth Shriberg. 1994. Expanding the scope of the atis task: The atis-3 corpus. In Proceedings of the Workshop on Human Language Technology, pages 43–48. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805."
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,10,page,"Catherine Finegan-Dollak, Jonathan K Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, and Dragomir Radev. 2018. Improving text-to-sql evaluation methodology. arXiv preprint arXiv:1806.09029. Yujian Gan, Xinyun Chen, Jinxia Xie, Matthew Purver, John R Woodward, John Drake, and Qiaofu Zhang. 2021. Natural sql: Making sql easier to infer from natural language specifications. arXiv preprint arXiv:2109.05153. Jiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao, Jian- Guang Lou, Ting Liu, and Dongmei Zhang. 2019. To- wards complex text-to-sql in cross-domain database with intermediate representation. arXiv preprint arXiv:1905.08205. Itay Levy, Ben Bogin, and Jonathan Berant. 2022. Diverse demonstrations improve in-context compositional generalization. arXiv preprint arXiv:2212.06800. Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. 2023a. Decoupling the skeleton parsing and schema linking for text-to-sql. arXiv preprint arXiv:2302.05965. Jinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei Huang, Wenyu Du, Luo Si, and Yongbin Li. 2023b. Graphix-t5: Mixing pre- trained transformers with graph-aware layers for text- to-sql parsing. arXiv preprint arXiv:2301.07507. Xi Victoria Lin, Richard Socher, and Caiming Xiong. 2020. Bridging textual and tabular data for cross- domain text-to-sql semantic parsing. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4870–4888. Aiwei Liu, Xuming Hu, Lijie Wen, and Philip S Yu. 2023. A comprehensive evaluation of chat- gpt’s zero-shot text-to-sql capability. arXiv preprint arXiv:2303.13547. Gabriel Poesia, Oleksandr Polozov, Vu Le, Ashish Ti- wari, Gustavo Soares, Christopher Meek, and Sumit Gulwani. 2022. Synchromesh: Reliable code gen- eration from pre-trained language models. arXiv preprint arXiv:2201.11227. Mohammadreza Pourreza and Davood Rafiei. 2023. Din-sql: Decomposed in-context learning of text-to-sql with self-correction. arXiv preprint arXiv:2304.11015. Patti Price. 1990. Evaluation of spoken language sys- tems: The atis domain. In Speech and Natural Lan- guage: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania, June 24-27, 1990. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring the limits of transfer learning with a unified text-to-text trans- former. J. Mach. Learn. Res., 21(140):1–67. Nitarshan Rajkumar, Raymond Li, and Dzmitry Bah- danau. 2022. Evaluating the text-to-sql capabil- ities of large language models. arXiv preprint arXiv:2204.00498. Ohad Rubin and Jonathan Berant. 2021. Smbop: Semi- autoregressive bottom-up semantic parsing. In Pro- ceedings of the 2021 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 311–324. Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2021. Learning to retrieve prompts for in-context learning. arXiv preprint arXiv:2112.08633. Torsten Scholak, Nathan Schucher, and Dzmitry Bah- danau. 2021. Picard: Parsing incrementally for constrained auto-regressive decoding from language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9895–9901. Peter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova. 2020. Compositional general- ization and natural language variation: Can a seman- tic parsing approach handle both? arXiv preprint arXiv:2010.12725. Peng Shi, Rui Zhang, He Bai, and Jimmy Lin. 2022. Xricl: Cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql semantic pars- ing. arXiv preprint arXiv:2210.13693. Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew Richardson. 2019. Rat-sql: Relation-aware schema encoding and linking for text- to-sql parsers. arXiv preprint arXiv:1911.04942. Tao Yu, Chien-Sheng Wu, Xi Victoria Lin, Bailin Wang, Yi Chern Tan, Xinyi Yang, Dragomir R Radev, Richard Socher, and Caiming Xiong. 2021. Grappa: Grammar-augmented pre-training for table semantic parsing. In ICLR. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingn- ing Yao, Shanelle Roman, et al. 2018. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. arXiv preprint arXiv:1809.08887. John M. Zelle and Raymond J. Mooney. 1996. Learn- ing to parse database queries using inductive logic programming. In Proceedings of the Thirteenth Na- tional Conference on Artificial Intelligence - Volume 2, pages 1050–1055."
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,11,page,"A Appendix A.1 Prompt Examples Below contains an example of a zero-shot nor- malized prompt, which contains the database Network_1 from Spider (Yu et al., 2018), a task instruction “Using valid SQLite, answer the fol- lowing questions for the tables provided above.”, and a test question “How many high schoolers are there?”. Zero-shot normalized prompt create table highschooler ( id int primary key , name text , grade int ); /* 3 example rows : select * from highschooler limit 3; id name grade 1510 Jordan 9 1689 Gabriel 9 1381 Tiffany 9 */ create table friend ( student_id int , friend_id int , primary key ( student_id , friend_id ), foreign key ( student_id ) references highschooler (id), foreign key ( friend_id ) references highschooler (id) ); /* 3 example rows : select * from friend limit 3; student_id friend_id 1510 1381 1510 1689 1689 1709 */ create table likes ( student_id int , liked_id int , primary key ( student_id , liked_id ), foreign key ( liked_id ) references highschooler (id), foreign key ( student_id ) references highschooler (id) ); /* 3 example rows : select * from likes limit 3; student_id liked_id 1689 1709 1709 1689 1782 1709 */ -- Using valid SQLite , answer the following questions for the tables provided above . Question : How many high schoolers are there ? select Below contains an example of a 4-shot single- domain normalized prompt, which contains a database prompt and 4 demonstration examples ahead of the test question. 4-shot single-domain normalized prompt create table highschooler ( id int primary key , name text , grade int ); /* 3 example rows : select * from highschooler limit 3; id name grade 1510 Jordan 9 1689 Gabriel 9 1381 Tiffany 9 */ create table friend ( student_id int , friend_id int , primary key ( student_id , friend_id ), foreign key ( student_id ) references highschooler (id), foreign key ( friend_id ) references highschooler (id) ); /* 3 example rows : select * from friend limit 3; student_id friend_id 1510 1381 1510 1689 1689 1709 */ create table likes ( student_id int , liked_id int , primary key ( student_id , liked_id ), foreign key ( liked_id ) references highschooler (id), foreign key ( student_id ) references highschooler (id) ); /* 3 example rows : select * from likes limit 3; student_id liked_id 1689 1709 1709 1689 1782 1709 */ -- Using valid SQLite , answer the following questions for the tables provided above . Question : What is Kyle 's id? select id from highschooler where name = ' Kyle '; Question : Return the names of friends of the high school student Kyle . select t3. name from friend as t1 join highschooler as t2 on t1. student_id = t2.id join highschooler as t3 on t1. friend_id = t3 .id where t2. name = 'Kyle '; Question : Show names of all high school students who do not have any friends . select name from highschooler except select t2. name from friend as t1 join highschooler as t2 on t1. student_id = t2.id; Question : What are the names and grades for each high schooler ? select name , grade from highschooler ; Question : How many high schoolers are there ? select"
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,12,page,"Below contains an example of a 4-shot cross- domain prompt, which contains 2 demonstration databases, each with 2 demonstration examples ahead of the test database and question. 4-shot cross-domain prompt create table publication ( publication_id int , book_id int , publisher text , publication_date text , price real , primary key ( publication_id ), foreign key ( book_id ) references book ( book_id ) ); /* 3 example rows : select * from publication limit 3; publication_id book_id publisher publication_date price 1 1 Pearson August 2008 15000000.0 2 3 Thomson Reuters March 2008 6000000.0 3 4 Wiley June 2006 4100000.0 */ create table book ( book_id int , title text , issues real , writer text , primary key ( book_id ) ); /* 3 example rows : select * from book limit 3; book_id title issues writer 1 The Black Lamb 6.0 Timothy Truman 2 Bloody Mary 4.0 Garth Ennis 3 Bloody Mary : Lady Liberty 4.0 Garth Ennis */ -- Using valid SQLite , answer the following questions for the tables provided above . Question : List the writers of the books in ascending alphabetical order . select writer from book order by writer asc ; Question : How many books are there ? select count (*) from book ; create table race ( race_id int , name text , class text , date text , track_id text , primary key ( race_id ), foreign key ( track_id ) references track ( track_id ) ); /* 3 example rows : select * from race limit 3; race_id name class date track_id 1 Rolex 24 At Daytona DP/GT January 26 January 27 1 2 Gainsco Grand Prix of Miami DP/GT March 29 2 3 Mexico City 250 DP/GT April 19 2 */ create table track ( track_id int , name text , location text , seating real , year_opened real , primary key ( track_id ) ); /* 3 example rows : select * from track limit 3; track_id name location seating year_opened 1 Auto Club Speedway Fontana , CA 92000.0 1997.0 2 Chicagoland Speedway Joliet , IL 75000.0 2001.0 3 Darlington Raceway Darlington , SC 63000.0 1950.0 */ -- Using valid SQLite , answer the following questions for the tables provided above . Question : Show the name and location for all tracks . select name , location from the track ; Question : Show the name of track and the number of races in each track . select t2.name , count (*) from race as t1 join track as t2 on t1. track_id = t2. track_id group by t1. track_id ; create table highschooler ( id int primary key , name text , grade int ); /* 3 example rows : select * from highschooler limit 3; id name grade 1510 Jordan 9 1689 Gabriel 9 1381 Tiffany 9 */ create table friend ( student_id int , friend_id int , primary key ( student_id , friend_id ), foreign key ( student_id ) references highschooler (id), foreign key ( friend_id ) references highschooler (id) ); /* 3 example rows : select * from friend limit 3; student_id friend_id 1510 1381 1510 1689 1689 1709 */ create table likes ( student_id int , liked_id int , primary key ( student_id , liked_id ), foreign key ( liked_id ) references highschooler (id), foreign key ( student_id ) references highschooler (id) ); /* 3 example rows : select * from likes limit 3; student_id liked_id 1689 1709 1709 1689 1782 1709 */ -- Using valid SQLite , answer the following questions for the tables provided above . Question : How many high schoolers are there ? select"
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,13,page,"A.2 Tests of Significance Table 1 contains the performance of Codex and ChatGPT using different database prompt construc- tions in the zero-shot setting. We observe that the normalization results in slightly improved perfor- mance for all database prompt constructions with Codex and 6 out of 7 database prompt construc- tions with ChatGPT. It is important to note, how- ever, that when comparing normalized and unnor- malized database prompt constructions using the same method, the results did not demonstrate statis- tical significance in McNemar’s test, with p-values greater than 0.05. Nevertheless, the primary advan- tage of normalization lies in its ability to reduce variations among different databases and minimize the overall prompt length. When evaluating various prompt con- structions, we note the advantages gained from incorporating both table relationships (Columns=[]+ForeignKey vs Columns=[]) and table content ( CreateTable+SelectCol 3 vs CreateTable) are mostly statistically significant in McNemar’s test, with p-values smaller than 0.05. Table 3 displays the results of the significant tests. The performance of Columns=[]+ForeignKey compared to Columns=[] is statistically sig- nificant in all cases, except for codex with normalized prompts. Likewise, the performance of CreateTable+SelectCol 3 is statistically significant for both Codex and ChatGPT, with both normalized and unnormalized prompts, when com- pared to CreateTable. These significant findings highlight the effectiveness of incorporating table relationships and database content. A.3 Detailed Single-domain Results Tables 4 and 5 provide detailed results of Codex and ChatGPT in the single-domain setting, respec- tively. The performance of both models is also illustrated in Figure 5. A.4 Impact of Demonstration Prompt for ChatGPT-16K Figure 8 presents the accuracy of ChatGPT-16K corresponding to different combinations of the number of databases and the number of examples per database used as demonstrations. Similar to our findings with Codex, presenting more databases does not always lead to improved performance for ChatGPT-16K. For a fixed number of examples per database, we observe an initial increase in its 1 2 4 8 12 16 # DBs 168421 # Examples/DB 77.0 77.0 77.6 76.4 75.8 76.4 77.4 77.9 75.9 74.8 76.2 75.9 77.5 77.6 75.9 76.1 76.3 76.7 76.5 76.6 75.9 76.4 75.5 77.3 76.4 77.3 76.3 75.0 75.5 76.0 76.5 77.0 77.5 Figure 8: A heat map of ChatGPT-16K’s execution ac- curacy using CreateTable+SelectRow 3 for different numbers of databases and examples per database in the demonstration. Darker color indicates higher accuracy. 1,1 2,1 4,1 8,1 12,1 16,1 1,2 2,2 4,2 8,2 12,2 16,2 1,4 2,4 4,4 8,4 12,4 16, 1,8 2,8 4,8 8,8 12,8 1,16 2,16 4,16 8,16 2k 4k 6k 8k 10k 12k 14k 72 73 74 75 76 77 78 1 example/DB 2 examples/DB 4 examples/DB 8 examples/DB 16 examples/DB # Prompt Tokens Execution Accuracy Figure 9: Execution accuracy of ChatGPT-16K in re- lation to the length of prompts. Each dot represents a demonstration construction, with the m, kdenoting the number of databases and examples per database. The lines represent second-degree polynomial trendlines fit- ted to the results. performance as the number of databases increases, however, this improvement is followed by a de- crease once the number of databases reaches a cer- tain threshold. To understand this phenomenon, we analyze the results in relation to the prompt length. Figure 9 shows the relationship between the ac- curacy of different demonstration prompts and their prompt lengths. Similar to Codex, the performance of ChatGPT-16K also exhibits an inverted-U shape as the prompt length increases for each number of examples per database. Additionally, we observe the performance starts to decrease once the prompt text length exceeds approximately 11K tokens. While Codex supports 8K tokens and ChatGPT- 16K supports 16K tokens, we notice that their performance tends to decline when dealing with demonstrations that exceed approximately 70% of the maximum prompt length."
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,2,caption,Table 1 contains the performance of Codex and
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,24,caption,Table 3 displays the results of the significant tests.
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,42,caption,Figure 8 presents the accuracy of ChatGPT-16K
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,65,caption,Figure 8: A heat map of ChatGPT-16K’s execution ac-
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,103,caption,Figure 9: Execution accuracy of ChatGPT-16K in re-
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,114,caption,Figure 9 shows the relationship between the ac-
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,14,page,"Prompt 1 Prompt 2 LLM Normalization Significant Test Columns=[] Columns=[]+ForeignKey Codex U ✓ Codex N ✗ ChatGPT U ✓ ChatGPT N ✓ CreateTable CreateTable+SelectCol 3 Codex U ✓ Codex N ✓ ChatGPT U ✓ ChatGPT N ✓ Table 3: Tests of Statistical Significance for comparing different prompt constructions. Prompt 1 and Prompt 2 were used to represent two distinct methods of constructing prompts in McNemar’s test. The prompts were categorized as U and N, representing unnormalized and normalized database prompts, respectively. The ✓ symbol indicates that the p-value is smaller than 0.05, indicating statistical significance, while the ✗ symbol indicates p-values greater than 0.05, indicating a lack of statistical significance. Database Prompt Construction 0-shot 1-shot 4-shot 8-shot 16-shot Table Schema Table(Columns) 71.9 70.7 74.9 78.0 81.6 Columns=[] 71.8 72.1 75.5 78.0 81.6 +Relationship Columns=[]+ForeignKey 73.1 73.2 76.1 78.5 81.6 CreateTable 73.1 72.4 76.0 78.7 81.3 +Relationship+Content CreateTable+InsertRow 3 71.9 72.9 77.6 80.5 82.5 CreateTable+SelectRow 3 74.1 73.4 77.3 80.5 82.9 CreateTable+SelectCol 3 75.7 74.1 77.9 80.7 82.5 Table 4: Single-domain results of Codex using different prompt constructions for database schema and content. The best and second-best results for each shot are highlighted in bold and underlined. Database Prompt Construction 0-shot 1-shot 4-shot 8-shot 16-shot Table Schema Table(Columns) 70.5 71.6 74.3 77.4 79.4 Columns=[] 69.1 70.7 74.4 77.8 79.5 +Relationship Columns=[]+ForeignKey 71.2 73.4 75.4 78.4 80.0 CreateTable 71.7 73.1 75.8 78.0 79.5 +Relationship+Content CreateTable+InsertRow 3 71.8 72.8 76.6 79.1 81.6 CreateTable+SelectRow 3 72.1 73.3 76.4 78.9 81.3 CreateTable+SelectCol 3 73.6 73.8 76.8 79.8 81.4 Table 5: Single-domain results of ChatGPT using different prompt constructions for database schema and content. The best and second-best results for each shot are highlighted in bold and underlined."
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,12,caption,Table 3: Tests of Statistical Significance for comparing different prompt constructions. Prompt 1 and Prompt 2 were
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,25,caption,Table 4: Single-domain results of Codex using different prompt constructions for database schema and content. The
Chang and Fosler-Lussier - 2023 - How to Prompt LLMs for Text-to-SQL A Study in Zer.pdf,35,caption,Table 5: Single-domain results of ChatGPT using different prompt constructions for database schema and content.
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,1,page,"The VLDB Journal (2023) 32:905–936 https://doi.org/10.1007/s00778-022-00776-8 REGULAR PAPER A survey on deep learning approaches for text-to-SQL George Katsogiannis-Meimarakis 1 · Georgia Koutrika 1 Received: 27 May 2022 / Revised: 31 October 2022 / Accepted: 10 December 2022 / Published online: 23 January 2023 © The Author(s) 2023 Abstract To bridge the gap between users and data, numerous text-to-SQL systems have been developed that allow users to pose natural language questions over relational databases. Recently, novel text-to-SQL systems are adopting deep learning methods with very promising results. At the same time, several challenges remain open making this area an active and ﬂourishing ﬁeld of research and development. To make real progress in building text-to-SQL systems, we need to de-mystify what has been done, understand how and when each approach can be used, and, ﬁnally, identify the research challenges ahead of us. The purpose of this survey is to present a detailed taxonomy of neural text-to-SQL systems that will enable a deeper study of all the parts of such a system. This taxonomy will allow us to make a better comparison between different approaches, as well as highlight speciﬁc challenges in each step of the process, thus enabling researchers to better strategise their quest towards the “holy grail” of database accessibility. Keywords Text-to-SQL · Deep learning · Natural language processing · Natural language interface for databases 1 Introduction In the age of the Digital Revolution, data is now an indis- pensable commodity that drives almost all human activities, from business operations to scientiﬁc research. Nevertheless, its explosive volume and increasing complexity make data querying and exploration challenging even for experts. Exist- ing data query interfaces are either form-based, which are easy to use but offer limited query capabilities, or low-level tools that allow users to synthesise queries in the underlying database query language (e.g. SQL) but are intended for the few (e.g. SQL experts). To empower everyone to access, use, understand, and derive value from data, we need to lift the technical barriers that impede access to data and eliminate dependency to IT experts. Expressing queries in natural lan- guage can open up data access to everyone. In the words of E. F. Codd: “If we are to satisfy the needs of casual users of databases we must break the barriers that presently pre- vent these users from freely employing their native language” [14]. B George Katsogiannis-Meimarakis katso@athenarc.gr Georgia Koutrika georgia@athenarc.gr 1 Athena Research Center, Athens, Greece Towards this direction, there has been an increasing research focus on Natural Language (NL) Interfaces for Databases (NLIDBs) that allow users to pose queries in nat- ural language and translate these queries to the underlying database query language. In particular, text-to-SQL (or NL- to-SQL) systems translate queries from NL to SQL. As the text-to-SQL problem is notoriously hard, these systems have been the holy grail of the database community for several decades [5]. Early efforts [36,37,60,110] rely primarily on the database schema and data indexes to build the respective SQL query from a NL query. A query answer is deﬁned as a graph where nodes are the relations that contain the query keywords and edges represent the joins between them. Parsing-based approaches parse the input question to understand its gram- matical structure, which is then mapped to the structure of the desired SQL query [ 42,53,69,90,98]. Recently, there has been a growing interest in neural machine translation (NMT) approaches [33,89,112] that formulate the text-to-SQL prob- lem as a language translation problem, and train a neural network on a large amount of {NL query/SQL} pairs. These approaches have bloomed due to the recent advances in deep learning and natural language processing (NLP), along with the creation of two large datasets (WikiSQL [ 112] and Spider [107]) for training text-to-SQL systems. As neural text-to-SQL systems are popping up “like mush- rooms after a rain” with promising results, an exciting, but, 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,2,page,"906 G. Katsogiannis-Meimarakis, G. Koutrika at the same time, highly competitive and fast-paced research ﬁeld is opening up. While a growing interest on the sub- ject is shown by various tutorials [ 44,45,54] and literature reviews [1,2,5,17,40,47,55,72] presented at top conferences and journals, an in-depth, systematic study and taxonomy of neural approaches for text-to-SQL is missing. We believe that in order to make real progress in building text-to-SQL systems, we need to de-mystify what has been done, under- stand how and when each model and approach can be used, and recognise the research challenges ahead of us. Two ear- lier works [ 2,55] study rule-based approaches that originated from the database community; our work has a different scope, focusing entirely on deep learning systems. Additionally, two studies consider both rule-based and neural text-to-SQL sys- tems: [47] provides a taxonomy of both types of systems and an experimental evaluation based on a new accuracy metric proposed by the authors, while [ 72] provides a large-scale overview of rule-based, neural and conversational NLIDBs. The biggest difference with these works is that we present an in-depth taxonomy tailored to neural systems and their peculiarities (while also covering more and newer efforts). Finally, three studies focus on neural text-to-SQL systems: [1] provides an overview of the neural text-to-SQL land- scape, but in a more bare-bones manner compared to our work, and [ 17,40], which are the closest to our work, since they both attempt to organise the existing neural text-to-SQL approaches. However, our work goes in greater depth than these works, both by presenting a taxonomy with additional dimensions, but also by using this taxonomy to analyse and compare different systems and design choices. We also point the interested reader to recent surveys on semantic pars- ing [ 43] and context-dependent semantic parsing [ 56], two broader domains that the text-to-SQL problem is a part of. In a nutshell, this survey aims at catching up with recent advances in deep learning text-to-SQL systems and sys- tematically organising all the different techniques that have been proposed for each step of the translation process. Our objective is to ( a) put different neural text-to-SQL works in perspective, ( b) create a ﬁne-grained taxonomy that cov- ers each step of the neural text-to-SQL pipeline, ( c) explain and organise all the techniques used for each dimension of the taxonomy, ( d) use the taxonomy to compare and high- light the strengths and weaknesses of different systems and techniques, and ( e) highlight open challenges and research opportunities for the database and the machine learning communities. Our study is also relevant to other areas, including the broader area of data exploration (e.g. natural language explanations, recommendations), entity resolution, and query optimisation, where the methods presented here may be transferred to or inspire the development of new methods. In particular, our contributions are the following: – We present the current state of the deep learning text- to-SQL landscape, the particularities of the problem, the benchmarks and evaluation methods that are most com- monly used, and a wide spectrum of the most recent efforts that leverage the latest and most sophisticated deep learning approaches – We provide a taxonomy that not only enables a side- by-side comparison of the systems but also allows decomposing the text-to-SQL problem in a number of sub-problems and categorising existing techniques accordingly – We provide a detailed discussion of methods used in these systems, taking advantage of our taxonomy to high- light the advantages and shortcomings of different design choices – We discuss in detail open challenges that are highlighted from our study and provide directions for critical future research The rest of this paper is organised as follows: Sect. 2 provides a deﬁnition and explanation of the text-to-SQL problem, including an analysis of the challenges that make the problem so hard. In Sect. 3, we present the datasets that are currently fuelling the creation of deep learning systems. We also touch on the problem of evaluating system performance based on these benchmarks. Section 4 presents a ﬁne-grained taxonomy for deep learning text-to-SQL systems, analysing the most important steps followed by all systems and pre- senting current work, open problems and hints for future research for each step. Section 5 gives an overview of the main neural building blocks used for text-to-SQL systems, as well as their most common usage. Having established a concrete set of axes for comparing and classifying text- to-SQL systems, in Sect. 6, a multitude of neural systems are presented and compared based on the aforementioned taxonomy, allowing the reader to grasp the progress that has been made in this domain and the differences between key approaches. In Sect. 7, we take advantage of the taxon- omy, to compare different design choices and provide useful insights for researchers and practitioners that are interested in implementing a novel text-to-SQL system. Finally, Sect. 8 aims at inspiring practitioners and researchers in the ﬁelds of database systems, natural language processing and deep learning, by shedding light on open problems that need to be addressed, as well as closely related areas that could both give and receive beneﬁt from research done in the text-to- SQL problem. 2 The text-to-SQL problem The text-to-SQL problem can be described as follows: 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,3,page,"A survey on deep learning approaches for text-to-SQL 907 Fig. 1 The text-to-SQL problem Given a natural language query (NLQ) on a Relational Database (RDB) with a speciﬁc schema, produce a SQL query equivalent in meaning, which is valid for the said RDB and that when executed will return results that match the user’s intent. A NLQ may be expressed as a complete and ﬂuent utterance (e.g. “What movies has Spielberg directed since 2012?”) or it may be just a few keywords (e.g. “Italian Restaurants in Vienna” ). A text-to-SQL example can be seen in Fig. 1. Translating a NLQ to SQL hides challenges related to the understanding of the input NL query as well as related to building the correct (syntactically and semantically) SQL query based on the underlying database schema. 2.1 NL challenges Ambiguity Natural language is inherently ambiguous, which means that it allows the formulation of expressions that are open to more than one interpretation. There are several types of ambiguity [ 3,66]. We describe the most common ones below. Lexical ambiguity (or polysemy) refers to a single word having multiple meanings. For example, “Paris” can be a city or a person. Syntactic ambiguity refers to a sentence having multiple interpretations based on its syntactic structure. For exam- ple, the question “Find all German movie directors” can be parsed into “directors that have directed German movies” or “directors from Germany that have directed a movie” . Semantic ambiguity refers to a sentence with multi- ple semantic interpretations. For instance, “Are Brad and Angelina married?” may mean they are married to each other or separately. Context-dependent ambiguity refers to a term having dif- ferent meanings depending on the query context, the data domain, and the user goals. The most common example terms are “top” and “best” . Based on the query context ,f o rt h e query “Who was the best runner of the marathon?” ,t h e one who completed the race faster ( min operation) should be returned, but when asking “Which was the best nation of the 2004 Olympics?” the one with the most medals ( max operation) is expected. Based on the domain , for the query “Return the top movie” on a movie database, “top” may mean based on the number of ratings collected. On the other hand, for the query “Return the top scorer” on a football database, “top” refers to the number of goals scored. Based on the user, for a business analyst, the query “Return the top product” should return the most proﬁtable products, whereas for a consumer it should return the top-rated products. ParaphrasingIn natural language, two sentences can have the exact same meaning but be expressed in two completely different ways. For instance, “How many people live in Texas?” and “What is the population of Texas?” . Both trans- late to the same SQL query, but the second one may actually be easier for a system because it is likely that a “popula- tion” attribute exists in the database schema, and thus, the user intent can be inferred with high conﬁdence. Paraphras- ing includes synonymy where multiple words have the same meaning (e.g. “movies” and “ﬁlms” ). Inference A query may not contain all information needed for a system to fully understand it. The system has to infer the missing information based on the given context. We dis- tinguish two main types of inference: Elliptical queries are sentences from which one or more words are omitted but can still be understood in the context of the sentence. 1 An example is “Who was the president before Obama”. The fact that the query refers to US presidents needs to be inferred. Follow-up questionsare common in conversations between humans. We ask a question, receive an answer, and then ask a follow-up question assuming that the context of the ﬁrst question is known. For example, “Q: Which is the capital of Germany?” , “A: Berlin” , “Q: What about France?” .I n the absence of the ﬁrst question, the second one does not make sense, but given the query context, it is obvious that it is asking about the capital city of France. User mistakes Spelling errors as well as syntactical or grammatical errors make the translation problem even more challenging. 2.2 SQL challenges SQL syntax SQL has a strict syntax, which leads to limited expressivity compared to natural language. There are queries that are easy to express in natural language, but the respective SQL query may be complex. For example, the query “Return the movie with the best rating” maps to a nested SQL query. Furthermore, while a sentence in natural language may contain some mistakes, and still be understood by a human, SQL is not that forgiving. An SQL query translated from a NL query needs to be syntactically and semantically correct in order to be executable over the underlying data. Database structure The user’s conceptual model of the data, i.e. the entities, their attributes and relationships that are 1 https://en.wikipedia.org/wiki/Ellipsis_(linguistics). 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,2,caption,Fig. 1 The text-to-SQL problem
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,4,page,"908 G. Katsogiannis-Meimarakis, G. Koutrika described in the data, may not match the database schema, and that poses several challenges. The vocabulary gap refers to the differences between the vocabulary used by the database and the one used by the user. For example, in the query “Who was the best actress in 2011?”, “actress” should map to the Actor .nameattribute in the database). Schema ambiguity is when a part of the query may map to more than one database element. For example, “model” could refer to car .modelor engine.model. Implicit join operations occur when parts of a query are translated into joins across multiple relations. For example, “Find the director of the movie “A Beautiful Mind”” entails joins due to database normalisation. Entity modelling is the problem where a set of entities may be modelled differently, e.g. as different tables or as rows (or values) in a single table. For example, in a university database, every person is either a Student or a Faculty mem- ber, so these two relations sufﬁce. On the other hand, movies have several genres that cannot be stored as different tables. T h e ya r es t o r e di na Genre relation and are connected with movies through a many-to-many relationship. As a result, similar queries, such as “Find comedies released in 2018” and “Find students enrolled in 2018” need in fact to be han- dled differently. The system maps “comedies” to a value in the Genre table and joins it with the Movie table whereas it maps “students” to the Student relation. 3 Datasets and evaluation To build a neural text-to-SQL system, it is necessary to con- sider the available datasets for training and evaluation, as well as the evaluation methodology for testing and compar- ing its performance to other systems. A text-to-SQL dataset (or benchmark) refers to a set of NL/SQL query pairs deﬁned over one or more databases. Early system evaluations did not rely on common datasets , they rather employed a variety of datasets that combined different databases and query sets of varying size and com- plexity. In general, the query sets were small and designed in an ad-hoc way by the system developers, and as a result it was hard to reach meaningful conclusions about the translation capabilities of a system. Often, the query sets were propri- etary and hence not available to reproduce the experiments. The lack of a common dataset to be used by different system evaluations and the poor cross-system evaluations impeded a fair system comparison and a clear view of the text-to-SQL landscape. In addition to these shortcomings, training deep learning text-to-SQL systems requires a substantial query set. As a result, for a long time, the lack of appropriate datasets delayed the adoption of deep learning techniques for the text- to-SQL problem. Table 1 A comparison of the two most popular text-to-SQL bench- marks: WikiSQL and Spider WikiSQL Spider Crowd-sourced Created by experts 25K Wikipedia tables 200 databases, 138 domains 80K NL questions 10K NL questions Single-table, simple queries Complex queries Contains errors Higher quality No query categorisation 4 hardness categories Table 2 An overview of text-to-SQL benchmarks and their size in queries and databases Year Dataset Queries Databases 1994 A TIS [ 15,71] 275 1 1996 GeoQuery [ 109] 525 1 2003 Restaurants [ 70,84]3 9 1 2014 Academic [ 53] 179 1 2017 IMDb [ 98] 111 1 Yelp [98]6 8 1 Scholar [ 42] 396 1 WikiSQL [112] 80,654 24,241 2018 Advising [ 27] 281 1 Spider [ 107] 10,181 200 2020 MIMICSQL [ 92] 10,000 1 SQUALL [80] 11,276 1679 FIBEN [ 77] 300 1 2021 Spider-Syn [ 28] 8034 160 Spider-DK [29] 535 10 KaggleDBQA [ 51] 272 8 SEDE [ 34] 12,023 1 This situation drastically changes with the emergence of WikiSQL [ 112] and Spider [ 107], in 2017 and 2018, respectively. These are the ﬁrst large-scale, multi-domain benchmarks that made it possible to train and evaluate neural text-to-SQL systems and provided a common tool to com- pare different systems easily. While other benchmarks have followed, these two remain the most popular ones. Table 1 summarises and compares the two benchmarks. This section provides an overview of various text-to-SQL datasets (summarised in Table 2), covering either a single or multiple domains, as well as the evaluation methodologies for comparing the system predictions to the ground truth. 3.1 Domain-specific text-to-SQL datasets Domain-speciﬁc text-to-SQL datasets focus on one domain and typically include a single database, such as: movies and television series (IMDb [ 98]), restaurant and shop reviews 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,52,caption,Table 1 A comparison of the two most popular text-to-SQL bench-
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,61,caption,Table 2 An overview of text-to-SQL benchmarks and their size in
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,5,page,"A survey on deep learning approaches for text-to-SQL 909 (Yelp [ 98] and Restaurants [ 70,84]), academic research (Scholar [ 42] and Academic [ 53]), ﬁnancial data (Advising [27] and FIBEN [ 77]), medical data (MIMICSQL [ 92]), and questions and answers from Stack Exchange (SEDE [ 34]). Interestingly, these datasets have not seen the same widespread use as WikiSQL or Spider for a number of rea- sons. Since they focus on a single domain, it is not possible to argue that a proposed system can be considered a “univer- sal solution” even if it performs well on a speciﬁc domain. Second, their size is relatively small compared to Spider and WikiSQL, usually not surpassing a thousand examples. Third, most of these datasets do not have a pre-deﬁned train/dev/test split so that systems trained and evaluated on them would be compared fairly to one another. Even though the generalisation capability of a text-to- SQL model is an important challenge, a realistic application would most likely require a text-to-SQL system to work with a single database of a speciﬁc domain, or with a few related databases. In such a scenario, a high performance on a sin- gle domain may be even more important than a cross-domain generalisation capability, and achieving it is very challenging [34]. Furthermore, datasets such as SEDE [ 34], are made specif- ically to reﬂect that SQL queries in real-life scenarios can be very complex and long; having numerical computations, vari- able declarations, date manipulations, and other elements that are not present in the Spider and WikiSQL datasets. SEDE’s authors demonstrate that the state-of-the-art systems which achieve high scores on Spider, do not perform as well on SEDE, proving the necessity for new and more advanced benchmarks. 3.2 Cross-domain text-to-SQL datasets WikiSQL WikiSQL [112] is a large crowd-sourced dataset for developing natural language interfaces for relational databases, released along with the Seq2SQL text-to-SQL system. It contains over 25,000 Wikipedia tables and over 80,000 natural language and SQL question pairs created by crowd-sourcing. Each entry in the dataset consists of a table with its columns, a Natural Language Question (NLQ) and a SQL query. Figure 2 shows an example from the dataset. The complexity of the SQL queries found in WikiSQL is low because each query is directed to a single table and not to a relational database and they are do not use any complex SQL clause such as JOIN, GROUP BY , ORDER BY , UNION, and INTERSECTION. Additionally, WikiSQL does not allow the selection of multiple columns in a single query or the use of the asterisk (*) operator. Consequently, the proposed task is much simpler than the ultimate goal of creating a natural language interface for relational databases. We must also note that WikiSQL contains multiple errors and ambiguities, which might hinder the performance of a model trained on it. Figure 3 demonstrates an example of a table incorrectly copied from Wikipedia that was never- theless used to generate a pair of a NLQ and a SQL query that, ultimately, make no sense. Research even suggests that the state-of-the-art systems have reached the upper barrier of accuracy on the task [ 39]. This is also demonstrated by evaluating human performance on a small proportion of the dataset. Spider Spider [ 107] is a large-scale complex and cross- domain semantic parsing and text-to-SQL dataset annotated by 11 Yale students. It contains 200 relational databases from 138 different domains along with over 10,000 natural language questions and over 5000 SQL queries. Its queries range from simple to hard, using all the common SQL ele- ments, including nesting. These characteristics of the dataset along with its high quality, since it was hand-crafted and re- checked, have led researchers to widely rely on it for building systems that can generate quite complex SQL queries. Other cross-domain datasets Recent cross-domain datasets focus on particular aspects of the text-to-SQL problem. Spider-DK [ 28] extends Spider to explore system capa- bilities at cross-domain generalisation (i.e. robustness to domain-speciﬁc vocabulary across different domains), while Spider-Syn [28] focuses on robustness to synonyms and dif- ferent vocabulary. Both datasets highlight very interesting and important requirements for a text-to-SQL system, and can be used as supplementary benchmarks. SQUALL [80] is based on a previous dataset named Wik- iTableQuestions [67], consisting of NL Questions posed on Wikipedia tables along with the expected answers. In contrast to WikiSQL, there are no structured queries in the WikiTable- Questions dataset. The authors of SQUALL have created the corresponding SQL queries for most of the examples in the WikiTableQuestions dataset, while also providing an align- ment between words in the NLQ and the parts of the SQL query that they refer to. This additional feature could steer more thorough research on the schema linking and schema ambiguity problems (brieﬂy mentioned in Sect. 2 and more thoroughly examined in Sect. 4). Finally, KaggleDBQA [51] is another cross-domain dataset, although of much smaller size, that has been extracted from Kaggle and features real-world databases taken from the Web, having all the peculiarities of a DB that are missing from Spider, whose DBs were created speciﬁcally for bench- marking text-to-SQL systems. KaggleDBQA also includes documentation and metadata for its DBs, posing an inter- esting research question of how this additional information could be used to improve the system performance. 3.3 Evaluation metrics Having a ground truth SQL query for each NLQ enables us to train and evaluate a deep learning text-to-SQL system on 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,6,page,"910 G. Katsogiannis-Meimarakis, G. Koutrika Fig. 2 An example from the WikiSQL dataset Fig. 3 An incoherent example from the WikiSQL dataset it. In this section, we will present metrics used to evaluate a text-to-SQL system’s predictions. String matching (introduced as Logical Form Accuracy [112]) is the simplest accuracy metric for text-to-SQL. It considers the ground truth and predicted queries as simple strings and checks whether they are identical. A match is only found when the predicted query is written exactly as the ground truth, without taking into account that many parts of a SQL query can be written in a different order or even in a different but still equivalent way. Execution accuracy [107,112] (or Query Accuracy [ 11]) is another simple approach for comparing SQL queries. For each NLQ, both the ground truth and the predicted queries are executed against the corresponding database (or table) and their results are compared. If the results are the same, then the prediction is considered correct. False positives can occur when both queries return the same results, but are different on a semantic level (e.g. when they return empty results or when an aggregation function is applied to different columns that happen to return the same result). Component matching [107] is proposed in order to obtain a better understanding of which parts of the SQL query are predicted correctly. For example, we might consider the SELECT column accuracy, i.e. the percentage of the pre- dicted queries that have the same columns in the SELECT clause as the corresponding ground truth queries. For some parts, a more sophisticated approach might be necessary to avoid incorrect classiﬁcations. For instance, when compar- ing the conditions of the WHERE clause, their order should not be taken into account. Exact set matching [107] (or Query Match Accuracy [ 96]) considers all the possible component matches and classiﬁes a prediction as correct if all component matches are cor- rect (e.g. aggregation function, condition operators, SELECT columns, etc.). Exact set match without values is a category in the Spider [107] dataset, that works in the same way as exact set match- ing, but does not take into account if the values that appear in the predicted query are the same as the ones that appear in the gold query. The reason for this simpliﬁcation is that predicting the correct values can be very challenging, espe- cially when these values appear in the NLQ differently to the way they are stored in the DB (e.g. the word “Greek” might imply a condition such as country=“Greece”). Although this metric might be considered as common practice in the Spi- der benchmark, as research shows [ 27], disregarding values during evaluation removes an important challenge of the text- to-SQL problem. Sub-tree elements matching (or Partial Component Match F1—PCMF1) [ 34] is a metric proposed to avoid a score of zero by the exact set match metric, when some parts of the predicted query are correct. It considers parts of the query such as the SELECT, WHERE and FROM clauses and it calculates the F1 score of each clause based on the precision and recall of the predicted attributes in the clause. The ﬁnal PCMF1 score of a predicted query is the average F1 score of all the considered query parts. For example, in large queries, the system might predict a large part of the query correctly and make some errors in the WHERE clause. While the exact match metric would assign a score of zero even for a small mistake, the PCMF1 metric would assign a score relatively close to one, thus providing a better assessment of the system performance. 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,2,caption,Fig. 2 An example from the WikiSQL dataset
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,3,caption,Fig. 3 An incoherent example from the WikiSQL dataset
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,7,page,"A survey on deep learning approaches for text-to-SQL 911 A more thorough methodology for evaluating the seman- tic equivalence of two SQL queries has been proposed by [47], but has yet to be adopted by any deep learning systems. This approach starts by comparing the execution result of the two queries, as well as their results on additional generated data, in case the original database contains a small amount of data. Furthermore, a prover is used to provide a proof of equivalence between the queries or a counter example in the case of non-equivalence. If the prover cannot work for the given queries, then a query re-writer is applied on both queries and the re-written queries’ parse trees are compared. If the re-written parse trees are structurally identical then the queries are semantically equivalent, otherwise the queries are manually evaluated by an expert. While this approach could detect matches even if queries are expressed in fundamen- tally different ways, the requirement of manual labour as well as the extra processing requirements it presents, are some of the reasons why it has not seen widespread use yet. What metric each system is using greatly depends on the dataset that each system is created for and aims at entering its leaderboard. 2,3 Speciﬁcally, systems that are built for the WikiSQL dataset, use Logical Form Accuracy and Execution Accuracy, while systems built for the Spider dataset use Exact Set Matching without V alues and Execution Accuracy. This strongly indicates the inﬂuence that benchmark creators have on the evaluation strategy of text-to-SQL systems. It also highlights the responsibility of the next benchmark creators to address the problems of current metrics and include more thorough evaluation metrics. 4 Taxonomy Despite the fact that deep learning approaches have only recently become popular for the text-to-SQL problem, numerous systems have already been proposed, that bring a wide variety of novelties and employ different approaches. Nevertheless, there are key parts that serve common purposes across almost all systems, which allow us to build a general model that can help us better understand them. Hence, the goal of this section is to present an overview of the most important parts of neural text-to-SQL systems as well as a taxonomy of the possible choices in each part. Figure 4 shows an overview of a neural text-to-SQL sys- tem. The main input of a text-to-SQL system is a NL query (NLQ) and the database (DB) that the NLQ is posed on. The ﬁrst step, whenever employed, is schema linking, which aims at the discovery of possible mentions of database elements (tables, columns and values) in the NLQ. These discovered 2 https://yale-lily.github.io/spider. 3 https://github.com/salesforce/WikiSQL. schema links, along with the rest of the inputs, will be fed into the neural network that is responsible for the translation. The core of this neural network consists of two main parts: the encoder and the decoder. The encoder takes one or more inputs of variable shapes and transforms them into one or more internal representations with ﬁxed shapes that are consumed by the decoder. Additionally, the encoder usu- ally infuses the representation of each input with information from the rest of the inputs, so as to create a more informed representation that better captures the instance of the prob- lem at hand. The decoder uses the representations calculated by the encoder and makes predictions on the most probable SQL query (or parts of it). Given that the inputs (NLQ, DB, schema links) are mainly textual, natural language representation is responsible for creating an efﬁcient numerical representation that can be accepted by the encoder. Input encoding is the process of further structuring the inputs in a format that can be accepted by the encoder, as well as the choice of an appropriate encoder network for processing them and producing an inter- nal hidden representation. Finally, output decoding consists of designing the structure of the predictions that the network will make, as well as choosing the appropriate network for making such predictions (e.g. a SQL query can be viewed as a simple string, or as a structured program which follows a certain grammar). While some systems perform the NL representation and encoding steps separately (e.g. a repre- sentation based on word embeddings which is then encoded by a LSTM), in some cases, they can be almost indistin- guishable (e.g. when using BERT [ 19]). It is even possible for all three steps to be merged into one (e.g. when using the T5 encoder–decoder pre-trained language model [ 74]). Finally, the neural training refers to the procedure followed for training the neural network. The last dimension of the taxonomy is the output reﬁne- ment, which can be applied during the decoding phase in order to reduce the possibility of errors and to achieve better results. Note that even though output reﬁnement is closely related to output decoding and even interacts with the decoder, it is not a part of the neural network. As such, in most cases, it is possible to add or remove an output reﬁne- ment technique once the system has been created and trained. 4.1 Schema linking To better grasp the concept of schema linking, let us think of how a human, asked to write a SQL query from a NLQ, would start by looking at the underlying database and by trying to identify how the entities mentioned in the NL are stored in the database. In other words, they would attempt to link parts of the NLQ to the database elements they are referring to. Intuitively, a text-to-SQL system could beneﬁt by doing the same when translating a NLQ. 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,43,caption,Figure 4 shows an overview of a neural text-to-SQL sys-
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,8,page,"912 G. Katsogiannis-Meimarakis, G. Koutrika Fig. 4 Overview of a neural text-to-SQL system, based on the proposed taxonomy More formally, schema linking is the process of discover- ing which parts of the NLQ refer to which database elements. The NLQ parts that could possibly refer to a database element are called query candidates , while the database elements that could occur in the NLQ are called database candi- dates. Query candidates can be words or phrases, while database candidates can be tables, columns, and values in the database. A connection between a query candidate and a database candidate is called a schema link, which can be fur- ther categorised as a table link or column link, when the query candidate maps to a table name or column name, respectively, and value link , when it matches a value of a column. Schema linking is very challenging for a variety of rea- sons. Query and database candidates may not use the same vocabulary nor appear in the exact same phrasing. For exam- ple, the phrase “sang by” in the NLQ might refer to the database column “singer” (same word stem, phrased differ- ently) or “artist” (vocabulary mismatch). This problem is even more challenging when the NLQ expresses a condition (i.e. a reference to a DB value) in a different way than how the value is stored in the DB. This is an issue because in contrast to the table and column names of the DB, the sheer volume of data stored in a DB prohibits using all DB val- ues as inputs to the system, making it very challenging for the system to build the correct SQL condition. For exam- ple, the word “female” might imply a condition such as “gender=F” . In this case, besides a schema link between “female” and the column “gender” , the system must also be given the value as it is stored in the DB ( “F” )a sp a r to ft h e input, in order to use it when constructing the SQL predic- tion. Otherwise, it will most likely produce a condition like “gender=female” , which would return no rows. Due to the volume of a DB, ﬁnding value links is not only hard but can be very computation-expensive. The schema linking process has two parts. Candidate dis- covery is the process of extracting query candidates from the NLQ and database candidates from the underlying database. Candidate matching is the process of comparing a set of query candidates and a set of database candidates and estab- lishing the links. Schema linking enhances the input, and a system can operate without it. Hence, performing no schema linking is possible too. In fact, while most recent systems incorporate some form of schema linking in their workﬂow, earlier ones (e.g. Seq2SQL [ 112], SQLNet [ 96]) and even some recent ones (e.g. HydraNet [62], T5+PICARD [76], SeaD [97]) sim- ply rely on their neural components to make predictions. 4.1.1 Query candidate discovery We ﬁrst walk through the techniques used for discovering query candidates. Single tokens A simple approach for ﬁnding query candi- dates is to consider all the single words of the NLQ as query candidates. This is obviously prone to errors as it is likely that a query candidate spans over multiple tokens (e.g. “New York”, “Iggy Pop and the Stooges” ). Multi-word candidates To ﬁnd all possible query can- didates, even multi-word ones, it is necessary to consider n-grams of varying length. For example, IRNet [ 33]u s e sa l l n-grams of length from 1 to 6 in the user question as query candidates. It processes them in descending order of length and if a n-gram is marked as a schema link, the system dis- cards all the smaller n-grams that are contained in it, to avoid generating duplicate links. Furthermore, IRNet [ 33] assumes that any phrase (n-gram) appearing inside quotes must be a reference to a value stored inside the database. Note that in this case, the system not only discovers a query candidate, but also asserts that the database candidate that will be linked to it must be a value. Named entities V alueNet [10] adds an extra step for intel- ligent candidate discovery, by performing Named Entity Recognition (NER) on the user’s NLQ to discover possible query candidates. This technique is very effective in discov- ering candidates that refer to a widely known entity such as a place or a person but might not generalise to entities that are speciﬁc to a certain domain. V alueNet asserts that candidates discovered through NER refer to a DB value, i.e. the DB can- didate they will be matched to, must be a value. TypeSQL 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,2,caption,"Fig. 4 Overview of a neural text-to-SQL system, based on the proposed"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,9,page,"A survey on deep learning approaches for text-to-SQL 913 [103] uses the Freebase 4 Knowledge Graph to perform NER. It searches for ﬁve types of entities, namely: Person, Place, Country, Organization and Sport . However, the query candi- dates that are found to be Named Entities are not matched to a DB candidate, but simply marked with the entity type that describes them. Additional candidates As mentioned earlier, creating cor- rect conditions can be even more challenging when the value is not expressed in the NLQ exactly as it is stored in the DB. V alueNet [10] proposes an improved pipeline for generat- ing additional candidates for value links that consists of: ( a) identifying possible query candidates using NER, ( b) gener- ating additional candidates by looking up similar values in the database and by using string manipulation, and ( c) vali- dating all the generated candidates by conﬁrming they appear in the database. The validated candidates are then given to the system, to aid it in generating correct conditions. Let us consider the following example, where the NLQ contains the phrase “New York” , but the DB contains the value “NY” . V alueNet would recognise “New York” as a named entity, it would generate additional similar candidates (e.g. “N. York” , “N.Y.” and “NY” ) and it would look them up in the DB. Doing so, it would discover that only “NY” appears in the DB, and would only add this value in the input to help the system create a correct condition (e.g. “state=NY” ). 4.1.2 Database candidate discovery Table and column names The ﬁrst and most obvious source for database candidates are the names of the tables and columns of the database. Given that most databases contain a relatively small number of tables and columns, all of them can be database candidates. V alues via lookup V alues stored in the database comprise another large pool for database candidates. However, due to the volume of data, iterating over all the DB values is not performance-wise. Indexes have been widely used in ear- lier text-to-SQL systems, which do not rely on deep learning [36,53], to accelerate the search. V alueNet [ 10]a l s ou s e s indexes and computationally cheap methods for retrieving values from the DB. It is necessary to note that a database lookup requires the use of an already discovered query can- didate. In order to avoid greedily looking up all the query candidates, the system might only look up certain query can- didates that seem more likely to refer to a value (e.g. because they are found inside quotes or based on heuristics). V alues via knowledge graphs IRNet [ 33] assumes that access to the database contents is not possible and employs the knowledge graph ConceptNet [ 82] for recognising value links. As a ﬁrst step, IRNet considers that all n-grams begin- ning and ending with single quotes are query candidates 4 https://developers.google.com/freebase. referring to values. In order to discover the DB column or table that could contain a value such as the discovered query candidate, the system searches each candidate in the knowl- edge graph and only keeps two types of results: is-type-of and related-terms. For example, when searching for “New York” in ConceptNet, one of the returned results is is-type-of “state” . This result helps IRNet link “New York” to a column named “state” or similarly. Note that this approach stands out from what has been discussed so far, in the way that a value link is discovered using an intermediate candidate (knowledge graph result) and the column names. 4.1.3 Candidate matching Having discovered the query and database candidates, an efﬁcient method is needed for comparing them to identify possible links. As discussed earlier, candidates are not always expressed in the same way in both sides, so identifying links is not straightforward. Techniques that can recognise semantic similarities between candidates are required. Exact and partial matching The simplest approach is to look for exact and partial matches, as it is done by IRNet [33]. An exact match requires that the candidates are iden- tical, while a partial match occurs when one candidate is a substring of the other. Admittedly, this approach is bare- bone and while it can discover more obvious links, it can also result in false positive matches when candidates share the same words (e.g. “residence” would be considered a par- tial match with “former residence” ). Fuzzy/approximate string matching Another useful tech- nique for identifying matches when the link in the candidates are written differently is approximate string matching. An example of such an approach is the Damerau–Levenshtein distance [ 16], used by V alueNet [ 10]. While such tech- niques aid at identifying matches with different spelling or spelling mistakes (e.g. “color” -“colour” ), they cannot han- dle synonyms and thus are not robust to the use of different vocabulary. Learned embeddings To calculate the similarity between words of the NLQ and schema entities, an earlier work in the area of semantic parsing [ 49] proposes the use of learned word embeddings. The system learns word embeddings using the words of the text-to-SQL training corpus and combines them with additional features that are calculated using NER, edit distance and indicators for exact token and lemma match. These embeddings are then used to calculate the similarity of query candidates to DB candidates. While this approach is more expensive than previous matching techniques, it allows for much more ﬂexible and intelligent matching. This approach was also adopted by text-to-SQL systems [ 8,9]a s well. 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,10,page,"914 G. Katsogiannis-Meimarakis, G. Koutrika Classiﬁers Given the complexity of schema linking, it may be possible to achieve better results by training a model to perform schema linking. A Conditional Random Field (CRF) model [ 50] can be trained on a small group of hand-labelled samples to recog- nise column links, table links and value links for numerical and textual values [ 11]. The predictions of this model can then be passed to the main neural network of the text-to- SQL system along with the rest of the inputs. DBTagger [ 86] uses a similar approach to solve the schema linking problem as a sequence tagging problem. It employs CRFs on every token of the NLQ to identify: ( a) its Part of Speech (POS), ( b) schema link type (e.g. table link, value link, etc.), and ( c)t h e speciﬁc schema element that it refers to. The authors argue that learning these three tasks in a multi-learning paradigm helps the system achieve better performance than it would if it only learned to identify the schema element each token refers to. The SDSQL [ 38] system is simultaneously trained on two tasks: ( a) the text-to-SQL task, similarly to all sys- tems, and ( b)t h e Schema Dependency Learning task. For this additional learning task, the system is essentially trained to discover schema links in the form of dependencies between the words of the NLQ and the parts of the SQL query. Namely, the possible dependencies are: select-column (S- Col), select-aggregation (S-Agg), where-column (W-Col), where-operator (W-Op) and where-value (W-V al). For exam- ple, a select-column (S-Col) label is assigned to the depen- dency between the column appearing in the SELECT clause and the word of the NLQ that refers to it. A deep biafﬁne network [ 23,25] is trained along the rest of the system to detect the existence and type of these dependencies. Train- ing data for this task is created from the already available NL and SQL pairs, by assigning dependency labels between the NLQ tokens and table columns. Although the schema links discovered by the system are not directly used for predict- ing the SQL query, training for both tasks simultaneously has a positive effect on the system performance. This task goes beyond the schema linking task, as some of the afore- mentioned dependencies include query candidates that might refer to query parts (e.g. aggregation functions and condition operations). It should also be noted that this approach has been applied to WikiSQL, but it has not yet been extended to the more challenging Spider dataset. Neural attention While attention layers do not directly determine a match, we mention them brieﬂy because of their capability to highlight connections between query and DB candidates, which can improve the system’s internal repre- sentation and boost its performance. SQLNet [ 96]w a st h e ﬁrst system to introduce such a mechanism, named Col- umn Attention , that processes the NLQ and column names and ﬁnds relevant columns for each word of the NLQ. The Transformer [ 87] neural architecture, which is based on an attention mechanism, has been instrumental to the widespread use of PLMs that have become the go-to solu- tion for input encoding, greatly beneﬁting the accuracy of text-to-SQL systems. Finally, RA T-SQL [ 89] proposed a modiﬁed Transformer layer, called Relation-Aware Trans- former (RA T), that biases the attention mechanism of the Transformer towards already-known relations from the DB schema and discovered schema links. 4.2 Natural language representation An essential step for text-to-SQL systems is creating and pro- cessing numerical representations of their NL inputs. Until recently, the most popular technique for NL representation has been pre-trained word embeddings. Recent advances in NLP , such as the introduction of the Transformer architecture [87] followed by its use to create large Pre-trained Language Models (PLMs), has tipped the scales greatly to its favour. Additionally, as new PLMs are emerging, a new research path is being paved focusing on the design of better PLMs or PLMs created speciﬁcally for certain problems (such as the text-to-SQL problem). 4.2.1 Word embeddings Word embeddings aim at mapping each word to a unique numerical vector. While there are simplistic approaches for creating such vectors (e.g. one-hot embeddings), more advanced algorithms [ 65,68] aim at making the value of each vector meaningful. These vectors are usually trained from a large text corpus (e.g. Wikipedia or Twitter) using a self-supervised algorithm that is mainly based on word co- occurrences. The set of pre-trained vectors can then be used to build a model that beneﬁts from the inherent knowledge that is present in the vectors due to their training. For example, the GloV e [ 68] embeddings, which capture interesting word relationships, were frequently used by the ﬁrst text-to-SQL systems. Such word relationships include words with similar meaning being near neighbours and lin- ear substructures that indicate similar relationships between words (e.g. the distances between the word pairs Paris-France and Athens-Greece will be similar because these words share a capital-country relation). A pre-trained set of GloV e embed- dings can be used to create numerical representations for NL inputs of a model, which can then be encoded using a RNN (such as a LSTM). 4.2.2 Pre-trained language models The introduction of the Transformer architecture [ 87] and its use in PLMs such as BERT [19] has led to a great performance boost in many NLP problems. The text-to-SQL problem is no exception, as the use of PLMs has quickly become the go-to 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,11,page,"A survey on deep learning approaches for text-to-SQL 915 solution for NL representation. In order to understand how a PLM can be used in a text-to-SQL system, it is ﬁrst necessary to highlight the difference between two main categories of PLMs: ( a) encoder-only and ( b) encoder–decoder models. Encoder-only models, like BERT [ 19], RoBERTa [ 59], and TaBERT [ 101], take a sequential input and produce a contextualised numerical representation for each input token. The term “contextualized” marks a notable difference to word embedding techniques, which map each word to a ﬁxed vec- tor, while the representations given by PLMs are computed taking all tokens of the input into account. This representa- tion can then be used by additional neural layers to make a prediction for the downstream task at hand. While GloV e representations can be seen as improved word embeddings and can be used in similar fashion (e.g. using an LSTM), this is not necessary. In fact, due to the robustness of PLMs, it is possible to process their outputs using very simple and small neural networks and still achieve better results than complex networks using word embeddings. Encoder–decoder models, like T5 [ 74] and BART [ 52], are full end-to-end models that take a sequential text input and return a sequential text output (seq-to-seq). These models produce the ﬁnal output on their own, without the need for any extra neural layers, and can be used on any downstream task as long as the expected output can be modelled as a text sequence. Furthermore, as such models are gaining more atten- tion, the creation of task-speciﬁc PLMs is becoming a new research area of its own. Such models can be customised to work with different types of inputs and perform better on less generic tasks, such as the text-to-SQL task. There are multiple PLMs, such as GraPPa [ 104] and TaBERT [ 101], that have been designed to work with structured and tabular data as well as to better generalise in tasks that use SQL, and they can improve the performance of a text-to-SQL system when used in place of a generic PLM. It must also be noted that while most text-to-SQL systems are originally proposed with BERT [19] or another general-purpose PLM, they often manage to achieve higher scores by replacing it with a PLM, such as TaBERT [ 101], that was speciﬁcally pre-trained for a task that uses structured data, like the text-to-SQL task. 4.3 Input encoding The dimension of input encoding examines how the input is structured and fed to the neural encoder of the system, so that it can be processed effectively. There are different inputs that are useful for translating a NLQ to SQL. The NLQ and the names of the DB columns and tables could be consid- ered the minimum required input. Other features that could improve the network performance include: ( a) the relation- ships present in the DB schema, including primary-to-foreign key relationships and relationships between columns and tables, and ( b) links and additional values that have been discovered during the schema linking process. The use of neural networks mandates the transformation of all inputs into a form that can be accepted by the network. This can be very restrictive, given how heterogeneous these types of inputs are and how difﬁcult it is to represent them all in a single type of input. In this section, we examine the most representative choices for input encoding, while also tak- ing into account the additional features that each choice can incorporate. We distinguish four encoding schemes: ( a) sep- arate NLQ and column encodings ( b) input serialisation ( c) encoding NLQ with each column separately, and ( d) schema graph encoding. A schematic overview of the possible encod- ing choices can be seen in Fig. 5. 4.3.1 Separate NLQ and column encodings A ﬁrst approach, used mostly by earlier systems (e.g. Seq2SQL [ 112], SQLNet [ 96]), is to encode the NLQ sepa- rately from the table columns. The main reason for encoding the two inputs separately is the shape mismatch between them; while the NLQ is a simple sentence (i.e. a sequence of words), the table header is a list of column names, where each name can contain multiple words, i.e. it is a sequence of sequences of words. In Seq2SQL [ 112], SQLNet [ 96] and IncSQL [ 79], each word (embedding) of the NLQ is fed into a bi-directional LSTM (bi-LSTM) that produces a hidden state representa- tion for each word. For column headers, since each column name can have multiple words, a bi-LSTM is used for each column name, and the ﬁnal hidden state of each column is used as the initial representation for the column. Notice that by keeping only the last state of each column name, the rep- resentation of the header becomes a simple sequence and not a nested sequence. Since the two inputs are encoded sep- arately, they must be combined at some point so that the output is inﬂuenced by both of them. This can be done by using cross-serial dot-product attention [ 61], concatenating the two representations, summing them or using a combina- tion of the above. None of the studied systems that follow this encoding approach use any extra features besides the NLQ and DB columns. This may be attributed to the fact that these are some of the earliest neural text-to-SQL proposals, which did not perform schema linking and focused on the simpler Wik- iSQL dataset. 4.3.2 Input serialisation A different approach is to serialise all the inputs into a single sequence and encode it all at once. This is a very common practice when using PLMs (e.g. BERT [ 19], T5 [74]) that cre- ate a contextualised representation of their input, because if 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,12,page,"916 G. Katsogiannis-Meimarakis, G. Koutrika Fig. 5 An overview of the possible encoding choices. Pink tokens represent words of the NLQ, blue tokens represent database elements and grey tokens are auxiliary tokens (color ﬁgure online) each input were to be encoded separately, the system would not beneﬁt from the PLM’s contextualisation ability. This approach simpliﬁes the encoding process and beneﬁts from the robustness of PLMs. However, it also carries disadvan- tages, such as losing schema structure information and being unable to easily represent relationships between the inputs (e.g. primary-foreign key relationships, schema links, etc.). As we go through some different serialisation approaches, we will also examine how much information can be retained in each case. It should also be noted that PLMs usually employ a few special tokens that are added to the serialised sequence. For example, BERT [ 19] uses the classiﬁcation [CLS] and the separating [SEP] special tokens. The [CLS] token is added at the start of the sequence. Its contextualised out- put, which gathers information from all the tokens in the sequence thanks to the underlying attention networks, can be used to make classiﬁcation predictions that concern the entire sequence. The [SEP] special token can be used to sep- arate different sentences in the same sequence. These tokens are also useful for the text-to-SQL problem. The simplest serialisation technique, used by several sys- tems [35,39,63] that work on the WikiSQL dataset, creates a single input sequence that only contains the NLQ and all the table headers. The serialised sequence starts with the [CLS] token, as is common for BERT, then the NLQ tokens are appended, followed by a [SEP] token marking the end of the NLQ and then each column name is added followed by a [SEP] token. This input is processed by BERT, which cre- ates a contextualised representation that has the same length as the input, and that can be processed by the rest of the network to make predictions. Since these systems only work with single tables, there is not a lot of information that needs to be preserved, but it could be argued that this approach sep- arates the column names much less strictly compared to the separate encoding approach. IRNet [33] (when using BERT) creates an input that starts with a [CLS] token, then continues with the NLQ’s tokens followed by a [SEP] token, the name of each column of the database followed by a [SEP] token, and ﬁnally the table names of the schema, each separated with a [SEP] token as well. In order to encode discovered schema links along with the rest of the input, IRNet uses three extra tokens, namely [Column], [Table], [V alue], that can be appended before a NLQ token or phrase, to mark that it was linked to a database candidate. Still, using this serialisation format, there is a lot of schema information not captured. For example, it is not possible to extract any primary-foreign key relationships, or to which table each column belongs. Finally, BRIDGE [ 57] constructs an input for a PLM that starts with a [CLS] token, followed by the NLQ and a [SEP] token, as well as the tables and column of the DB, where a [T] and [C] token is added before each table and column name, respectively, so as to better preserve each attribute’s role. The difference between IRNet’s and BRIDGE’s use of the special [C]/[Column] and [T]/[Table] tokens is that the former uses them in the NLQ part to indicate a schema link to a column or table, while the latter uses them to indicate that the tokens after a [C] or [T] token are a column or table name, respectively. BRIDGE also uses an extra third token [V] along with a value, after a column name, to mark that this value appears under the column at hand and was discovered as a possible value link to some NLQ candidate. In this case, BRIDGE uses the [V] token in the DB schema part of the input while also appending a value after it, while IRNet uses the [V alue] token in the NLQ part, without providing the actual value. Additionally, all the columns belonging to a certain table are added right after the table’s name in the sequence so as to better preserve the schema structure in this serialised representation. Nevertheless, all relationships between attributes (e.g. primary/foreign keys) are still lost when following this representation. 4.3.3 Encoding NLQ with each column separately HydraNet [ 62] employs a unique approach: it processes the NLQ with each column separately and makes predictions for each column independently. For each table column, a different input is constructed by concatenating the NLQ with the column name and type and the table name. Using this input, the system predicts the probability of the column at hand appearing in the SELECT clause, the probability of the 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,2,caption,"Fig. 5 An overview of the possible encoding choices. Pink tokens represent words of the NLQ, blue tokens represent database elements and grey"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,13,page,"A survey on deep learning approaches for text-to-SQL 917 Fig. 6 System categorisation on the taxonomy dimensions of natural language representation, input encoding, output decoding, neural train- ing and output reﬁnement column appearing in the WHERE clause, the operation that will be used if this column appears in the WHERE clause, and so on. It could be argued that this approach does not allow the system to have a complete view of the problem instance, because the neural network makes predictions for each column separately, without being aware of the rest of the table columns. Nevertheless, HydraNet achieves exceptional performance on the WikiSQL benchmark. This approach does not utilise any additional features (e.g. schema links). However, given that it also serialises its inputs (albeit, only keeping a single column each time), it could draw inspiration from the serialisation techniques described in Sect. 4.3.2 to encode information about schema links. For example, it could append values similarly to BRIDGE [ 57], or use [Table] and [Column] tokens to explicitly mark column and table names in the input NLQ. It should be noted, however, that generalising this approach to a complete relational DB would not be an easy task. First of all, a DB usually has multiple tables, each containing mul- tiple columns, which means that the network would have to make predictions for a much larger number of columns, greatly increasing time complexity for predicting a single SQL query. Furthermore, queries posed on complete DBs often contain JOIN clauses and other operations that depend on more than one entity; as such, processing each column sep- arately becomes very counter-intuitive. Finally, this approach is based on a sketch-based decoder (more in Sect. 4.4), which is hard to extend for complete DBs. 4.3.4 Schema graph encoding A graph is the most effective way for representing the DB elements and their relationships. Representing and encoding the input using a graph is used only by a handful of systems [8,9,89]. Each node in the graph represents a database table or a column, while their relationships can be represented by edges that connect the respective nodes. It is also possible to add the NLQ words as nodes in the graph, and add edges that connect the query candidates with their equivalent database candidates for representing all the discovered schema links. Additionally, the used graph representation may allow for different classes of nodes and edges leading to even higher expressivity. There can be different classes of nodes to distin- guish between tables, columns and NLQ words and different classes of edges to distinguish between edges that represent foreign-primary key relations, edges that indicate a column belonging to a table and edges that represent schema links. Even though representing the system input as a graph allows for minimal loss of information and can include many types of additional inputs, processing a graph with a neural network is far more difﬁcult than processing a sequence. This is the main reason why graphs have yet to see widespread use in the text-to-SQL problem. However, recent advances 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,2,caption,Fig. 6 System categorisation on the taxonomy dimensions of natural
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,14,page,"918 G. Katsogiannis-Meimarakis, G. Koutrika in graph neural networks and the clever use of Transformers [87] proposed by RA T-SQL [ 89] and [ 78], are showing very promising and might be a good choice for future research. 4.4 Output decoding Text-to-SQL systems following the encoder–decoder archi- tecture can be divided into three categories based on how their decoder generates the output [ 13]: ( a) sequence-based, ( b) grammar-based, and (c) sketch-based slot-ﬁlling approaches. 4.4.1 Sequence-based approaches This category includes systems that generate the predicted SQL, or a large part of it, as a sequence of words (comprising SQL tokens and schema elements) [ 11,57,112]. This decod- ing technique is the simplest, and was adopted by Seq2SQL [112], which is one of the ﬁrst deep-learning text-to-SQL sys- tems. Later systems steered away from sequence decoding because it is prone to errors. The main drawback of sequence decoding is that it treats the SQL query as a sequence that needs to be learnt, and at prediction time, there are no measures to safeguard from producing syntactically incorrect queries. When generating a query, it does not take into account the strict SQL grammat- ical rules, nor does it actively prevent generating incorrect column and table names that do not exist in the DB. Nevertheless, sequence-based approaches are starting to be used again and are proving to be very efﬁcient thanks to two advances: ( a) the introduction of large pre-trained seq- to-seq Transformer [ 87] models (e.g. T5 [ 74], BART [ 52]) and (b) the use of smarter decoding techniques that constrain the predictions of the decoder and prevent it from producing invalid queries (e.g. PICARD [ 76]). 4.4.2 Sketch-based slot-ﬁlling approaches Systems in this category [ 35,39,62,63,96,103] aim at simpli- fying the difﬁcult task of generating a SQL query to the easier task of predicting certain parts of the query, such as predict- ing the table columns that appear in the SELECT clause. In this way, the SQL generation task is transformed into a clas- siﬁcation task. In particular, we consider a query sketch with a number of empty slots that must be ﬁlled in, and develop neural networks that predict the most probable elements for each slot. A basic prerequisite for such approaches is to have a query sketch that, when completed, will be able to capture the NLQ’s intention. While dividing the text-to-SQL problem into small sub- tasks makes it easier to generate syntactically correct queries, sketch-based approaches may have two drawbacks. Firstly, the resulting neural network architecture may end up being quite complex since dedicated networks may be used for each slot or part of the query. Furthermore, it is hard to extend to complex SQL queries, because generating sketches for any type of SQL query is not trivial. 4.4.3 Grammar-based approaches Systems using a grammar-based decoder [ 13,22,33,79,89] are an evolution of sequence-to-sequence approaches, and produce a sequence of grammar rules instead of simple tokens in their output. These grammar rules are instructions that, when applied, can create a structured query. The most often used grammar-based decoders by text- to-SQL systems have been previously proposed for code generation as an Abstract Syntax Tree (AST) [ 99,100]. These models take into account the grammar of the target code lan- guage (in our case, the SQL grammar) and consider the target program to be an AST, whose nodes are expanded at every tree level using the grammar rules, until all branches reach a terminal rule. When it reaches a terminal rule, the model might generate a token, for example, a table name, an operator or a condition value, in the case of text-to-SQL. The decoder uses a LSTM-based architecture that predicts a sequence of actions, where each action is the next rule to apply to the program AST. Because the available predictions are based both on the given grammar and the current state of the AST, the possibility of generating a grammatically incorrect query is greatly reduced. Grammar-based approaches are considered the most advantageous option for generating complex SQL queries, as sequence-based approaches were too prone to errors and sketch-based approaches are difﬁcult to be extended to com- plex queries. While their status is recently being challenged by the advances of sequence-based decoders discussed ear- lier, the quest for the most effective decoding technique is far from over. 4.5 Neural training Another dimension that must be examined when consider- ing a neural text-to-SQL system is the methodology that is followed to train it. Even though the description of a system is usually focused around its architecture and neural layers as well as the way it encodes the inputs and decodes the output, the dimension of neural training is important, because it is the process that enables the neural network to learn how to perform the task at hand. Earlier systems adopted the simple paradigm of training the network exclusively on a text-to-SQL dataset, however, recent systems have proposed more sophisticated approaches that can greatly beneﬁt the network performance and its gen- eralisation capabilities. 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,15,page,"A survey on deep learning approaches for text-to-SQL 919 Fresh start The most common approach is to train the network from scratch, i.e. initialise all the weights with a ran- dom initialisation algorithm and train them on a downstream task. However, recent developments in the domain of NLP are showing that pre-trained networks and self-supervised learning are able to achieve much better performance. Transfer learning The use of transfer learning is quickly gaining ground in the NLP community, due to the introduc- tion of Transformers [ 87], which greatly reduce training time compared to RNNs. Transfer learning refers to when a model trained on a different, usually more generic task, and a differ- ent dataset, is incorporated to a new model and further trained on a downstream task (e.g. text-to-SQL). Language models, i.e. networks that have been trained to predict missing words or phrases on huge text corpora, are becoming the standard approach for most NLP tasks, given the performance boost they provide in almost all cases. Some systems, such as HydraNet [ 62], rely on language models almost completely, only using linear output layers to produce predictions. Most systems however, incorporate language models as an alternative or an enhancement for word embeddings and RNNs. Additional objectives Another interesting approach that follows the success of language models and self-supervised learning is that of using additional self-supervised tasks while training for the text-to-SQL problem. Recent research [12,38,97] suggests that training neural models for more generic tasks besides the downstream task of text-to-SQL that the model is designed to solve, can improve performance on the downstream task. When using additional objectives, one must decide whether the model should be trained on all the auxiliary objectives along the downstream task or whether it should be ﬁrst trained on the auxiliary tasks and then ﬁne- tuned on the downstream task. – Erosion The erosion task, proposed by [ 97], consists of randomly permuting, removing and adding columns to the input schema and training the model to produce the correct SQL query using the eroded schema. Addition- ally, the system must learn to produce an unknown token when it has to use a column that has been removed from the given schema. – Shufﬂing The shufﬂe task, proposed by [ 97], randomly changes the order of schema entities and condition values in the input SQL query and NLQ, training the model to correctly re-order them. – Graph pruning The graph pruning task, proposed by [ 12], trains the model to prune all the nodes of the input graph representation that are irrelevant to the given NLQ. – Schema dependency learning SDSQL [ 38] proposes an additional task to the text-to-SQL task, that closely resonates to the schema linking problem. SDSQL is designed for the WikiSQL dataset. Schema Dependency Learning consists of predicting which words or phrases of the NLQ have a dependency to which columns of the table and the type of the dependency that connects them. The goal is to learn which parts of the NLQ signify that a speciﬁc column will appear in the SQL query and the role that the column will have in it (e.g. if it appears in the SELECT clause, if it implies the use of the MAX aggregation function, etc.). Pre-training speciﬁc components Another approach is to train speciﬁc parts of our network so that they can better adjust to the peculiarities of the task. For example, GP [ 111] proposes a framework that pre-trains the system decoder, before training the entire system, in order to better train it on the context-free parts of the SQL grammar, e.g. SQL queries always start with SELECT, the FROM clause is second, and so forth. For this purpose, the encoder’s semantic information is replaced by zero vectors so that the decoder is pre-trained without any information about the particular NLQ. 4.6 Output refinement Once trained, a neural model can be used for inference. There is one last dimension to consider; that of output reﬁnement, i.e. additional techniques that can be applied on a trained model to produce even better results, or to avoid producing incorrect SQL queries. None An obvious approach is to use the trained model as is, without output reﬁnement. The most important reason for this approach concerns time and resource availability; in some applications, it might be crucial to achieve low latency responses or to run on everyday machines. For example, PICARD [ 76], increases inference time by 0.6s when running on a machine with very high-end GPU and arguably even more so on a personal computer. It must be noted however that almost all leader-board entries that achieve high results, use some reﬁnement technique. Execution-guided decoding This is a mechanism [ 91] that helps prevent text-to-SQL systems from predicting SQL queries that return execution errors. Even though sketch- based approaches are designed to avoid syntactical errors, the possibility for semantical errors is ever-present. Some exam- ples of such errors include aggregation functions mismatches (e.g. using A VERAGE on a string type column), condition type mismatches (e.g. comparing a ﬂoat type column with a string type value), and so forth. To avoid these type of errors, execution-guided decoding can execute partially complete SQL queries at prediction time and decide to avoid a certain prediction if the execution fails or if it returns an empty out- put. Execution-guided decoding is system-agnostic and can be applied to most sketch-based systems (e.g. HydraNet, IE- SQL), increasing their accuracy in almost all cases. Let us note that even though some systems presented in this work 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,16,page,"920 G. Katsogiannis-Meimarakis, G. Koutrika might not be proposed using execution-guided decoding in their original paper, they are subsequently shown to perform better in the WikiSQL leaderboard when using it. For this reason, they are shown to use execution-guided decoding in Fig. 6 and Table 3. Constrained decoding While generative models with sequence-based outputs are becoming more powerful for NL generation, they are clearly prone to errors when it comes to generating structured language like SQL. PICARD [ 76] proposes a novel method for incrementally parsing and con- straining auto-regressive decoders, to prevent them from producing grammatical or syntactical errors. For each token prediction, PICARD examines the generated sequence so far along with the k most probable next tokens and dis- cards all tokens that would produce a grammatically incorrect SQL query, use an attribute that is not present in the DB at hand, or use a table column without having its table in the query scope (i.e. not having the appropriate table in the FROM clause). Using PICARD, a seq-to-seq pre-trained transformer model (T5-3B [ 74]) has managed to reach the top of the SPIDER leader-board, lifting the barriers of using sequence-based decoders for text-to-SQL. It should be noted that while PICARD could be considered as the most sophis- ticated constrained decoding technique, other systems with sequence-based decoders have proposed similar decoding techniques to avoid errors. Some examples of such systems are SeaD [ 97] and BRIDGE [ 57]. Discriminative re-ranking The Global-GNN parser [ 9] proposes an additional network that re-ranks the top-k predictions of the main text-to-SQL network and is trained separately from it. The discriminative re-ranker net- work takes into account the words of the NLQ and the database elements used by each of the k highest-conﬁdence SQL predictions, by the text-to-SQL network, and re-ranks them based on how relevant it believes they are. Its authors argue that while the text-to-SQL network usually predicts the correct structure for the target SQL query, it might not always predict the correct columns, tables and aggregation functions, because each of them is predicted only knowing already predicted elements and not future predictions. On the other hand, the re-ranker can look at the completed predic- tions and judge the use of each database element in hindsight, thus improving the prediction quality. 5 Neural architecture Neural architecture refers to the building blocks used to create all neural parts of the system. This section examines the types of neural layers used by text-to-SQL systems and analyses the roles and functions that each one of them is often used for. Linear networks Linear (or Dense) Neural Networks are often used as output layers for sketch-based decoders or to process an internal representation. Given that this type of neu- ral layer is not suited for processing data in a sequence format, they are not effective at processing input such as a NLQ, or producing output in a sequence format (e.g. in a sequence or grammar-based decoder). In sketch-based decoders, how- ever, where the network must predict the correct choice for a certain slot, linear layers are the best suited option to perform this classiﬁcation task (i.e. choose the best option for ﬁlling a slot out of all the available options). Recurrent neural networks Recurrent neural networks (RNNs) have long been considered the go-to solution for NLP , only to be recently dethroned by the powerful Trans- formers. The main advantage of RNNs is their ability to ( a) effectively process series inputs, such as a NLQ, which is a series of words, and ( b) to generate a series output, such as the condition value of a WHERE clause, or a series of gram- mar rules that can generate a SQL query. Well-known RNN architectures include the LSTM (Long Short-Term Memory) and the GRU (Gated Recurrent Unit). The LSTM is popular for NLP tasks and most often used in text-to-SQL systems. Early systems, such as Seq2SQL [ 112] and SQLNet [ 96], relied on LSTMs for input encoding (along with pre-trained word embeddings), but this type of use is now outperformed by pre-trained Language Models. Even though the recent success of Transformers and Language Models has greatly reduced the use of RNNs in the input encoding phase, RNNs are still being used to assist LMs in input encoding and to generate non-NL series outputs. For example, IRNet [ 33] uses BERT to encode the input NLQ and schema but also employs LSTMs to create single-token representations for columns and tables with more than one word in their name (and more than one token to represent them). RNNs are also often used for generating a series out- put. For example, Seq2SQL [ 112] and SQLNet [ 96]e m p l o y pointer networks [ 88] comprised of LSTM layers that gen- erate the entire WHERE clause or the condition value of the WHERE clause, respectively. Another case of RNNs for out- put generation is seen in systems (e.g. IRNet [ 33], RA T-SQL [89]) that employ a grammar-based decoder that generates an SQL query as an abstract syntax tree, leveraging work in semantic parsing [ 99] that uses LSTMs. Transformers In text-to-SQL systems, Transformers are commonly used in Transformer-based Pre-trained Language Models for input encoding, to create a contextualised rep- resentation of the input text. Pre-trained Language Models offer more robust representations and greatly improve the model performance almost all of the times, making them more preferable than pre-trained word embeddings. To use them for input encoding, one can simply replace the input encoder (e.g. word embeddings and LSTM) with a model like BERT. 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,6,caption,Fig. 6 and Table 3.
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,17,page,"A survey on deep learning approaches for text-to-SQL 921 There have been also other, rarer uses of Transformers in text-to-SQL systems. For example, HydraNet is a system completely reliant on a pre-trained language model. In this case, the text-to-SQL problem is formulated so that it matches the pre-training logic of a language model and only very simple linear networks are used to make predictions using the contextualised representations created by the Language Model. Another unique example is RA T-SQL [ 89], which uses speciﬁcally modiﬁed Relation Aware Transformers (RA T) to encode its input. What is special about RA T is that they also accept pre-deﬁned relations about the elements of input series, which essentially allows to bias the encoder towards already known relations in the database schema and the user question. A similar approach is used by [ 78] in order to extend the Transformer architecture to support relations between elements of the inputs, in the form of a GNN Sub-layer. This extension of the Transformer allows to encode the input as a graph, where the edges can have different layers, similarly to RA T-SQL; however, its performance is much lower on the Spider benchmark. Conditional random ﬁelds (CRFs) CRFs [50] are a type of discriminative machine learning model that excels at mod- elling relations and dependencies. Because of this capability, CRFs are often used in NLP for labelling tasks such as Part- of-Speech (POS) tagging and Named Entity Recognition (NER). Even though CRFs are rarely used in text-to-SQL, there is a notable mention of a system integrating them in its neural architecture for a speciﬁc sub-task. Namely, IE-SQL [63] employs CRFs tasked with two schema-linking tasks of recognising: ( a) which words in the NLQ are slot men- tions to SQL elements, such as the SELECT column and the WHERE columns, and ( b) ﬁnding slot relations , i.e. group- ing each of the WHERE column mentions with the mentions of operations and values that correspond to them. Both tasks are modelled as labelling tasks, which is why CRFs are a good choice. Convolutional neural networks (CNNs) Convolutional networks are very rarely used for the text-to-SQL task, since they are best suited for processing visual data. One exam- ple of a system using CNNs is RY ANSQL [ 13], which uses CNNs with Dense Connections [ 102], in order to encode the inputs. However, the authors of RY ANSQL demonstrate that replacing this CNN-based encoder with a PLM can greatly improve the model’s performance, making the choice to steer away from CNNs all the more obvious. 6 Systems Having established a taxonomy for deep learning text-to- SQL systems, let us now zoom in on key systems that have introduced novel and interesting ideas and have shaped the area. This section provides insights and explanations on these systems while also grouping them based on impor- tant milestones of this research area. Figure 7 presents a chronological view on deep learning text-to-SQL systems, along with important datasets and language representation advancements that have had a great impact on the domain. While certain systems could obviously ﬁt in multiple sec- tions, this speciﬁc categorisation is based on the novelty introduced by each system at the time of its publishing, its inﬂuence on later systems, as well as the possible importance of each novelty given its capability to address future and open research problems. 6.1 The dawn of an era As mentioned before, the era of deep learning text-to-SQL systems essentially starts with the release of the ﬁrst large annotated text-to-SQL dataset. WikiSQL was released along with Seq2SQL [112], which was one of the ﬁrst neural net- works for the text-to-SQL task and was based on previous work focusing on generating logical forms using neural net- works [21]. The system predicts the aggregation function and the column for the SELECT clause as classiﬁcation tasks and generates the WHERE clause using a seq-to-seq pointer network. The latter part of the system is burdened with gen- erating parts of the query that can lead to syntactic errors, which is its major drawback. A big difference from almost all other systems is that Seq2SQL is partly trained using reinforcement learning. While the aggregation function and SELECT column predic- tors are trained using cross entropy loss, the WHERE clause predictor is trained using a reward function that returns a pos- itive reward if the produced query returns the same results as the ground truth query and a negative reward if the query returns different results or if it cannot be executed due to errors. The reasoning behind using reinforcement learning, even though it generally performs worse than supervised learning, is that the WHERE clause can be expressed in mul- tiple ways and still be correct. To address these problems, i.e. that sequence decoders can produce errors and that reinforcement learning is not ideal, SQLNet [96] proposed using a query sketch with ﬁxed slots that, when ﬁlled, form a SQL query. This sketch can be seen in Fig. 8, and it covers all the queries present in the WikiSQL dataset. Using a sketch allowed the problem to be formu- lated almost entirely as a classiﬁcation problem, since the network has to predict: ( a) the aggregation function between a ﬁxed number of choices, ( b) the SELECT column among a number of columns present in the table, ( c) the number of conditions (between 0 and 4 in the WikiSQL dataset), ( d) the columns present in the WHERE clause (as multi-label classiﬁcation, since they can be more than one), ( e) the oper- ation of each condition among a ﬁxed number of operations 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,18,page,"922 G. Katsogiannis-Meimarakis, G. Koutrika Fig. 7 A timeline of deep learning text-to-sql systems, datasets and language representation techniques Fig. 8 Query Sketch proposed by SQLNet (≤, =, ≥) and ( f ) the value of each condition. Predicting the value is achieved using a sequence generator network, which in this case is only responsible for the value and not for the SQL syntax or grammar, so syntactic mistakes are avoided. Another improvement introduced in SQLNet is the intro- duction of a column attention neural architecture to the network. Given that SQLNet encodes the NLQ and table columns separately, the encoded representation of the NLQ does not have any information on the available columns and thus cannot inform the system on which words in the NLQ are important for generating the correct SQL query. Column attention is an attention mechanism that infuses the NLQ rep- resentation with information about the table columns, so as to emphasise the words that might be more related to the table. Other than that, both systems are similar to each other, using GloV e [68] embeddings for text representation and LSTM networks for encoding them. 6.2 Sketch generation While the use of a sketch greatly simpliﬁes the text-to-SQL problem and makes predictions simpler for neural networks, the complexity of SQL queries the system can generate using a single sketch is restricted. Systems such as Coarse2Fine [22] and RY ANSQL [ 13] have tried to generalise sketch- based decoding, by attempting to not only ﬁll in the slots of a sketch but also generate the appropriate sketch for a given NLQ. Coarse2Fine [22] is a semantic parser that can generate various types of programs, one of which is SQL. Its main highlight is that it decomposes the decoding process into two steps: ﬁrst, it generates a rough (coarse) sketch of the target program without low-level details, and then it ﬁlls this sketch with the missing (ﬁne) details. Its authors argue that a great advantage of this approach is that the network can disentangle high-level from low-level knowledge and learn each one of them more effectively. Unfortunately, this system is only used on the WikiSQL dataset and is not extended to more complex SQL queries, which is not trivial work. In fact, because Coarse2Fine is designed for the WikiSQL dataset, the sketches it generates only differ between them in the number of conditions that appear in the WHERE clause and the operations in each condition. As such, while the idea it proposes might be very interesting, in practice, it essentially achieves generating SQL queries of no greater complexity than what simple sketch-based systems do. RYANSQL [13] is another system that generates the appro- priate sketch before ﬁlling it, but in contrast to the previous, it manages to produce much more complex SQL queries such as the ones present in the Spider dataset. This is achieved by breaking down each SQL query into a non-nested form that consists of multiple, simpler, sub-queries. The authors propose 7 types of sub-queries, each with its own sketch, that can be combined to produce more complex queries. The network then learns to recursively predict the type of each sub-query and to subsequently ﬁll in its sketch. RY ANSQL achieved the ﬁrst position in the Spider benchmark at the time of its publication, but has since been surpassed by other systems, while no other similar approach has been able to achieve comparable performance. SyntaxSQLNet [105] follows a similar approach, but instead of generating the query sketch, it follows a pre- deﬁned SQL grammar that determines which of its 9 slot- ﬁlling modules needs to be called to make a prediction. This allow the system to produce grammatically correct complex queries while enjoying the beneﬁts of a sketch-based decoder. At each prediction step, the grammar and the prediction his- tory from the previous steps are used to determine the module (e.g. COLUMN module, AGGREGA TOR module, OPERA- TOR module, HA VING module, etc.) that needs to make a prediction in order to build the SQL query. Although this is a hybrid approach, the architecture of the decoder modules classiﬁes SyntaxSQLNet as a sketch-based decoding system. The main difference is that most sketch-based decoders call all their slot-ﬁlling modules simultaneously to ﬁll the sketch, whereas SyntaxSQLNet calls speciﬁc modules recursively because the grammar deﬁnes what needs to be ﬁlled in at each prediction step. SyntaxSQLNet was one of the ﬁrst sys- tems proposed for Spider. Since then, many systems have achieved better performance scores while steering away from 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,2,caption,"Fig. 7 A timeline of deep learning text-to-sql systems, datasets and language representation techniques"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,3,caption,Fig. 8 Query Sketch proposed by SQLNet
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,19,page,"A survey on deep learning approaches for text-to-SQL 923 this methodology, hinting at its weaknesses. For example, one of the main challenges is to effectively pass all the infor- mation of the prediction history and the current state of the generated SQL to each module, at every prediction step. 6.3 Graph representations The use of graphs for input encoding has only recently seen increased use, despite its powerful capability to represent the DB schema. This section explores key systems that have shown new perspectives on how graphs can be represented and used in the text-to-SQL task. A natural option for processing graphs are Graph Neu- ral Networks (GNNs). However, while being a good option for tasks such as node classiﬁcation, node clustering and edge prediction, they are not as suitable for generative tasks like the text-to-SQL problem. Two systems manage to lever- age GNNs to encode the database schema and its elements: the GNN parser [ 8] and its successor Global-GNN parser [9]. To achieve this, the database schema is represented as a graph, where tables and columns are represented as nodes, and different types of edges represent the relation- ships between them (e.g. which columns appear in which table and which columns and tables are connected with a primary-foreign key relationship). For NLQ encoding, both systems use word embeddings and LSTM networks, while node encodings calculated by the GNNs are concatenated to each word embedding, based on the discovered schema links. For decoding, both systems use a grammar-based decoder [99] that generates a SQL query as an Abstract Syntax Tree (AST), which is often used by grammar-based systems [10,33,89]. Global-GNN [ 9] introduces the use of a re-ranker that, given k SQL predictions from the network, chooses the best interpretation based on the database elements used and the graph representation calculated. In order to avoid the disadvantages of GNNs, other efforts modify architectures that have already shown their power in the text-to-SQL task, such as the Transformer [ 87], so that they can accept edge information and process a graph. RAT- SQL [89] uses a graph representation of the input, but instead of using GNNs, it proposes a modiﬁed Transformer archi- tecture named Relation Aware Transformer (RA T). Firstly, it creates a question-contextualised schema graph , i.e. a graph representing the database tables and columns as well as the words of the NLQ as nodes and the relationships between them as edges. An edge can appear either between two database nodes, similarly to the previous systems, or between a database node and a word node. In this graph, schema link- ing is performed to discover connections between a database node and a word node that might refer to it. The names of all the nodes in the graph are ﬁrst encoded using BERT [ 19] and then processed by the RA T network, along with the edge information of each node. The RA T neural block performs relation aware self-attention on its inputs, which essentially biases the network towards the given relations (edges). This allows the system to use Transformers and even pre-trained language models to process the graph as a series while also utilising the information present in the graph edges. Finally, it generates a SQL query as an AST using the method men- tioned above [ 99]. All systems discussed in this section have grammar-based decoders. This happens mainly because they aim to produce complex queries such as the ones in the Spider dataset, and at the time of their publication, grammar-based decoders were the most common option. It would be possible for a system using a graph representation of the input to use a different decoder with its own advantages and drawbacks. 6.4 Using intermediate languages Following the success of the grammar-based methods in generating complex SQL queries over multi-table DBs, researchers also examined the use of languages during the decoding phase that can better align with NL than SQL mak- ing it easier for the system to make predictions, but at the same time they can be deterministically translated into SQL. We examine key systems that use an Intermediate Language, either a pre-existing language or one created speciﬁcally for this task, as the target language for the neural decoder. IRNet [33] is a grammar-based system capable of gener- ating complex SQL queries, such as the ones in the Spider dataset. It uses the same AST decoding method [ 99] for code generation used in other grammar-based text-to-SQL sys- tems (e.g. RA TSQL [89] and the GNN parser [ 8]). The main difference is that it predicts an AST of a SemQL program, which is an Intermediate Language created speciﬁcally for this system. Its authors argue that it is easier to generate queries in this language and then transform them to SQL. Furthermore, IRNet performs schema linking by consider- ing all n-grams of length 1 to 6 as query candidates and all column and table names as DB candidates and uses exact and partial matches to discover links between them. It also searches for all query candidates that appear inside quotes in the ConceptNet knowledge graph [ 82] in order to link them to a database column or table. Input encoding uses BERT followed by linear and recurrent neural networks. SmBoP [75] is a grammar-based system that introduces various novelties in the decoding phase. The use of rela- tional algebra as an Intermediate Language is one of them. Its authors argue that, along with being better aligned with NL, relational algebra is a language that is already used by DB engines, unlike SemQL. Additionally, in order to decode ASTs of queries in relational algebra, SmBoP uses a bottom- up parser, in contrast to the usual approach of generating ASTs by performing top-down depth-ﬁrst traversal, followed by almost all text-to-SQL systems. The bottom-up decoder 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,20,page,"924 G. Katsogiannis-Meimarakis, G. Koutrika generates at time step t, the top- k sub-trees of height ≤ t, where k is a given parameter that represents the number of beams used during the decoding search. The main advan- tage of the bottom-up parsing is that at any given time-step, the generated sub-trees are meaningful and executable sub- programs, while in the top-down parsing, intermediate states are partial programs without a clear meaning. 6 . 5T h ea g eo fB E R T Much like in other NLP problems, replacing a conventional encoder with a pre-trained language model such as BERT [19] has been shown to improve performance of a text-to- SQL system. SQLova [39] is a sketch-based approach focused on the WikiSQL dataset. It employs a large and complex network almost identical to the one used by SQLNet, with its main difference being that instead of GloV e embeddings, it uses BERT to create a contextualised representation of the NLQ and table headers. The representations are then passed to 6 networks, each responsible for a different part of the query sketch, that are very similar to the sub-networks used by SQLNet. The result is a staggering, almost 20%, increase in execution accuracy on the test set of WikiSQL, indicating BERT’s power in the text-to-SQL task. HydraNet [62] is another sketch-based approach on the WikiSQL benchmark taking advantage of the BERT lan- guage model. Its main difference from SQLova is that HydraNet aligns itself better to the way that BERT has been pre-trained and only uses a simple linear network after receiv- ing the contextualised representations from BERT, instead of large networks with LSTMs and attention modules like SQLova. Furthermore, HydraNet processes each table header separately instead of jointly encoding them, an approach that is unique to this system. As a result, it can only make predic- tions for each column on its own, i.e. it decides if the column at hand will appear in the SELECT clause, if it will appear in the WHERE clause, what its operation will be if it appears in the WHERE clause and so on. HydraNet, with its simpler architecture leveraging BERT, achieves better accuracy on WikiSQL than SQLova, which employs a larger and more complex network. X-SQL [35] is a sketch-based system using the MT- DNN pre-trained language model [ 58], that was built for the WikiSQL benchmark. Similarly to HydraNet, it uses much simpler networks than SQLova for ﬁlling the slots of the query sketch. However, it encodes all table headers simulta- neously, along with the user question. Additionally, instead of using segment embeddings that originally indicate the span of different sentences in the language model’s input, X-SQL uses type embeddings. These embeddings differenti- ate between the different types of elements in the input, such as the user’s question, categorical columns and numerical columns. Furthermore, it uses an attention layer to create a single token representation for columns that have more than one token (i.e. more than one word in their name). X-SQL also outperforms the much more complex SQLova, achieving slightly lower scores than HydraNet. 6.6 Schema linking focus As discussed earlier, schema linking is a major part of creat- ing a SQL query from a NLQ. This section looks into systems that have put extra effort on schema linking, or even based their entire workﬂow on this process. TypeSQL [103] is one of the ﬁrst systems to introduce a process similar to schema linking in its workﬂow, and one of the few systems working on WikiSQL that uses schema linking. Its methodology is described as Type Recognition , but closely resonates to the concept of schema linking. The goal of this methodology is to assign a “type” to every token of the NLQ. It considers all n-grams in the NLQ of length from 2 to 6 and tries to assign them one of the following “types”: ( a) Column, if it matches the name of a column or a value that appears under a column, ( b) Integer , Float, Date or Year, if it a numerical n-gram, ( c) Person, Place, Country, Organization or Sport by performing NER using the Free- base knowledge graph. Even though this process is unilateral, as its main goal is to classify the query candidates into a type category and not to explicitly link them to a DB candidate, it is one of the ﬁrst attempts towards schema linking. V alueNet[10] builds on the grammar-based system IRNet [33] focusing on schema linking and condition value dis- covery. The main motivation of the system is that despite the constant improvement of text-to-SQL systems, even the state-of-the-art is falling behind at predicting the correct val- ues in the SQL conditions. Similarly to IRNet, V alueNet decodes a SQL query in a SemQL 2.0 AST. SemQL 2.0 extends the SemQL grammar with values. Additionally, since condition values might not be written by the user in the exact same way they appear in the DB, V alueNet employs an extended value discovery workﬂow of ﬁve steps: • value extraction to recognise possible value mentions in the NLQ, it uses NER and heuristics; • value candidate generation to create additional candidate values, it uses string similarity, hand-crafted heuristics and n-grams; • value candidate validation to reduce the number of can- didate values, it keeps only the candidates that appear in the DB; • value candidate encoding it appends each candidate to the input along with the table and the column it was found under, and 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,21,page,"A survey on deep learning approaches for text-to-SQL 925 • neural processing the encoded representations are pro- cessed by the neural network, which eventually decides if and where they will be used. The authors also provide a classiﬁcation of the Spider queries based on the difﬁculty of discovering the values. This is another important aspect of the text-to-SQL problem usu- ally overlooked by other works. SDSQL [38] is a sketch-based system designed for the WikiSQL task. What is special about this system is that it can be viewed as two neural networks tackling two tasks at the same time. The ﬁrst network predicts SQL queries using the same architecture used by SQLova [39], while the second net- work performs schema dependency predictions. The schema dependency network uses bi-afﬁne networks [ 24] to predict dependencies between the words of the NLQ and the table headers. Such dependencies include: ( a)t h e select-column dependency that connects a query candidate that maps to a column that will appear in the SELECT clause with the corresponding column of the table, and ( b)t h e where-value dependency that connects the query candidate that refers to a value that will appear in the WHERE clause to the table column it belongs to. It must be noted that even though the second network performs schema linking, its predictions are not directly used by the ﬁrst network to construct the SQL query. Instead, a combined loss from the predictions of both tasks is used to train the weights of the networks, which allows the schema dependency learning to improve the ﬁrst network’s performance indirectly. IE-SQL [63] proposes a unique approach to the text-to- SQL problem almost completely based on schema linking. It uses two instances of BERT [ 19] to perform two differ- ent tasks: a mention extractor and a linker. The mention extractor recognises which query candidates are mentions of columns that will be used in the SELECT and WHERE clauses of the SQL query, mentions of aggregation func- tions, condition operators and condition values. Additionally, the mention extractor recognises mentions that should be grouped together. For example, the mentions of the column, the operator and the value that belong to the same condi- tion are grouped together. Having extracted the mentions, the linker maps the mentions of column names to the actual columns of the table they are referring to. The linker also maps value mentions without a grouped column to the appro- priate table column. By using the predictions of the mention extractor and the linker, IE-SQL can predict an SQL query, without any additional neural component. Even though this approach may not be a clear match with any of the three decoding categories, we classify it as a sketch-based system because its methodology is heavily based on the existence of a query sketch similar to the one used by SQLNet [ 96]. IE-SQL can better learn the dependencies between the slots and uses a more robust approach. Still, the mention types it recognises are a direct match to the slots of the query sketch. Therefore, extending it to queries beyond the sketch is not trivial. 6.7 The return of the sequence Generating SQL queries using a sequence-based decoder was initially avoided as it could produce syntax and grammar errors, as discussed in Section 4.4. Grammar-based decoders were instead regarded as the best choice for a system to effectively generate complex SQL queries. However, recent works [57,76,97] have changed the landscape by introducing a series of techniques that minimise the possibility of errors by sequence-based decoders. These techniques have made the use of very powerful pre-trained encoder–decoder mod- els [52,74] a viable and high-performing option, allowing the systems that use them to achieve top performance in both the Spider and WikiSQL benchmarks. SeaD [97] is a system based on the BART [ 52] encoder– decoder pre-trained language model designed on WikiSQL. To overcome the drawbacks of its sequence-based decoder, SeaD employs two techniques: ( a) it introduces two addi- tional tasks on which the model is trained at the same time with the text-to-SQL task, and ( b) it uses execution- guided decoding [ 91], slightly modiﬁed to work with its sequence-based decoder. Its main contribution is the use of the two additional training tasks named erosion and shufﬂe (see Sect. 4.5), which are designed speciﬁcally to help the model better understand the nature of the text-to-SQL prob- lem and the tables used by the WikiSQL dataset. The use of additional training tasks is also closely aligned with how language models are pre-trained to understand the more gen- eral notion of natural language before being ﬁne-tuned to a speciﬁc task. Nevertheless, while SeaD has managed to over- come the limitations of sequence-based decoders and achieve the best performance on the WikiSQL benchmark, both the decoding technique and the additional objectives it employs are designed with the WikiSQL dataset in mind. Extending them to full relational databases would not be a trivial matter. BRIDGE [57] is another recent system with a sequence- based decoder that works on Spider, although it does not use an encoder–decoder language model. Instead, it uses BERT [ 19] and LSTM networks for input encoding and enriches the input representation using linear networks that use metadata such as foreign and primary key relation- ships, as well as column type information. Additionally, the system performs schema linking using fuzzy string match- ing between query candidates and the values of columns that only take values from a pre-deﬁned list (i.e. picklist attributes). The discovered values are added in the input sequence to help the network create better SQL queries. Finally, the sequence-based decoder used by BRIDGE is a pointer generator network using schema-consistency guided 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,22,page,"926 G. Katsogiannis-Meimarakis, G. Koutrika decoding, a constraining strategy to avoid the aforemen- tioned drawbacks of sequence-based decoders. In order to use schema-consistency guided decoding, BRIDGE is trained (and makes predictions) on SQL queries written in execution order, i.e. all queries start with the FROM clause, followed by the WHERE, GROUP BY , HA VING, SELECT, ORDER BY and LIMIT clauses, strictly in that order. This means that all columns that appear in the query, must appear after the table that they belong to has been generated. Based on this, BRIDGE can limit the search space of columns and avoid using columns that will produce invalid SQL queries. PICARD [76] is a constraining technique for auto- regressive decoders of language models, that is speciﬁcally created to improve their performance on the text-to-SQL task. Essentially, at each prediction step, it constrains the model’s set of possible predictions by removing tokens that could pro- duce syntactically and grammatically incorrect SQL queries. It is used at inference time, by looking at the conﬁdence scores of the model’s prediction and the schema of the under- lying DB, and it operates at three levels: • it rejects misspelled attributes and keywords, as well as tables and columns that are invalid for the given schema, • it parses the output as an AST to reject grammatical errors, such as an incorrect order of keywords and clauses or an incorrect query structure, • it checks that all used tables have been brought into scope by being included in the FROM clause and that all used columns belong to exactly one table that has been brought into scope. When PICARD is used with the T5 [ 74] pre-trained lan- guage model (the 3B parameters version), it ranked ﬁrst on the Spider leaderboard for execution with values. This of course does not come without any drawbacks, such as the increased prediction time due to the constrained decoding, as well as the tremendous computational and memory require- ments for training and running such a large model as T5-3B. 7 Discussion and higher-level comparison In what follows, we make several observations regard- ing how the landscape is shaped along the dimensions of our taxonomy, presented in Sect. 4. Table 3 provides an overview of the design choices of each system studied in this survey. Additionally, we provide some higher-level insights that can be useful for practitioners interested in introduc- ing a deep learning text-to-SQL system in a real-world use case. These insights include remarks concerning: adaptabil- ity to new databases, difﬁculty of implementation, technical demands, and other advantages and drawbacks of certain design choices. A summary of these insights can be seen in Table 4. Output decoding There is a connection between the decod- ing approach used by a system and the benchmark on which it operates. Systems that operate on Spider do not use a sketch-based decoder. This is due to the fact that sketch- based approaches are more cumbersome to be adapted for generating complex SQL queries. RY ANSQL [13] attempted extending the sketch-based approach to Spider, but later systems steered away from this choice. Furthermore, while until recently grammar-based decoders dominated the Spider benchmark and sketch-based decoders dominated WikiSQL, recent improvements in sequenced-based decoders have turned the tables, bringing sequence-based decoders on the top of both benchmarks (i.e. T5-3B+PICARD [ 76] for Spider and SeaD [ 97] for WikiSQL). The output decoder is what deﬁnes the system’s SQL expressiveness and the effort needed to implement and extend the system to new types of SQL queries. For example, grammar-based decoders are harder to implement, since an extensive grammar is required in order for the system to cover all the possible SQL queries that the use case in question might require. Additionally, extending a system to use mathematical operations (e.g. WHERE end_year - start_year < 4 ) will require varying degrees of effort depending on the type of decoder. In the case of a sketch- based or grammar-based decoder, an extension of the sketch or grammar is necessary to cover the new query type. On the other hand, sequence-based decoders can effectively gener- ate everything (which is usually a drawback), as long as there are training examples to learn from. NL representation . There is a clear tendency by the lat- est models to use PLMs for NL representation. Besides the systems that use GNNs for input encoding [ 8,9], the only sys- tems that use word embeddings for NL representation, were published before PLMs were widely available. In almost all cases, the use of a PLM instead of word embeddings leads to a boost in performance. This is also shown in some systems that were originally designed to work with word embeddings, but are also tested with a PLM during ablation studies (e.g. RA T-SQL [89], RY ANSQL [13]). In fact, with the constant introduction of new PLMs, the question of which PLMs is more suitable becomes all the more relevant. However, a major, typically overlooked, drawback of PLMs is their com- putational cost and hardware requirements. Even though the cost of pre-training can be alleviated because it is very easy to ﬁnd a pre-trained model online, there is still the cost of training for the text-to-SQL downstream task, as well as dur- ing inference. Running a model with a PLM will also require an additional amount of computational resources (usually memory and/or a GPU) due to the size of these models. For example, BERT-base [19] has 110M parameters, BERT-large has 340M parameters, and T5 [ 74] has variations of similar 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,23,page,"A survey on deep learning approaches for text-to-SQL 927 Table 3 Systems examined in this work Year System Benchmark Schema linking Natural language Input encoding Output decoding Neural training Output reﬁnement 2017 Seq2SQL WikiSQL × WE Separate Sequence FS × SQLNet WikiSQL × WE Separate Sketch FS × 2018 IncSQL WikiSQL × WE Separate Grammar FS × TypeSQL WikiSQL ✓ WE Separate Sketch FS × Coarse2Fine WikiSQL × WE Separate Sketch FS × SyntaxSQLNet Spider × WE Separate Sketch FS × 2019 SQLova WikiSQL × E-PLM Serialise Sketch TL EG decoding IRNet Spider ✓ WE or E-PLM Serialise Grammar TL × X-SQL WikiSQL × E-PLM Serialise Sketch TL EG decoding RA T-SQL Spider ✓ WE or E-PLM Graph Grammar TL × GNN Spider ✓ WE Graph Grammar FS × Global-GNN Spider ✓ WE Graph Grammar FS Re-ranking 2020 V alueNet Spider ✓ E-PLM Serialise Grammar TL × BRIDGE Spider ✓ E-PLM Serialise Sequence TL Constr. decoding HydraNet WikiSQL × E-PLM Per column Sketch TL EG decoding IE-SQL WikiSQL ✓ E-PLM Serialise Sketch TL EG decoding RY ANSQL Spider × WE or E-PLM Serialise Sketch TL × SmBoP Spider ✓ E-PLM Graph Grammar TL × 2021 SDSQL WikiSQL ✓ E-PLM Serialise Sketch TL + AO EG decoding SeaD WikiSQL × ED-PLM Serialise Sequence TL + AO Constr. decoding T5-3B+PICARD Spider × ED-PLM Serialise Sequence TL Constr. decoding In the natural language column, WE, E-PLM and ED-PLM stand for word embeddings, encoder-only PLM and encoder–decoder PLM accordingly. In the neural t raining column, FS, TL and AO stand for fresh start, transfer learning and additional objective accordingly. In the output reﬁnement column, EG decoding and constr. Decoding sta nd for execution-guided and constrained decoding accordingly 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,2,caption,Table 3 Systems examined in this work
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,24,page,"928 G. Katsogiannis-Meimarakis, G. Koutrika Table 4 Higher-level comparison of taxonomy dimensions on various practical dimensions ( ➚ signiﬁes good performance, ➘ signiﬁes poor performance, and ➙ signiﬁes average performance) WE E-PLMED-PLMSeparateSerialisePer columnGraphSketchGrammarSequenceFresh StartTr. learningAdd. objectivesEG-deco ding Constr. decoding Re-ranking Natural Input Output Neural Output language encoding decoding training reﬁnement Ease of implementation Use with full RDBs Extend to new SQL types Computational costs Handling large schemas sizes that reach up to 11B parameters, while the one pre- sented with PICARD [ 76] has 3B parameters. This must be considered, especially when building applications that must support heavy workloads or have low latency requirements. Input encoding Regarding input encoding, there are two main observations to point out: ( a) while earlier systems performed separate encoding, later systems use serialised or graph encoding, and ( b) newer systems working on the Wik- iSQL, all use serialised encoding. The clear tendency to use serialised encoding can be easily attributed to the extensive use of PLMs, which offer much better performance with a serialised input. This is even more true in the case of Wik- iSQL, because single tables can easily be serialised along with the NLQ making the combination of PLMs and seri- alised encoding an easy and powerful choice. However, when it comes to DBs with several tables and relationships among them and their columns, a more ﬂexible and informative rep- resentation is required. Some systems have examined the more innovative approach of graph encoding, which so far seems promising, offering a lot of ground for future research. Another practical limitation that must be taken into account is how ﬂexible each encoding option is when it comes to DBs with large schemas. For example, the SDSS database, which stores data from astronomical surveys, has 87 tables with some tables containing up to a hundred columns. Serialising such a schema would result in a very long sequence that can- not be processed by a PLM due to their limitation in input length. Similarly, performing separate encoding might cre- ate a bottleneck in the schema encoder side. Graph encoding might be more efﬁcient for handling larger schemas, since GNNs can encode each schema element as a single node. However, this approach is also prone to poorer performance as the schema gets larger. Schema linking Table 5 displays the schema linking tech- niques used by each system studied in this survey. While the ﬁrst text-to-SQL systems did not perform any kind of schema linking, later systems have proposed various intri- cate schema linking pipelines. On the query side, we observe that almost all systems consider single-word and multi-word tokens, while V alueNet [ 10] also performs NER to ﬁnd pos- sible candidates. On the DB side, using the table and column names is the baseline for most systems, while some sys- tems also lookup the values that are present in the DB. Finally, to match the candidates, some systems use simple text matching (either exact or partial), while newer systems have experimented with the use of classiﬁers instead of string operations to ﬁnd matches. It becomes quickly apparent that schema linking is mostly explored by systems operating on the Spider dataset, accompanied by very few systems using the WikiSQL benchmark. This is somewhat expected, given that as the SQL complexity and the volume of tables, columns and data increase, researchers seek to aid the neural network by providing auxiliary information. However, what is very peculiar is that some high performing recent systems (i.e. T5-3B+PICARD [ 76] and SeaD [ 97]) do not perform any schema linking at all. This is an open research question. Can powerful neural architectures, pre-trained on vast amounts of data, defy the need for schema linking? Or, can they achieve even higher scores if combined with schema link- ing? One important observation is that very little effort has been put into testing how fast and scalable these approaches are, especially for very large databases. In fact, to the best of our knowledge, only a single work [ 86] provides exper- imental evaluations concerning the time and memory used for schema linking. Hence, extra caution is necessary when using these methods in a real-world system, as most of them are not adequately optimised. Neural training The neural training dimension is closely connected to the NL representation adopted by each system. This happens because using a PLM means that the model adopts the Transfer Learning paradigm, because it further trains an already pre-trained neural component on a new downstream task. There are no cases of systems perform- ing transfer learning on other parts of the model besides the 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,2,caption,"Table 4 Higher-level comparison of taxonomy dimensions on various practical dimensions ( ➚ signiﬁes good performance, ➘ signiﬁes poor"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,25,page,"A survey on deep learning approaches for text-to-SQL 929 NL representation part. This is mostly due to the fact that PLMs perform exceptionally well, and making an improve- ment through a different transfer learning technique would be very difﬁcult. Furthermore, there are only two models that use additional objectives during training [ 57,97]. This relatively novel approach follows the success of PLMs using vari- ous auxiliary tasks during pre-training and seems to be very promising in training a model that achieves better generalisa- tion. It must be noted that the time and computing resources needed to train a model using each training approach are usu- ally not taken into account when presenting new models, in favour of better performance metrics. It is however necessary to address them in order to make the use of such models feasi- ble in a real-world application. For example, the pre-training part of transfer learning is very costly, unless the pre-trained model is made available by its creators. Similarly, using addi- tional objectives will greatly increase the computations that must be performed, thus increasing the cost of training. Output reﬁnement The output reﬁnement heavily depends on the approach used for output decoding, as well as the dataset that the system operates on. A system designed for WikiSQL can use execution-guided decoding [ 91], no matter the type of its decoder because of the simplicity of the Wik- iSQL queries. Systems with sequence-based decoders can use constrained decoding techniques to improve their pre- dictions and reduce the possibilities of errors. In fact, this output reﬁnement technique is one of the main reasons why they can be so effective. The re-ranking technique could be used by any system that can produce more than one predic- tions for a single input, but in practice it has not been adopted by any other system after being proposed by Global-GNN [ 9]. Furthermore, each reﬁnement technique adds an additional burden to the system that translates to extra computational cost and more time needed to make a prediction. When used in a real-time application, it is necessary to consider if the performance boost gained from the reﬁnement step, is worth the extra time and resources required. 8 Research challenges While a lot of progress has been made on the text-to-SQL problem, several important issues need to be tackled. In this section, we outline some of the most challenging prob- lems and highlight interesting research opportunities for the database and the machine learning communities that could greatly impact the state of the art in text-to-SQL research and beyond. 8.1 Benchmarks As mentioned earlier, WikiSQL and Spider are large-scale query benchmarks that provide a common way to evaluate and compare different systems. They have simpliﬁed system evaluation, and they are often seen as the panacea for text- to-SQL evaluation. Researchers tend to over-rely on these benchmarks to argue that their systems are advancing the state of the art, and they do not spend time performing addi- tional experiments on other benchmarks. However, given the progress in system-building, new standards are necessary for benchmarking text-to-SQL systems, in order to make these systems applicable to real-world scenarios, and to continue pushing the state of the art. First of all, datasets such as WikiSQL, that contain single- table databases and very simple SQL queries, cannot be seen as realistic benchmarks for real-world applications. Given that the SQL queries in WikiSQL can be covered by a very simple sketch, as the one shown in Fig. 8, and current systems have reached very high accuracy scores on this dataset, there is a need for more challenging benchmarks. These bench- marks were a good start for the neural text-to-SQL ﬁeld and have allowed a lot of novel ideas to be implemented in a “sandbox” environment, but the state of the art is now able to achieve much more. Similarly, Spider contains DBs and queries that were speciﬁcally created for text-to-SQL evaluation but they are rather simplistic and do not reﬂect the characteristics of real- world DBs. For example, the Spider DBs have a simple schema or too little data stored. In fact, the 166 DBs of Spider that are available to the public (i.e. train and dev set, since the test set is held-out by the authors) sum up to less than 1GB. Ideally, new benchmarks should aspire to introduce real cases of DBs taken from the industry and academia, accom- panied with real logs of SQL queries performed on them by their users. The NLQ part could be obtained either by asking the users to specify what their intention was when running these queries, by asking SQL experts to explain them, or by employing a SQL-to-text system. Another important drawback of current benchmarks is their relatively small number of examples (i.e. NL-SQL pairs), especially compared to datasets used by deep neu- ral networks in other problems (e.g. the SQuAD Question Answering dataset contains more than 100K examples). Besides the obvious contribution of creating a new large- scale dataset from scratch, there are a few other paths that could be considered. For example, it would be possible to create a novel benchmark suite containing multiple previ- ous benchmarks. This would not be a trivial work, since a lot of consideration is needed concerning how to split the datasets in a way that the train and test sets could help devel- opers understand if their system can successfully generalise to unseen DBs, domains, SQL query patterns, NLQ vocab- ulary, etc. Another way to create a new benchmark could be by transforming similar benchmarks used in slightly dif- ferent tasks, or different query languages. For example, a text-to-SPARQL dataset, such as the CFQ dataset [ 46] that 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,26,page,"930 G. Katsogiannis-Meimarakis, G. Koutrika Table 5 A comparison of schema linking techniques used by the examined systems; the schema linking process is divided in the query candidate discovery, DB candidate discovery and candidate matching phases, as described in our taxonomy Single tokens Multi-word tok ens Named entitiesAdd itional ca ndi dates Tab le and col umn names Values via lookupValues via KG s Exact matchingPart ial matching Appro ximate matching Learned embeddings Classiﬁers Year System Benchmark Query DB Matching 2017 Seq2SQL WikiSQL SQLNet WikiSQL 2018 IncSQL WikiSQL TypeSQL WikiSQL Coarse2Fine WikiSQL SyntaxSQLNet Spider 2019 SQLova WikiSQL IRNet Spider X-SQL WikiSQL RAT-SQL Spider GNN Spider Global-GNN Spider 2020 ValueNet Spider BRIDGE Spider HydraNet WikiSQL IE-SQL WikiSQL RYANSQL Spider SmBoP Spider 2021 DBTagger - SDSQL WikiSQL SeaD WikiSQL T5-3B+PICARD Spider contains more than 200 thousand NLQ-to-SPARQL exam- ples or the LC-QuAD dataset [ 85] that contains 5 thousand examples, would be very beneﬁcial if converted to SQL (sim- ilarly to how the WikiTableQuestions [ 67] was used to create the SQUALL [ 80] text-to-SQL dataset). Another critical limitation of existing benchmarks is that they fail to address the question of what type of NL and SQL queries a system can understand and build, respectively. This is due to the lack of a clear query categorisation. For instance, Spider has four very coarse-grained classes of queries. This highlights the need for new benchmarks and in-depth system evaluations, in the spirit of [ 6,30], that provide ﬁne-grained query categories and allow researchers to understand the strengths and weaknesses of a system. Furthermore, existing benchmarks assume that for each NL query, there is only one correct SQL query. This may be restricting. First, there are NL queries that may have more than one correct translations over the data. Second, equivalent SQL queries are written in a different way but return the same results. Finally, while the state-of-the-art systems are still dealing with “getting the answer right”, they are mostly overlooking the “getting the answer fast”. The database community could come up with benchmarks that focus on efﬁciency (not just effectiveness) and allow evaluating systems based on execu- tion time and resource consumption in addition to translation accuracy. 8.2 System efficiency and technical feasibility Focusing on the translation accuracy of the system is only one side of the coin. Evaluating system efﬁciency is important in order to understand the viability of a solution and pinpoint the pain points that need to be addressed. Deep learning text-to- SQL systems are typically relying on very complex models, which have been trained and evaluated in toy databases (like the ones contained in existing benchmarks). Hence, it comes to no surprise that they have not yet seen practical applica- tions in real-life use-cases and domains, and their usefulness is to be proved. Several important challenges need to be tack- led ﬁrst. Firstly, while the use of PLMs for NL Representation is highly favoured by newer systems, these models introduce a large overhead at inference time, and while using larger 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,2,caption,Table 5 A comparison of schema linking techniques used by the examined systems; the schema linking process is divided in the query candidate
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,27,page,"A survey on deep learning approaches for text-to-SQL 931 PLMs usually translates to higher accuracy, it also trans- lates to higher inference times. Output reﬁnement techniques are also adding extra overhead that might make a system impractical to use in a real-world scenario. For example, one of the best-performing models on the Spider dataset, T5-3B+PICARD, uses a large PLM along with a compu- tationally intensive output reﬁnement technique. Adapting such a model to work with fewer resources, reducing its training time, or optimising its output reﬁnement would be a signiﬁcant scientiﬁc and engineering achievement. Furthermore, input encoding techniques such as serialisa- tion combined with PLMs have an input size limit (usually of 512 tokens), which poses no problem for the DBs in Spi- der, but is restricting when working with real-world database schemas. The challenge of creating a robust input encod- ing technique that can efﬁciently work with larger schemas, must also be tackled in order to make text-to-SQL systems technically feasible. Additionally, schema linking techniques have been shown to work and be beneﬁcial for systems working on the Spider dataset, but they have yet to be tested on a real, large- scale DB. Even though using indices and other DB lookup techniques might speed up schema linking, it is still ques- tionable if looking up multiple words or n-grams for every NLQ, is efﬁcient in a real application. Advanced matching techniques, such as classiﬁers, also introduce additional over- head. There is a lot of room for contributions in optimising schema linking, and this could be the area where the DB community has the most to offer in order to make the break- throughs of the NLP world usable in practice. In a nutshell, improving translation speed by building efﬁ- cient methods is necessary. But this may not be enough. Text-to-SQL translation creates overhead to the overall query execution time that the user will experience, and hence needs to be weighted in. Early text-to-SQL systems originating from the DB community [ 36,37,53,60,110] not only tried to generate correct SQL queries but also optimal in terms of execution speed. Hence, many of them contained logic for generating code that would return the desired results fast. Ultimately, allowing the user to express questions in natu- ral language should free them from the technical details of how this query should be expressed in the underlying system language and how it should be executed efﬁciently. 8.3 Universality of the solution Another challenge is the universality of the solution, i.e. per- forming equally well for different databases. This problem becomes highly relevant when applying a text-to-SQL sys- tem to an actual database [ 34] that is used in a business, research or any other real-world use case. Apart from the large number of tables and attributes that we have already discussed, such databases may contain table and column names that use domain-speciﬁc terminology. For example, the SDSS [ 83] database has attributes such as “speccobj” (spectroscopic object) and “photoobj” (photometric object), that are unknown to and hence cannot be translated by any of the available text-to-SQL systems. That is why in real-life applications, ontologies and domain knowledge are used to enable reliable text-to-SQL translations [ 4,73]. It is also important to enable natural language queries in languages other than English, which is the main focus of current efforts. Due to the problem’s multidisciplinarity, database, ML, and NLP approaches can join forces to push the barrier further. 8.4 Data augmentation The need of deep learning models to train on a high volume of training examples, combined with the relatively small size of available benchmarks and the cost of manually creating new examples, has elevated data augmentation to an important problem. DBPal [94] is a template-based approach that uses manu- ally crafted templates of NL/SQL-pairs, which can be ﬁlled with the names of tables, columns and values in order to cre- ate training instances. The NLQs can be further augmented, with the use of NL techniques such as paraphrasing, random deletions and synonym substitutions. Nevertheless, such tem- plates and NL techniques can not work consistently across all new DBs and might often result to “robotic” or unnatural NLQs. Another approach [ 32] uses a similar template-based approach to create SQL queries by sampling column names and values from a given table and then applies recurrent neu- ral networks (RNNs) to generate the equivalent NLQ. A more recent work [ 95] proposes a pipeline that can generate exam- ples spanning over multiple tables of a relational database. SQL queries are created using an abstract syntax tree gram- mar and ﬁlling them with attributes from the database. The NLQs are then generated using a hierarchical, RNN-based neural model, that recursively generates explanations for all parts of the queries and then concatenates them. However, even though some initial efforts have been made, a systematic evaluation of how each approach affects differ- ent systems, as well as the quality of generated data in each case, is still missing. Additionally, another research question that arises is how to train a system using domain-speciﬁc or augmented data, along with a general-domain dataset such as Spider. For example, should the system be trained simul- taneously on domain-speciﬁc as well as general-use data, or only on domain-speciﬁc data, or should a more advanced sampling method be used [ 95]? 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,28,page,"932 G. Katsogiannis-Meimarakis, G. Koutrika 8.5 The path to data democratisation While the text-to-SQL problem is a major research challenge, it is also important to understand that it is a piece of the greater puzzle of data democratisation. In order to allow all users, no matter their technical knowledge, to easily access data and to derive value from it, we must consider complementary prob- lems, such as query explanations, query result explanations, and query recommendations. These problems can also bene- ﬁt from and be inspired by the models and methods presented in our study. Query explanations When a user formulates a query using a text-to-SQL system, the question is how they can conﬁrm that the obtained results match the intention of the NLQ. Query explanations in natural language would allow the user to cross-check their NLQ to the explanations of the predicted SQL queries and validate the results. This is the SQL-to- text problem [ 26,48,93], which has been understudied so far, and would greatly beneﬁt from models and methods for the text-to-SQL problem. Many interesting questions arise, including: how to transfer existing text-to-SQL meth- ods to solve this problem, what evaluation metrics to use, and whether we can design systems that can use the same model to solve both problems. In this direction, models such as T5 seem promising. Query result explanations In a similar vein, results to a query are typically presented in a tabular form that is not self- explanatory. Generating NL explanations for query results is another open research area [ 18,81]. Interestingly, while there has been considerable work on the “sibling” area of data-to-text generation [7], the problem of query result expla- nations (or QR-to-text) has several intricacies that do not allow directly adapting methods from the data-to-text gen- eration domain. The need to capture query semantics (that are implied by the results), the lack of appropriate bench- marks, and the fact that query results may contain several rows from different tables that are joined are just a few of the open issues. Query recommendations Even when the user understands the data that is kept in the database, it might not always be clear what kind of queries can be asked and what kind of knowledge can be extracted. For this reason, query recom- mendations can help a user ﬁnd interesting queries to ask the database, either based on the user preferences and history, or on queries that are frequently asked by other users of the same database [ 41] or by analysing the data [ 31]. In this context, adapting deep-learning models for query recommendations offers numerous challenges and opportunities. Conversational text-to-SQL Developing a conversational DB interface is another promising task, very similar to ear- lier non-DL approaches such as Analyza [ 20], which heavily involves the user in the translation process. Since our ultimate goal is creating a user-friendly and seamless experience, it would be very interesting to allow the user to access and query data solely through the power of natural language and conver- sation. The release of a conversational (CoSQL [ 106]), and a context-dependent (SParC [ 108]) text-to-SQL dataset, based on the Spider [ 107] dataset, has allowed for more focused progress in this domain. The conversational version of the problem carries new aspects and difﬁculties that candidate systems must tackle. First and foremost, for each prediction, the system must take into account all previous interactions with the user (i.e. all previous NLQs and the predicted SQL queries). Additionally, it is often necessary to ask the user for clariﬁcations when facing vague questions, or ask the user to choose between possible interpretations of an utterance in the conversation. While some of the systems presented in this work can be adapted to work in a conversational setting, heavier modiﬁcations are often necessary in order for the model to effectively encode the conversation history and the previous SQL predictions (note that we have only discussed about encoding NL and DB schemas). Ultimately, this aspect of the problem opens the path towards “intelligent data assis- tants“ [ 64], similar to but extremely more powerful than the intelligent personal assistants that are gaining more and more popularity and use through our smartphones and dedicated speakers devices. 9 Conclusions The domain of text-to-SQL translation has received increas- ingly larger attention by both the database and NLP com- munities. The recent introduction of two large text-to-SQL datasets [107,112] has enabled the use of deep learning mod- els and spurred a new wave of innovation. To understand which milestones have been conquered and what obstacles lie ahead, it is necessary to provide a systematic and organ- ised study of the ﬁeld. This work explained the text-to-SQL problem and the available benchmarks, before diving into the systems. We provided a ﬁne-grained taxonomy of deep learning text- to-SQL systems, based on six axes: ( a) schema linking, (b) natural language representation, ( c) input encoding, ( d) output decoding, ( e) neural training, and ( f ) output reﬁne- ment. For each axis of our taxonomy, we analysed all the approaches that have been presented so far and explained their strengths and weaknesses. We relied on this taxonomy to present some of the most important systems that have been proposed, grouping them together, in order to highlight their similarities, differences and innovations. Finally, having presented the current state of the art, we discussed open challenges and research opportunities that must be tackled in order to truly advance the ﬁeld of text-to- SQL, as well as broader challenges that are closely related to it. It is important to keep in mind, that the ultimate goal of text-to-SQL research is to empower the casual user to 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,29,page,"A survey on deep learning approaches for text-to-SQL 933 access and derive value from data. This is a goal that requires the combined effort of multiple disciplines and cannot be measured by a single performance metric. Acknowledgements This work has been partially funded by the Euro- pean Union’s Horizon 2020 research and innovation program (Grant Agreement No. 863410). Funding Open access funding provided by HEAL-Link Greece. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adap- tation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indi- cate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copy- right holder. To view a copy of this licence, visit http://creativecomm ons.org/licenses/by/4.0/. References 1. Abbas, S., Khan, M.U., Lee, S.U.-J., Abbas, A., Bashir, A.K.: A review of nlidb with deep learning: ﬁndings, challenges and open issues. IEEE Access. 10, 14927–14945 (2022) 2. Affolter, K., Stockinger, K., Bernstein, A.: A comparative sur- vey of recent natural language interfaces for databases. VLDB J. 28(5), 793–819 (2019) 3. Ambiguity. https://stanford.io/2YXcECi 4. Amer-Yahia, S., Koutrika, G., Braschler, M., Calvanese, D., Lanti, D., Lücke-Tieke, H., Mosca, A., de Farias, T Mendes, Papadopou- los, D., Patil, Y ., Rull, G., Smith, E., Skoutas, D., Subramanian, S., Stockinger, K.: Inode: building an end-to-end data exploration system in practice. SIGMOD Rec. 50(4), 23–29 (2022) 5. Androutsopoulos, I., Ritchie, G.D., Thanisch, P .: Natural language interfaces to databases—an introduction. Nat. Lang. Eng. 1(1), 29–81 (1995) 6. Belmpas, T., Gkini, O., Koutrika, G.: Analysis of database search systems with THOR. In: Maier, D., Pottinger, R., Doan, A., Tan, W., Alawini, A., Ngo, H.Q. (eds.) Proceedings of the 2020 Interna- tional Conference on Management of Data, SIGMOD Conference 2020, online conference [Portland, OR, USA], June 14–19, 2020, pp. 2681–2684. ACM (2020) 7. Berant, J., Deutch, D., Globerson, A., Milo, T., Wolfson, T.: Explaining queries over web tables to non-experts. In: 2019 IEEE 35th International Conference on Data Engineering (ICDE), pp. 1570–1573 (2019) 8. Bogin, B., Berant, J., Gardner,M.: Representing schema struc- ture with graph neural networks for text-to-SQL parsing. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4560–4565. Association for Com- putational Linguistics, Florence, Italy (2019) 9. Bogin, B., Gardner, M., Berant, J.: Global reasoning over database structures for text-to-SQL parsing. In: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro- cessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3659–3664. Asso- ciation for Computational Linguistics, Hong Kong, China (2019) 10. Brunner, U., Stockinger, K.: V aluenet: a neural text-to-sql archi- tecture incorporating values (2020). arXiv:2006.00888 11. Cai, R., Xu, B., Zhang, Z., Yang, X., Li, Z., Liang, Z.: An encoder– decoder framework translating natural language to database queries. In: Lang, J. (ed.) Proceedings of the Twenty-Seventh International Joint Conference on Artiﬁcial Intelligence, IJCAI 2018, July 13–19, 2018, Stockholm, Sweden, pp. 3977–3983. ijcai.org (2018) 12. Cao, R., Chen, L., Chen, Z., Zhao, Y ., Zhu, S., Y u, K.: LGESQL: line graph enhanced text-to-SQL model with mixed local and non- local relations. In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Interna- tional Joint Conference on Natural Language Processing (V olume 1: Long Papers), pp. 2541–2555. Association for Computational Linguistics (2021) 13. Choi, D., Shin, M.C., Kim, E., Shin, D.R.: RY ANSQL: recur- sively applying sketch-based slot ﬁllings for complex text-to-SQL in cross-domain databases. Comput. Linguist. 47(2), 309–332. https://doi.org/10.1162/coli_a_00403 14. Codd, E.F.: Seven steps to rendezvous with the casual user. In: Klimbie, J.W., Koffeman, K.L. (eds.) Data Base Management, Proceeding of the IFIP Working Conference Data Base Manage- ment, Cargèse, Corsica, France, April 1–5, 1974, pp. 179–200. North-Holland (1974) 15. Dahl, D.A., Bates, M., Brown, M., Fisher, W., Hunicke-Smith, K., Pallett, D., Pao, C., Rudnicky, A., Shriberg, E.: Expanding the scope of the atis task: The atis-3 corpus. In: Proceedings of the Workshop on Human Language Technology, HLT ’94, pp. 43-48. Association for Computational Linguistics, USA (1994) 16. Damerau, F.J.: A technique for computer detection and correction of spelling errors. Commun. ACM 7(3), 171–176 (1964) 17. Deng, N., Chen, Y ., Zhang, Y .: Recent advances in text-to-SQL: a survey of what we have and what we expect. In: Proceedings of the 29th International Conference on Computational Linguis- tics, pp. 2166–2187. International Committee on Computational Linguistics, Gyeongju, Republic of Korea (2022) 18. Deutch, D., Frost, N., Gilad, A.: Explaining natural language query results. VLDB J. 29(1), 485–508 (2020) 19. Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: BERT: pre- training of deep bidirectional transformers for language under- standing. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, vol. 1 (Long and Short Papers), pp. 4171–4186. Association for Computational Linguis- tics, Minneapolis (2019). https://doi.org/10.18653/v1/N19-1423 20. Dhamdhere, K., McCurley, K.S., Nahmias, R., Sundararajan, M., Yan, Q.: Analyza: Exploring Data with Conversation. ACM, New Y ork (2017) 21. Dong, L., Lapata, M.: Language to logical form with neural attention. In: Proceedings of the 54th Annual Meeting of the Asso- ciation for Computational Linguistics (V olume 1: Long Papers), pp. 33–43. Association for Computational Linguistics, Berlin (2016). https://doi.org/10.18653/v1/P16-1004 22. Dong, L., Lapata, M.: Coarse-to-ﬁne decoding for neural semantic parsing. In: Proceedings of the 56th Annual Meeting of the Associ- ation for Computational Linguistics (V olume 1: Long Papers), pp. 731–742. Association for Computational Linguistics, Melbourne (2018). https://doi.org/10.18653/v1/P18-1068 23. Dozat, T., Manning, C.D.: Deep biafﬁne attention for neural dependency parsing (2017) 24. Dozat, T., Manning, C.D.: Deep biafﬁne attention for neural dependency parsing. In: 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24–26, 2017, Conference Track Proceedings. OpenReview.net (2017) 25. Dozat, T., Manning, C.D.: Simpler but more accurate semantic dependency parsing. In: Proceedings of the 56th Annual Meet- ing of the Association for Computational Linguistics (V olume 2: Short Papers), pp. 484–490. Association for Computational 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,30,page,"934 G. Katsogiannis-Meimarakis, G. Koutrika Linguistics, Melbourne (2018). https://doi.org/10.18653/v1/P18- 2077 26. Eleftherakis, S., Gkini, O., Koutrika, G.: Let the database talk back: natural language explanations for SQL. In: Mottin, D., Lissandrini, M., Roy, S.B., V elegrakis, Y . (eds.) Proceedings of the 2nd Workshop on Search, Exploration, and Analysis in Het- erogeneous Datastores (SEA-Data 2021) Co-located with 47th International Conference on V ery Large Data Bases (VLDB 2021), Copenhagen, Denmark, August 20, 2021, volume 2929 of CEUR Workshop Proceedings, pp. 14–19. CEUR-WS.org (2021) 27. Finegan-Dollak, C., Kummerfeld, J.K., Zhang, L., Ramanathan, K., Sadasivam, S., Zhang, R., Radev, D.: Improving text-to- SQL evaluation methodology. In: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (V ol- ume 1: Long Papers), pp. 351–360. Association for Computational Linguistics, Melbourne, Australia (2018) 28. Gan, Y ., Chen, X., Huang, Q., Purver, M., Woodward, J.R., Xie, J., Huang, P .: Towards robustness of text-to-SQL models against synonym substitution. In: Proceedings of the 59th Annual Meet- ing of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (V olume 1: Long Papers), pp. 2505–2515. Association for Com- putational Linguistics (2021) 29. Gan, Y ., Chen, X., Purver, M.: Exploring underexplored limita- tions of cross-domain text-to-SQL generalization. In: Proceedings of the 2021 Conference on Empirical Methods in Natural Lan- guage Processing, pp. 8926–8931, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics (2021) 30. Gkini, O., Belmpas, T., Ioannidis, Y ., Koutrika, G.: An in-depth benchmarking of text-to-sql systems. In: SIGMOD Conference. ACM (2021) 31. Glenis, A., Koutrika, G.: Pyexplore: query recommendations for data exploration without query logs. In: Li, G., Li, Z., Idreos, S., Srivastava, D. (eds.) SIGMOD ’21: International Conference on Management of Data, Virtual Event, China, June 20–25, 2021, pp. 2731–2735. ACM (2021) 32. Guo, D., Sun, Y ., Tang, D., Duan, N., Yin, J., Chi, H., Cao, J., Chen, P ., Zhou, M.: Question generation from SQL queries improves neural semantic parsing. In: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Process- ing, pp. 1597–1607. Association for Computational Linguistics, Brussels, Belgium (2018) 33. Guo, J., Zhan, Z., Gao, Y ., Xiao, Y ., Lou, J.-G., Liu, T., Zhang, D.: Towards complex Text-to-SQL in cross-domain database with intermediate representation (2019) 34. Hazoom, M., Malik, V ., Bogin, B.: Text-to-SQL in the wild: a naturally-occurring dataset based on stack exchange data. In: Pro- ceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Prog 2021), pp. 77–87. Association for Computational Linguistics (2021) 35. He, P ., Mao, Y ., Chakrabarti, K., Chen, W.: X-sql: reinforce schema representation with context (2019) 36. Hristidis, V ., Gravano, L., Papakonstantinou, Y .: Efﬁcient IR-style keyword search over relational databases. In: VLDB, pp. 850–861 (2003) 37. Hristidis, V ., Papakonstantinou, Y .: Discover: keyword search in relational databases. In: VLDB, pp. 670–681 (2002) 38. Hui, B., Shi, X., Geng, R., Li, B., Li, Y ., Sun, J., Zhu, X.: Improving text-to-sql with schema dependency learning (2021) 39. Hwang, W., Yim, J., Park, S., Seo, M.: A comprehensive explo- ration on wikisql with table-aware word contextualization (2019) 40. Iacob, R.C.A., Brad, F., Apostol, E.-S., Truic˘ a, C.-O., Hosu, I.A., Rebedea, T.: Neural approaches for natural language interfaces to databases: A survey. In: Proceedings of the 28th International Conference on Computational Linguistics, pp. 381–395. Interna- tional Committee on Computational Linguistics, Barcelona, Spain (2020) 41. Idreos, S., Papaemmanouil, O., Chaudhuri, S.: Overview of data exploration techniques. In: Sellis, T.K., Davidson, S.B., Ives, Z.G. (eds.) Proceedings of the 2015 ACM SIGMOD International Con- ference on Management of Data, Melbourne, Victoria, Australia, May 31–June 4, 2015, pp. 277–281. ACM (2015) 42. Iyer, S., Konstas, I., Cheung, A., Krishnamurthy, J., Zettlemoyer, L.: Learning a neural semantic parser from user feedback. In: Barzilay, R., Kan, M. (eds.) Proceedings of the 55th Annual Meet- ing of the Association for Computational Linguistics, ACL 2017, V ancouver, Canada, July 30–August 4, V olume 1: Long Papers, pp. 963–973. Association for Computational Linguistics (2017) 43. Kamath, A., Das, R.: A survey on semantic parsing. In: 1st Confer- ence on Automated Knowledge Base Construction, AKBC 2019, Amherst, MA, USA, May 20–22, 2019 (2019) 44. Katsogiannis-Meimarakis, G., Koutrika, G.: A deep dive into deep learning approaches for text-to-sql systems. In: Proceedings of the 2021 International Conference on Management of Data, SIGMOD/PODS ’21, pp. 2846–2851, New Y ork, NY , USA. Asso- ciation for Computing Machinery (2021) 45. Katsogiannis-Meimarakis, G., Koutrika, G.: Deep learning approaches for text-to-sql systems. In: EDBT, pp. 710–713 (2021) 46. Keysers, D., Schärli, N., Scales, N., Buisman, H., Furrer, D., Kashubin, S., Momchev, N., Sinopalnikov, D., Staﬁniak, L., Tihon, T., et al.: Measuring compositional generalization: a com- prehensive method on realistic data (2019). arXiv:1912.09713 47. Kim, H., So, B.-H., Han, W.-S., Lee, H.: Natural language to sql: where are we today? Proc. VLDB Endow. 13(10), 1737–1750 (2020) 48. Kokkalis, A., V agenas, P ., Zervakis, A., Simitsis, A., Koutrika, G., Ioannidis, Y . E.: Logos: a system for translating queries into narratives. In: Candan, K.S., Chen, Y ., Snodgrass, R.T., Gravano, L., Fuxman, A. (eds.) Proceedings of the ACM SIGMOD Inter- national Conference on Management of Data, SIGMOD 2012, Scottsdale, AZ, USA, May 20–24, 2012, pp. 673–676. ACM (2012) 49. Krishnamurthy, J., Dasigi, P ., Gardner, M.: Neural semantic parsing with type constraints for semi-structured tables. In: Pro- ceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 1516–1526. Association for Computa- tional Linguistics, Copenhagen, Denmark (2017) 50. Lafferty, J.D., McCallum, A., Pereira, F.C.N.: Conditional random ﬁelds: probabilistic models for segmenting and labeling sequence data. In: Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pp. 282–289. Morgan Kauf- mann Publishers Inc, San Francisco, CA, USA (2001) 51. Lee, C.-H., Polozov, O., Richardson, M.: KaggleDBQA: Realistic evaluation of text-to-SQL parsers. In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (V olume 1: Long Papers), pp. 2261–2273. Association for Computational Linguistics (2021) 52. Lewis, M., Liu, Y ., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V ., Zettlemoyer, L.: BART: denoising sequence-to-sequence pre-training for natural language genera- tion, translation, and comprehension. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguis- tics, pp. 7871–7880. Association for Computational Linguistics (2020). https://doi.org/10.18653/v1/2020.acl-main.703 53. Li, F., Jagadish, H.V .: Constructing an interactive natural language interface for relational databases. PVLDB 8(1), 73–84 (2014) 54. Li, Y ., Raﬁei, D.: Natural language data management and inter- faces: recent development and open challenges. In: Proceedings of the 2017 ACM International Conference on Management of 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,31,page,"A survey on deep learning approaches for text-to-SQL 935 Data, SIGMOD ’17, pp. 1765–1770. Association for Computing Machinery, New Y ork, NY , USA (2017) 55. Li, Y ., Raﬁei, D.: Natural Language Data Management and Interfaces. Synthesis Lectures on Data Management. Morgan & Claypool Publishers, San Rafael (2018) 56. Li, Z., Qu, L., Haffari, G.: Context dependent semantic parsing: a survey. In: Proceedings of the 28th International Conference on Computational Linguistics, pp. 2509–2521. International Com- mittee on Computational Linguistics, Barcelona, Spain (2020) 57. Lin, X.V ., Socher, R., Xiong, C.: Bridging textual and tabular data for cross-domain text-to-SQL semantic parsing. In: Find- ings of the Association for Computational Linguistics: EMNLP 2020, pages 4870–4888. Association for Computational Linguis- tics (2020) 58. Liu, X., He, P ., Chen, W., Gao, J.: Multi-task deep neural networks for natural language understanding. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguis- tics, pp. 4487–4496. Association for Computational Linguistics, Florence (2019). https://doi.org/10.18653/v1/P19-1441 59. Liu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V .: Roberta: a robustly optimized bert pretraining approach (2019) 60. Luo, Y ., Lin, X., Wang, W., Zhou, X.: Spark: top-k keyword query in relational databases. In: ACM SIGMOD, pp. 115–126 (2007) 61. Luong, T., Pham, H., Manning, C. D.: Effective approaches to attention-based neural machine translation. In: Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 1412–1421. Association for Computational Lin- guistics, Lisbon (2015). https://doi.org/10.18653/v1/D15-1166 62. Lyu, Q., Chakrabarti, K., Hathi, S., Kundu, S., Zhang, J., Chen, Z.: Hybrid ranking network for text-to-sql. Technical Report MSR- TR-2020-7, Microsoft Dynamics 365 AI, March (2020) 63. Ma, J., Yan, Z., Pang, S., Zhang, Y ., Shen, J.: Mention extraction and linking for SQL query generation. In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process- ing (EMNLP), pp. 6936–6942. Association for Computational Linguistics (2020) 64. Mandamadiotis, A., Koutrika, G., Eleftherakis, S., Glenis, A., Skoutas, D., Stavrakas, Y .: Datagent: the imminent age of intel- ligent data assistants. Proc. VLDB Endow. 14(12), 2815–2818 (2021) 65. Mikolov, T., Chen, K., Corrado, G., Dean, J.: Efﬁcient estimation of word representations in vector space. In: Bengio, Y ., LeCun, Y . (eds.) 1st International Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2–4, 2013, Workshop Track Proceedings (2013) 66. Notes on ambiguity. http://bit.ly/2YTLFeR 67. Pasupat, P ., Liang, P .: Compositional semantic parsing on semi- structured tables. In: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna- tional Joint Conference on Natural Language Processing (V olume 1: Long Papers), pp. 1470–1480. Association for Computational Linguistics, Beijing, China (2015) 68. Pennington, J., Socher, R., Manning, C.: GloV e: Global vectors for word representation. In: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1532–1543. Association for Computational Linguistics, Doha (2014). https://doi.org/10.3115/v1/D14-1162 69. Popescu, A., Armanasu, A., Etzioni, O., Ko, D., Yates, A.: Mod- ern natural language interfaces to databases: composing statistical parsing with semantic tractability. In: COLING (2004) 70. Popescu, A.-M., Etzioni, O., Kautz, H.: Towards a theory of nat- ural language interfaces to databases. In: Proceedings of the 8th International Conference on Intelligent User Interfaces, IUI ’03, pp. 149–157. Association for Computing Machinery, New Y ork, NY , USA (2003) 71. Price, P .J.: Evaluation of spoken language systems: the atis domain. In: Proceedings of the Workshop on Speech and Natural Language, HLT ’90, pp. 91–95. Association for Computational Linguistics, USA (1990) 72. Quamar, A., Efthymiou, V ., Lei, C., Özcan, F.: Natural lan- guage interfaces to data. Found. Trends Databases 11(4), 319–414 (2022) 73. Quamar, A., Özcan, F., Miller, D., Moore, R.J., Niehus, R., Kreulen, J.: Conversational bi: an ontology-driven conversa- tion system for business intelligence applications. Proc. VLDB Endow. 13(12), 3369–3381 (2020) 74. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., Liu, P .J.: Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. J. Mach. Learn. Res. 21(1), 1532–4435 (2022) 75. Rubin, O., Berant, J.: SmBoP: semi-autoregressive bottom-up semantic parsing. In: Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 311–324. Asso- ciation for Computational Linguistics (2021). https://doi.org/10. 18653/v1/2021.naacl-main.29 76. Scholak, T., Schucher, N., Bahdanau, D.: PICARD: parsing incre- mentally for constrained auto-regressive decoding from language models. In: Proceedings of the 2021 Conference on Empirical Methods in Natural La nguage Processing, pp. 9895–9901. Asso- ciation for Computational Linguistics (2021). https://doi.org/10. 18653/v1/2021.emnlp-main.779 77. Sen, J., Lei, C., Quamar, A., Ozcan, F., Efthymiou, V ., Dalmia, A., Stager, G., Mittal, A., Saha, D., Sankaranarayanan, K.: A THENA++: natural language querying for complex nested sql queries. Proc. VLDB Endow. 13(11), 2747–2759 (2020) 78. Shaw, P ., Massey, P ., Chen, A., Piccinno, F., Altun, Y .: Generating logical forms from graph representations of text and entities. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 95–106. Association for Compu- tational Linguistics, Florence, Italy (2019) 79. Shi, T., Tatwawadi, K., Chakrabarti, K., Mao, Y ., Polozov, O., Chen, W.: Incsql: training incremental text-to-sql parsers with non-deterministic oracles (2018) 80. Shi, T., Zhao, C., Boyd-Graber, J., Daumé III, H., Lee, L.: On the potential of lexico-logical alignments for semantic parsing to SQL queries. In: Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1849–1864 (2020). Association for Computational Linguistics 81. Simitsis, A., Koutrika, G., Ioannidis, Y .: Précis: from unstructured keywords as queries to structured databases as answers. VLDB J. 17(1), 117–149 (2008) 82. Speer, R., Havasi, C.: Representing general relational knowledge in conceptnet 5. In: LREC (2012) 83. Szalay, A.S., Gray, J., Thakar, A.R., Kunszt, P .Z., Malik, T., Rad- dick, J., Stoughton, C., vandenBerg, J.: The sdss skyserver: public access to the sloan digital sky server data. In: Proceedings of the 2002 ACM SIGMOD International Conference on Management of data, pp. 570–581 (2002) 84. Tang, L.R., Mooney, R.J.: Automated construction of database interfaces: intergrating statistical and relational learning for semantic parsing. In: 2000 Joint SIGDA T Conference on Empir- ical Methods in Natural Language Processing and V ery Large Corpora, pp. 133–141. Association for Computational Linguis- tics, Hong Kong, China (2000) 85. Trivedi, P ., Maheshwari, G., Dubey, M., Lehmann, J.: Lc- quad: a corpus for complex question answering over knowledge graphs. In: International Semantic Web Conference, pp. 210–218. Springer (2017) 123"
Katsogiannis-Meimarakis and Koutrika - 2023 - A survey on deep learning approaches for text-to-S.pdf,32,page,"936 G. Katsogiannis-Meimarakis, G. Koutrika 86. Usta, A., Karakayali, A., Ulusoy, O.: Dbtagger: multi-task learn- ing for keyword mapping in nlidbs using bi-directional recurrent neural networks. Proc. VLDB Endow. 14(5), 813–821 (2021) 87. V aswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.: Attention is all you need. In: Proceedings of the 31st International Conference on Neural Information Processing Systems. NIPS’17, pp. 6000–6010. Cur- ran Associates Inc., Red Hook, NY (2017) 88. Vinyals, O., Fortunato, M., Jaitly, N.: Pointer networks. In: Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., Garnett, R. (eds.) Advances in Neural Information Processing Systems, vol. 28. Cur- ran Associates, Inc. (2015) 89. Wang, B., Shin, R., Liu, X., Polozov, O., Richardson, M.: RA T-SQL: relation-aware schema encoding and linking for Text- to-SQL parsers. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7567–7578. Asso- ciation for Computational Linguistics (2020). https://doi.org/10. 18653/v1/2020.acl-main.677 90. Wang, C., Cheung, A., Bodík, R.: Synthesizing highly expressive SQL queries from input-output examples. In: 38th ACM SIG- PLAN, pp. 452–466 (2017) 91. Wang, C., Tatwawadi, K., Brockschmidt, M., Huang, P .-S., Mao, Y ., Polozov, O., Singh, R.: Robust text-to-sql generation with execution-guided decoding (2018) 92. Wang, P ., Shi, T., Reddy, C.K.: Text-to-sql generation for question answering on electronic medical records. In: Proceedings of The Web Conference, vol. 2020, pp. 350–361 (2020) 93. Wang, W., Bhowmick, S.S., Li, H., Joty, S.R., Liu, S., Chen, P .: Towards enhancing database education: natural language gener- ation meets query execution plans. In: Li, G., Li, Z., Idreos, S., Srivastava, D. (eds.) SIGMOD ’21: International Conference on Management of Data, Virtual Event, China, June 20–25, 2021, pp. 1933–1945. ACM (2021) 94. Weir, N., Utama, P ., Galakatos, A., Crotty, A., Ilkhechi, A., Ramaswamy, S., Bhushan, R., Geisler, N., Hättasch, B., Eger, S., Cetintemel, U., Binnig, C.: Dbpal: a fully pluggable nl2sql training pipeline. In: Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data, SIGMOD ’20, pp. 2347–2361. Association for Computing Machinery, New Y ork, NY , USA (2020) 95. Wu, K., Wang, L., Li, Z., Zhang, A., Xiao, X., Wu, H., Zhang, M., Wang, H.: Data augmentation with hierarchical SQL-to-question generation for cross-domain text-to-SQL parsing. In: Proceed- ings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 8974–8983, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics (2021) 96. Xu, X., Liu, C., Song, D.: Sqlnet: generating structured queries from natural language without reinforcement learning (2017) 97. Xu, K., Wang, Y ., Wang, Y ., Wang, Z., Wen, Z., Dong, Y .: SeaD: end-to end Text-to-SQL generation with schema-aware denoising. In: Findings of the Association for Computational Linguistics: NAACL 2022, pp. 1845–1853. Association for Com- putational Linguistics, Seattle (2021). https://doi.org/10.18653/ v1/2022.ﬁndings-naacl.141 98. Yaghmazadeh, N., Wang, Y ., Dillig, I., Dillig, T.: Sqlizer: query synthesis from natural language. In: PACMPL, pp. 63:1–63:26 (2017) 99. Yin, P ., Neubig, G.: A syntactic neural model for general-purpose code generation. In: Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers), pp. 440–450. Association for Computational Linguistics, V ancouver (2017). https://doi.org/10.18653/v1/P17-1041 100. Yin, P ., Neubig, G.: TRANX: a transition-based neural abstract syntax parser for semantic parsing and code generation. In: Pro- ceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 7–12. Asso- ciation for Computational Linguistics, Brussels, Belgium (2018) 101. Yin, P ., Neubig, G., tau Yih, W., Riedel, S.: TaBERT: pretraining for joint understanding of textual and tabular data. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8413–8426. Association for Computational Lin- guistics (2020). https://doi.org/10.18653/v1/2020.acl-main.745 102. Y oon, D., Lee, D., Lee, S.: Dynamic self-attention: computing attention over words dynamically for sentence embedding (2018) 103. Y u, T., Li, Z., Zhang, Z., Zhang, R., Radev, D.: TypeSQL: knowledge-based type-aware neural Text-to-SQL generation. In: Proceedings of the 2018 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, vol. 2 (Short Papers), pp. 588–594. Association for Computational Linguistics, New Orleans (2018). https://doi.org/10.18653/v1/N18-2093 104. Y u, T., Wu, C.-S., Lin, X.V ., Wang, B., Tan, Y .C., Yang, X., Radev, D., Socher, R., Xiong, C.: Grappa: grammar-augmented pre-training for table semantic parsing (2020) 105. Y u, T., Yasunaga, M., Yang, K., Zhang, R., Wang, D., Li, Z., Radev, D.: SyntaxSQLNet: syntax tree networks for complex and cross-domain text-to-SQL task. In: Proceedings of the 2018 Con- ference on Empirical Methods in Natural Language Processing, pp. 1653–1663. Association for Computational Linguistics, Brus- sels (2018). https://doi.org/10.18653/v1/D18-1193 106. Y u, T., Zhang, R., Er, H. Y ., Li, S., Xue, E., Pang, B., Lin, X. V ., Tan, Y . C., Shi, T., Li, Z., Jiang, Y ., Yasunaga, M., Shim, S., Chen, T., Fabbri, A., Li, Z., Chen, L., Zhang, Y ., Dixit, S., Zhang, V ., Xiong, C., Socher, R., Lasecki, W.S., Radev, D.: CoSQL: a con- versational Text-to-SQL challenge towards cross-domain natural language interfaces to databases. In: Proceedings of the 2019 Con- ference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 1962–1979. Association for Computational Linguistics, Hong Kong (2019). https://doi.org/ 10.18653/v1/D19-1204 107. Y u, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z., Ma, J., Li, I., Yao, Q., Roman, S., Zhang, Z., Radev, D.: Spider: a large-scale human-labeled dataset for complex and cross-domain semantic parsing and Text-to-SQL task. In: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 3911–3921. Association for Computational Lin- guistics, Brussels (2018). https://doi.org/10.18653/v1/D18-1425 108. Y u, T., Zhang, R., Yasunaga, M., Tan, Y . C., Lin, X. V ., Li, S., Er, H., Li, I., Pang, B., Chen, T., Ji, E., Dixit, S., Proctor, D., Shim, S., Kraft, J., Zhang, V ., Xiong, C., Socher, R., Radev, D.: SParC: cross-domain semantic parsing in context. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4511–4523. Association for Computational Lin- guistics, Florence (2019). https://doi.org/10.18653/v1/P19-1443 109. Zelle, J.M., Mooney, R.J.: Learning to parse database queries using inductive logic programming. In: Proceedings of the Thir- teenth National Conference on Artiﬁcial Intelligence—V olume 2, AAAI’96, pp. 1050–1055. AAAI Press (1996) 110. Zeng, Z., Lee, M. L., Ling, T. W.: Answering keyword queries involving aggregates and groupby on relational databases. EDBT, pp. 161–172 (2016) 111. Zhao, L., Cao, H., Zhao, Y .: Gp: Context-free grammar pre- training for text-to-sql parsers (2021) 112. Zhong, V ., Xiong, C., Socher, R.: Seq2sql: generating struc- tured queries from natural language using reinforcement learning (2017) Publisher’s Note Springer Nature remains neutral with regard to juris- dictional claims in published maps and institutional afﬁliations. 123"
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,1,page,"Evaluating the Text-to-SQL Capabilities of Large Language Models Nitarshan Rajkumar1∗, Raymond Li2, Dzmitry Bahdanau2345 1University of Cambridge, 2ServiceNow, 3Mila, 4McGill University, 5Canada CIFAR AI Chair nr500@cam.ac.uk, {raymond.li,dzmitry.bahdanau}@servicenow.com https://github.com/nitarshan/codex-text2sql Abstract We perform an empirical evaluation of Text-to- SQL capabilities of the Codex language model. We ﬁnd that, without any ﬁnetuning , Codex is a strong baseline on the Spider benchmark; we also analyze the failure modes of Codex in this setting. Furthermore, we demonstrate on the GeoQuery and Scholar benchmarks that a small number of in-domain examples provided in the prompt enables Codex to perform better than state-of-the-art models ﬁnetuned on such few-shot examples. 1 Introduction Translating natural language questions to SQL queries (Text-to-SQL) is an important business problem which has seen signiﬁcant research in- terest. A common approach to this task involves training a model to produce a SQL query when given a question, a database schema, and possibly database content as inputs. A clear trend in this area is to ﬁnetune models pretrained on natural lan- guage; notably, performance signiﬁcantly improves as larger pretrained models are used (Shaw et al., 2021; Scholak et al., 2021). Recent results from the broader ﬁeld demon- strate that simply scaling training data and model size for generative language models brings ad- vanced capabilities, such as few-shot learning with- out ﬁnetuning (GPT-3, Brown et al., 2020) and code generation (Codex, Chen et al., 2021). In this work we study if such models are already com- petitive Text-to-SQL solutions without any further ﬁnetuning on task-speciﬁc training data, evaluating Codex and GPT-3 models of different sizes with varied prompts on Text-to-SQL benchmarks. We ﬁnd that Codex achieves a competitive per- formance of up to 67% execution accuracy on the Spider development set. We analyze the predicted queries that automatic evaluation judged as wrong ∗Work partially done at Mila and the Université de Mon- tréal. Model V A EX TS Finetuned T5-base 72.7 57.9 54.5 T5-large 84.1 67.2 61.4 T5-3B 87.6 71.4 65.7 T5-3B∗ 88.2 74.4 68.3 T5-3B + PICARD∗ 97.8 79.1 71.7 BRIDGE v2∗ – 68.0 – Inference-only GPT-3 ada 33.8 2.3 0.3 GPT-3 babbage 48.8 5.7 3.9 GPT-3 curie 70.9 12.6 8.3 GPT-3 davinci 65.0 26.3 21.7 Codex cushman∗ 86.3 63.7 53.0 Codex davinci∗ 91.6 67.0 55.1 Table 1: Best Spider development set performance across models, as measured by percentage of predic- tions which are valid SQL (V A), execution accuracy (EX), test-suite accuracy (TS). Models marked with ∗ use database content. T5 results are from Scholak et al. (2021), BRIDGE v2 results are from Lin et al. (2020). and ﬁnd that many of them would be judged correct by humans, whereas others could likely be ﬁxed within the no-ﬁnetuning paradigm. Lastly, using GeoQuery and Scholar benchmarks we show that adapting Codex to a speciﬁc domain by prompting it with few examples can be more effective than ﬁne-tuning a smaller language model on the same examples. 2 Experimental Setup Models Our evaluation focuses on the models ac- cessible via the OpenAI API: GPT-3 (in the as- cending ada, babbage, curie and davinci sizes) and Codex (in the ascending cushman-codex and davinci-codex sizes)1. These are generative lan- guage models which perform next-token prediction during training and inference; GPT-3 is trained on a diverse set of sources from the internet, and Codex is further ﬁnetuned on code from GitHub. We compare GPT-3 and Codex against methods from Shaw et al. (2021) using the T5 encoder-decoder 1See Appendix A.2 for a discussion on parameter counts. arXiv:2204.00498v1 [cs.CL] 15 Mar 2022"
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,62,caption,Table 1: Best Spider development set performance
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,2,page,"model. Starting from public checkpoints pretrained on Common Crawl, the T5 model is ﬁnetuned on Spider to predict the output SQL, conditioned on the question and schema. The 3B parameter T5 model is currently the state-of-the-art on Spider when combined with constrained inference using the PICARD algorithm (Scholak et al., 2021). We also compare to BRIDGE v2 (Lin et al., 2020), a sequence-to-sequence model based on BERT. Zero-Shot Experiments We use the Spider benchmark (Yu et al., 2019) for cross-domain Text- to-SQL. We report performance using percentage of development set predictions which are valid (ex- ecutable) SQLite SQL, execution accuracy, and test-suite execution accuracy. The latter metric was proposed by Zhong et al. (2020) to measure seman- tic equivalence of SQL queries written in different styles, which is essential when comparing Codex to models trained on Spider. We address concerns around possible memorization of Spider data by Codex in Appendix A.5. Few-Shot Experiments We re-purpose the question-splits of the GeoQuery and Scholar datasets (Zelle and Mooney, 1996; Iyer et al., 2017; Finegan-Dollak et al., 2018) to perform experi- ments in a few-shot setting. The examples in these datasets are grouped by query templates. Exam- ples corresponding to the same template have the same SQL query structure, but may have different English questions and SQL literals. To deﬁne the few-shot task, we ﬁrst sort the templates by their frequency in the training set. In the n-shot setting we then use one random example for each of the n most frequent templates. Prompts We use six prompt structures in our experiments (examples provided in Appendix C). Question provides no database information and just includes the question as a SQL comment. API Docs follows the style of the Text-to-SQL example in Codex documentation and includes a schema in a comment style which does not conform to SQLite standards. Select X includes in comments the results of executing a SELECT * FROM T LIMIT X query on each table, including schemas via column headers. Create Table includes the CREATE TABLE commands for each table, in- cluding column type and foreign key declarations. Create Table + Select X2 is a combination of the 2Only the davinci-codex model can evaluate Create Table + Select X prompts with more than 1 row, due to its expanded 4096-token prompt window compared to the 2048-token win- dow of all other models. In addition, GPT-3 models prepro- Prompt V A EX TS Question 14.0 8.3 8.2 API Docs 83.8 56.8 47.5 Select 1 86.3 60.9 52.0 Select 3 85.8 60.3 52.2 Select 5 85.2 60.5 51.5 Select 10 86.0 60.8 51.2 Create Table 89.8 59.9 50.0 + Select 1 92.5 64.8 53.7 + Select 3 91.6 67.0 55.1 + Select 5 91.0 65.3 53.9 + Select 10 91.2 63.3 52.4 Table 2: Spider development set performance across prompt styles on the davinci-codex model, as measured by percentage of predictions which are valid SQL (V A), execution accuracy (EX), test-suite accuracy (TS). preceding two prompt formats. Finally, Fewshot additionally includes question-query pairs. 3 Zero-Shot Results We present results for different model sizes in Ta- ble 1 and for different prompt styles in Table 2. Full results are available in Table 4 in Appendix B. Codex provides a strong baseline for Text-to- SQL tasks In Table 1 the best performing model (davinci-codex, Create Table + Select 3) achieves 67% execution accuracy and 56.5% test suite execu- tion accuracy on Spider. This is comparable to the performance of the BRIDGE v2 (Lin et al., 2020) model which achieved a (then) state-of-the-art 68% execution accuracy in December 2020. Prompt design is critical for performance As seen in Table 2, providing the question alone re- sults in a low 8.3% execution accuracy. There is a progressive improvement to 56.8% as schema information is introduced in API Docs, to 59.9% when valid SQL and foreign key information is used in Create Table, and to 67.0% when database content is introduced with Create Table + Select 3. More database content can harm perfor- mance In Table 2 we observe that for the Select Limit X prompts there is a negligible change in performance when adding more rows. By contrast, Create Table + Select Limit X prompt accuracy peaks with 3 rows before signiﬁcantly decreasing in performance as more rows are added. Diminishing returns for Codex model size While GPT-3 performance signiﬁcantly beneﬁts from increased model size, the davinci-codex model does not perform drastically better than cess whitespace tokens less efﬁciently than Codex models, and therefore cannot evaluate Create Table + Select X prompts at all."
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,65,caption,Table 2: Spider development set performance across
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,3,page,"14% Shortcuts NL What is the number of car models created by the car maker American Motor Company? Gold SELECT count(*) FROM CAR_MAKERS AS T1 JOIN MODEL_LIST AS T2 ON T1.Id = T2.Maker WHERE T1.FullName = ’American Motor Company’; Pred SELECT COUNT(Model) FROM model_list WHERE Maker = 1; NL Give the city and country for the Alton airport. Gold SELECT City , Country FROM AIRPORTS WHERE AirportName = ""Alton"" Pred SELECT City, Country FROM airports WHERE AirportCode = ’ALN’; 8% SELECT Extra Columns NL List names of conductors in descending order of years of work. Gold SELECT Name FROM conductor ORDER BY Year_of_Work DESC Pred SELECT Name, Year_of_Work FROM conductor ORDER BY Year_of_Work DESC; 5% SELECT Convention NL What are all the makers and models? Gold SELECT Maker , Model FROM MODEL_LIST; Pred SELECT DISTINCT car_makers.Maker, model_list.Model FROM car_makers JOIN model_list ON car_makers.Id = model_list.Maker; Figure 1: Examples of error types, as made by the davinci-codex model with Create Table + Select 3 prompt. NL stands for natural language question. Percentage indicates the percentage of errors which are of the given type. Further examples are provided in Figure 3 in Appendix B. cushman-codex. Full results in Table 4 in Ap- pendix B show cushman-codex generally being within 1 percentage point of davinci-codex for the same prompt style; it even performs 3 percentage points better for the Create Table prompt. These results suggest that davinci-codex’s longer context window may be a greater contributor to its peak performance than increased parameter count. 3.1 Error Analysis We focus our error analysis on the davinci-codex model with Create Table + Select 3 prompt, and present a breakdown of prediction types in Table 3 and examples of errors in Figure 1. Our error cate- gories were chosen to surface the most interesting Codex-speciﬁc behaviours we observed amongst the errors made. We randomly selected and anno- tated 100 predictions which were valid SQL yet were judged incorrect by test-suite evaluation. We ﬁrst consider Semantic Incorrect be- haviours, which Spider evaluation and the hu- man annotator both view as incorrect predictions. Shortcut errors are where Codex made use of ei- ther speciﬁc table values or “world knowledge” from GPT-3 pretraining, while the ground-truth query contained the exact literals from the question. GROUP BY Convention errors are where Codex incorrectly groups on a non-primary-key column (such as a name or title column). We also consider Ambiguous Correct be- haviours which are semantically different from the gold query and are therefore judged as incorrect by Spider evaluation, but which the human annotator viewed as being an acceptable SQL translation of Annotation % E% Test-Suite Correct 55.1 – Semantic Incorrect 25.2 69 – Shortcuts 5.1 14 – GROUP BY Convention 1.5 4 – Other 18.6 51 Ambiguous Correct 11.3 31 – SELECT Extra Columns 2.9 8 – SELECT Convention 1.8 5 – Argmax 1.5 4 – Other 5.1 14 Invalid SQL 8.4 – – Ambiguous column name 1.9 – – No such column 4.5 – Table 3: Breakdown of prediction annotations over Spi- der development set for the davinci-codex model with Create Table + Select 3 prompt. % is percentage of all predictions, E% is percentage of manually annotated erroneous queries (see Section Section 3.1 for details). the given question. SELECT Convention errors are where Codex selects a different column than the per-database convention of the gold queries (such as name instead of ID). SELECT Extra Columns errors are where Codex includes additional useful columns in its query beyond what the gold query includes. Argmax errors are where Codex differs from the gold query in how a min/max resolution (such as “youngest singer”) is handled for ties. We observe in Table 3 that a signiﬁcant 31% of valid yet erroneous predictions are penalized by Spider evaluation as being incorrect though a human annotator viewed them as acceptable solu- tions. Future work could be to investigate to what extent one can control the behaviour of Codex. This could allow to ﬁx these ambiguous errors, either by prompt design or using a few examples."
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,18,caption,"Figure 1: Examples of error types, as made by the davinci-codex model with Create Table + Select 3 prompt. NL"
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,68,caption,Table 3: Breakdown of prediction annotations over Spi-
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,4,page,"4 Few-Shot We investigate whether Codex can perform few- shot Text-to-SQL. As described in Section 2, we re-purpose the GeoQuery and Sholar datasets in a few-shot setting. It is well known that models trained on Spider transfer poorly to other single- database Text-to-SQL datasets (Suhr et al., 2020) in a zero-shot setting. Studying few-shot Text-to-SQL on GeoQuery and Scholar should show to what extent models are able to leverage a small amount of examples to effectively adapt to a new domain. Baseline The baseline is a T5-3B model that was ﬁnetuned on Spider, reaching 71% exact-match accuracy on Spider validation set. The model is then further ﬁnetuned on the new domain – Geo- Query or Scholar. The learning rate for domain- speciﬁc-ﬁnetuning was selected in the 20-shot set- ting among [0.1, 0.2, 0.5, 1, 2] · 10−5, based on the best validation set performance after 300 steps. We use batch-size 1024, such that all the few-shot ex- amples ﬁt in the same batch. Codex Building on the Create Table + Select X prompt, we append n question-query examples to the input in an n-shot setting. An example of this prompt is provided in Figure 11. All samples are generated using greedy decoding, with temperature 0. Note that for a given n-shot setting, the baseline and Codex use the same set of support examples. These examples are in the prompt for Codex, and used to ﬁnetune the baseline on the new domain. Given the limited window-size of API models, on GeoQuery we can feed up to 40 support exam- ples to davinci-codex, and up to 10 examples to cushman-codex and GPT-3 models. On Scholar the queries are longer and the schema more complex – we ﬁt only 10 examples in the prompt of davinci- codex, 5 for cushman-codex, and none at all for GPT-3 models. 4.1 Results Figure 2 shows test-suite accuracies on the Scholar and GeoQuery datasets. The baseline reaches 85.7% test-set performance when trained on the complete GeoQuery training set (549 examples). Respectively, it reaches 87.2% test accuracy when trained on the whole Scholar training set (499 ex- amples). This simple baseline is a very compet- itive model when considering the entire datasets. However Figure 2 shows that it is largely beaten by Codex in few-shot settings. In a zero-shot set- ting, both davinci-codex and cushman-codex al- (a) GeoQuery. When trained on the whole GeoQuery training set (549 examples), the ﬁnetuned T5 reaches 85.7% accuracy. (b) Scholar. When trained on the whole Scholar training set (499 examples), the ﬁnetuned T5 reaches 87.2% accuracy. Figure 2: Test-suite accuracy with varying number of support examples. The x-axis shows the number of few- shot examples used. ready beat the baseline on GeoQuery. We spec- ulate that Codex performs well here because it uses the same argmax convention as the GeoQuery dataset, which is different than the convention used in Spider. With up to 40 examples in the prompt, davinci-codex outperforms a T5-3B model ﬁne- tuned on these same examples by a large margin, whereas GPT-3 davinci performs quite poorly on this task. On the other hand, the T5 model outper- forms Codex in a zero-shot setting on Scholar. In 5 and 10-shot settings, Codex shows better adap- tation from these few samples and beats the T5 baseline. 5 Conclusion We demonstrated that generative language mod- els trained on code provide a strong baseline for Text-to-SQL. We also provided analysis of failure modes for these models, which we hope guides fur- ther prompt design (whether few-shot or through natural language instructions) in this setting. Fi- nally, we showed that prompt-based few-shot learn- ing with these models performs competitively with ﬁnetuning-based few-shot learning of smaller mod- els. A clear direction for future work is to evaluate the beneﬁts of ﬁnetuning with Codex models."
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,40,caption,Figure 2 shows test-suite accuracies on the Scholar
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,55,caption,Figure 2: Test-suite accuracy with varying number of
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,5,page,"Acknowledgements Nitarshan performed all zero-shot and ﬁnetuning experiments as well as error-analysis, and wrote most of the paper. Raymond performed all few-shot experiments and the associated writing. Dzmitry supervised, and contributed to paper editing. We thank Dóra Jámbor for insightful discus- sions, Laurent Charlin for providing funding for Nitarshan and for providing feedback on this work, Fraser Kelton and Dave Cummings for support with the OpenAI API, and Ruiqi Zhong for assistance with Spider test suites. We also thank anonymous ARR reviewers for their feedback and criticism in the review process. Nitarshan additionally thanks the city of Mon- tréal and its cafés for providing inspirational set- tings in which to conduct this work. References Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mc- Candlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learn- ers. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka- plan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas- try, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cum- mings, Matthias Plappert, Fotios Chantzis, Eliza- beth Barnes, Ariel Herbert-V oss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welin- der, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Eval- uating large language models trained on code. Xiang Deng, Ahmed Hassan Awadallah, Christopher Meek, Oleksandr Polozov, Huan Sun, and Matthew Richardson. 2021. Structure-grounded pretraining for text-to-sql. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Catherine Finegan-Dollak, Jonathan K. Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, and Dragomir Radev. 2018. Improving text-to-SQL evaluation methodology. In Proceed- ings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers), pages 351–360, Melbourne, Australia. Asso- ciation for Computational Linguistics. Leo Gao. 2021. On the Sizes of OpenAI API Models. Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant Krishnamurthy, and Luke Zettlemoyer. 2017. Learn- ing a neural semantic parser from user feedback. In Proceedings of the 55th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), pages 963–973, Vancouver, Canada. Association for Computational Linguistics. Xi Victoria Lin, Richard Socher, and Caiming Xiong. 2020. Bridging textual and tabular data for cross- domain text-to-sql semantic parsing. Torsten Scholak, Nathan Schucher, and Dzmitry Bah- danau. 2021. Picard: Parsing incrementally for constrained auto-regressive decoding from language models. Peter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova. 2021. Compositional general- ization and natural language variation: Can a se- mantic parsing approach handle both? In Proceed- ings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Interna- tional Joint Conference on Natural Language Pro- cessing (Volume 1: Long Papers) , pages 922–938, Online. Association for Computational Linguistics. Alane Suhr, Ming-Wei Chang, Peter Shaw, and Ken- ton Lee. 2020. Exploring unexplored generalization challenges for cross-database semantic parsing. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics , pages 8372– 8388, Online. Association for Computational Lin- guistics. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. 2019. Spider: A large- scale human-labeled dataset for complex and cross- domain semantic parsing and text-to-sql task. John M. Zelle and Raymond J. Mooney. 1996. Learn- ing to parse database queries using inductive logic programming. In Proceedings of the Thirteenth Na- tional Conference on Artiﬁcial Intelligence - Volume 2, pages 1050–1055. Ruiqi Zhong, Tao Yu, and Dan Klein. 2020. Semantic evaluation for text-to-sql with distilled test suites. Albert Ziegler. 2021. Research recitation."
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,6,page,"A API Details At time of writing, the OpenAI API was ac- cessible at https://openai.com/api/. The example from which our API Docs prompt draws from can be found at https://beta.openai.com/examples/ default-sql-translate. A.1 Hyperparameters We sample 200 tokens from GPT-3 and Codex with temperature 0, with the following strings used as stop tokens to halt generation: “--”, “\n\n”, “;”, “#”. A.2 Parameter Counts Parameter counts for OpenAI API models are not openly available. Gao (2021) evaluated API GPT- 3 models across a variety of language modelling tasks to compare to published results in Brown et al. (2020), ﬁnding that “Ada, Babbage, Curie and Davinci line up closely with 350M, 1.3B, 6.7B, and 175B respectively”. We presume that the davinci- codex model is the same size as the GPT-3 davinci model; cushman-codex is a new model name so we can only guess that it is of a similar (but not the same) size to GPT-3 curie. Nevertheless these remain guesses which should not be relied on. A.3 Model Versioning The exact models served through the OpenAI API may vary over time. We veriﬁed that for each model type, only a single model version was used to generate results. These versions are ada:2020-05-03, babbage:2020-05-03, curie:2020-05-03, davinci:2020-05-03, cushman-codex:2021-08-03, davinci-codex:2021-08-03. A.4 Finetuning In Table 4 we include preliminary results from ﬁne- tuning GPT-3 models on the Spider training set. We used the full training set, and the default ﬁne- tuning settings of 4 epochs, a batch size of 8, and a learning rate multiplier of 0.1. We did not perform a hyperparameter sweep due to the signiﬁcant cost this would incur. A.5 Memorization The Spider development set is available on GitHub, and is therefore possibly in the training set of Codex. We believe that this does not manifest as memorization for our results however, for the fol- lowing reasons. Evaluation data on Spider’s repo is formatted differently to our prompts. Most related is the dev.sql ﬁle, which contains evaluation question- query pairs in the following format: Question 1: ... SQL: ... ... This resembles but isn’t identical to our ""Question"" prompt. We prompted Codex with verbatim frag- ments of this ﬁle and generations failed to replicate any ﬁle contents. Our ""Question"" prompt has very poor performance - hardly an indication of mem- orization from dev.sql. Furthermore, most of Codex’s performance is due to including in the prompt the schemas (see Table 2), which are not present in dev.sql. As well, Codex prediction style is very different to evaluation gold queries. Gold queries make use of a consistent table aliasing strategy (using T1, T2, etc.) which we never see with Codex (see Figure 3 for example comparisons). Furthermore, in Table 4 we reported perfor- mance for all models on spider-realistic (Deng et al., 2021), a modiﬁcation of the spider evaluation set that removes column name references in ques- tions. We observe a similar trend in performance across models as on spider (the consistent perfor- mance drop on spider-realistic is expected due to the difﬁculty of the updated dataset). Memorization cannot account for the performance observed, as spider-realistic is not publicly available on GitHub. Finally, Ziegler (2021) studied memorization in Copilot, a derivative of the Codex models, and found that ""Copilot can quote a body of code ver- batim, but that it rarely does so, and when it does, it mostly quotes code that everybody quotes, and mostly at the beginning of a ﬁle"". Spider evaluation data is rare on GitHub, and we use long contexts in our prompts that signiﬁcantly differ from the ﬁles on GitHub. A.6 Choice of Spider Evaluation Set We chose not to evaluate on the held-out test set of Spider, as this could not be done ofﬂine - it would instead require sending these held-out examples through the API to OpenAI, which risks inadver- tently leaking them for retraining of Codex."
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,7,page,"B Additional Tables and Figures Engine Prompt V A EX TS GPT-3 ada Question 1.2 (1.0) 0.0 (0.0) 0.0 (0.0) ada Docs 3.4 (2.2) 0.2 (0.2) 0.1 (0.0) ada 1 Row 40.1 (34.6) 1.1 (0.6) 0.2 (0.0) ada Schema 33.8 (33.9) 2.3 (3.5) 0.3 (0.0) babbage Question 4.4 (2.0) 1.0 (0.2) 1.0 (0.2) babbage Docs 22.5 (20.3) 1.0 (0.6) 0.7 (0.2) babbage 1 Row 56.0 (49.8) 5.1 (1.6) 3.9 (0.0) babbage Schema 48.8 (44.9) 5.7 (0.8) 3.9 (0.0) curie Question 9.0 (6.7) 2.9 (2.4) 2.5 (1.8) curie Docs 25.2 (25.0) 7.4 (5.5) 6.3 (3.3) curie 1 Row 70.6 (67.3) 10.8 (7.3) 7.6 (1.4) curie Schema 70.9 (72.2) 12.6 (11.0) 8.3 (4.1) davinci Schema 65.0 (65.4) 26.3 (23.2) 21.7 (14.2) Finetuned GPT-3 ada Schema 27.5 (21.3) 20.2 (14.0) 19.1 (13.0) babbage Schema 47.2 (38.0) 34.8 (23.6) 31.9 (20.9) curie Schema 66.9 (60.2) 51.3 (37.8) 46.9 (32.9) Codex cushman Question 11.3 (8.1) 8.5 (3.9) 8.3 (3.9) cushman Docs 83.8 (80.5) 53.2 (45.1) 43.5 (32.3) cushman 1 Row 84.7 (80.9) 59.6 (49.2) 48.5 (32.5) cushman 3 Rows 82.9 (79.1) 60.3 (49.2) 49.4 (33.7) cushman 5 Rows 83.6 (78.3) 61.5 (49.6) 50.4 (33.9) cushman Schema 88.3 (83.1) 62.1 (49.6) 53.1 (36.2) cushman + 1 Row 86.3 (85.0) 63.7 (54.9) 53.0 (39.6) davinci Question 14.0 (8.9) 8.3 (4.5) 8.2 (4.1) davinci Docs 83.8 (87.4) 56.8 (51.8) 47.5 (39.0) davinci 1 Row 86.3 (83.5) 60.9 (54.7) 52.0 (41.3) davinci 3 Rows 85.8 (82.7) 60.3 (53.3) 52.2 (40.0) davinci 5 Rows 85.2 (80.9) 60.5 (51.4) 51.5 (38.4) davinci 10 Rows 86.0 (80.7) 60.8 (53.3) 51.2 (39.2) davinci Schema 89.8 (87.8) 59.9 (52.2) 50.0 (38.4) davinci + 1 Row 92.5 (90.7) 64.8 (58.7) 53.7 (41.7) davinci + 3 Rows 91.6 (90.6) 67.0 (60.2) 55.1 (42.9) davinci + 5 Rows 91.0 (90.2) 65.3 (60.0) 53.9 (42.1) davinci + 10 Rows 91.2 (90.6) 63.3 (58.3) 52.4 (40.2) Table 4: Performance on Spider across all evaluated models and prompts, as measured by percentage of predictions which are valid/executable SQL (V A), execution accuracy (EX), test-suite accuracy (TS). Main results are on the Spider development set, results in parantheses are on Spider-Realistic (Deng et al., 2021), a modiﬁed subset of the Spider development set with explicit references to column names removed from questions."
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,40,caption,"Table 4: Performance on Spider across all evaluated models and prompts, as measured by percentage of predictions"
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,8,page,"14% Shortcuts Q What are the template ids with template type description ""Presentation"". Gold SELECT T2.template_id FROM Ref_template_types AS T1 JOIN Templates AS T2 ON T1.template_type_code = T2.template_type_code WHERE T1.template_type_description = ""Presentation"" Pred SELECT Template_ID FROM Templates WHERE Template_Type_Code = ’PPT’; 8% SELECT Extra Columns NL Which city has the most frequent destination airport? Gold SELECT T1.City FROM AIRPORTS AS T1 JOIN FLIGHTS AS T2 ON T1.AirportCode = T2.DestAirport GROUP BY T1.City ORDER BY count(*) DESC LIMIT 1 Pred SELECT City, COUNT(*) AS ’Number of Destinations’ FROM airports JOIN flights ON airports.AirportCode = flights.DestAirport GROUP BY City ORDER BY COUNT(*) DESC LIMIT 1; 5% SELECT Convention NL Which airlines have a ﬂight with source airport AHD? Gold SELECT T1.Airline FROM AIRLINES AS T1 JOIN FLIGHTS AS T2 ON T1.uid = T2.Airline WHERE T2.SourceAirport = ""AHD"" Pred SELECT Airline FROM flights WHERE SourceAirport = ’AHD’; 4% GROUP BY Convention NL Which makers designed more than 3 car models? List full name and the id. Gold SELECT T1.FullName , T1.Id FROM CAR_MAKERS AS T1 JOIN MODEL_LIST AS T2 ON T1.Id = T2.Maker GROUP BY T1.Id HAVING count(*) > 3; Pred SELECT car_makers.FullName, car_makers.Id FROM car_makers JOIN model_list ON car_makers.Id = model_list.Maker GROUP BY car_makers.FullName HAVING COUNT(model_list.Model) > 3; 4% Argmax NL Give the name, year of independence, and surface area of the country that has the lowest population. Gold SELECT Name, SurfaceArea, IndepYear FROM country ORDER BY Population LIMIT 1 Pred SELECT Name, IndepYear, SurfaceArea FROM country WHERE Population = (SELECT MIN(Population) FROM country); 51% Other - Semantic Incorrect NL Which countries have either English or Dutch as an ofﬁcial language? Gold SELECT * FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code = T2.CountryCode WHERE T2.Language = ""English"" AND IsOfficial = ""T"" UNION SELECT * FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code = T2.CountryCode WHERE T2.Language = ""Dutch"" AND IsOfficial = ""T"" Pred SELECT * FROM countrylanguage WHERE IsOfficial = ’T’ OR IsOfficial = ’F’; NL Show the name and the release year of the song by the youngest singer. Gold SELECT song_name , song_release_year FROM singer ORDER BY age LIMIT 1 Pred SELECT Name, Song_release_year FROM singer WHERE Age = (SELECT MIN(Age) FROM singer); NL What is the maximum miles per gallon of the car with 8 cylinders or produced before 1980 ? Gold select max(mpg) from cars_data where cylinders = 8 or year < 1980 Pred SSELECT MAX(MPG) FROM cars_data WHERE Cylinders = 8 AND Year < 1980; 14% Other - Ambiguous Correct NL Find the ﬁrst name and age of students who have a pet. Gold SELECT DISTINCT T1.fname , T1.age FROM student AS T1 JOIN has_pet AS T2 ON T1.stuid = T2.stuid Pred SELECT Fname, Age FROM Student WHERE StuID IN (SELECT StuID FROM Has_Pet); NL How many countries speak both English and Dutch? Gold SELECT COUNT(*) FROM (SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code = T2.CountryCode WHERE T2.Language = ""English"" INTERSECT SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.Code = T2.CountryCode WHERE T2.Language = ""Dutch"") Pred SELECT COUNT(*) FROM countrylanguage WHERE Language = ’Dutch’ AND CountryCode IN (SELECT CountryCode FROM countrylanguage WHERE Language = ’English’); NL What are the Asian countries which have a population larger than that of any country in Africa? Gold SELECT Name FROM country WHERE Continent = ""Asia"" AND population > (SELECT min(population) FROM country WHERE Continent = ""Africa"") Pred SELECT Name FROM country WHERE Continent = ’Asia’ AND Population > (SELECT MAX(Population) FROM country WHERE Continent = ’Africa’); Figure 3: Additional examples of error types, as made by davinci-codex model with Create Table + Select 3 prompt. NL stands for natural language question. Percentage indicates the percentage of errors which are of the given type."
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,62,caption,"Figure 3: Additional examples of error types, as made by davinci-codex model with Create Table + Select 3 prompt."
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,9,page,"10-shot examples what states does the missouri river run through SELECT RIVERalias0.TRAVERSE FROM RIVER AS RIVERalias0 WHERE RIVERalias0.RIVER_NAME = ""missouri"" ; - what is the size of texas SELECT STATEalias0.AREA FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME = ""texas"" ; - what are the major cities in texas SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION > 150000 AND CITYalias0.STATE_NAME = ""texas"" ; - what is the capital of pennsylvania SELECT STATEalias0.CAPITAL FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME = ""pennsylvania"" ; - what is the biggest city in nebraska SELECT CITYalias0.CITY_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.POPULATION = ( SELECT MAX( CITYalias1.POPULATION ) FROM CITY AS CITYalias1 WHERE CITYalias1.STATE_NAME = ""nebraska"" ) AND CITYalias0.STATE_NAME = ""nebraska"" ; - what is the population of austin SELECT CITYalias0.POPULATION FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = ""austin"" ; - which state is kalamazoo in SELECT CITYalias0.STATE_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = ""kalamazoo"" ; - name all the rivers in colorado SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE = ""colorado"" ; - what states border missouri SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = ""missouri"" ; - how many people live in new mexico SELECT STATEalias0.POPULATION FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME = ""new mexico"" ; Very similar query in the few-shot prompt ﬁxes the example Question which states border iowa Gold SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = ""iowa"" ; 0-shot pred SELECT state_name FROM border WHERE border = ’iowa’ 10-shot pred SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = ""iowa"" Argmax convetion ﬁxed Question what state has the smallest population Gold SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.POPULATION = (SELECT MIN(STATEalias1.POPULATION) FROM STATE AS STATEalias1) ; 0-shot pred SELECT state_name FROM state ORDER BY population LIMIT 1 10-shot pred SELECT STATEalias0.STATE_NAME FROM STATE AS STATEalias0 WHERE STATEalias0.POPULATION = (SELECT MIN(STATEalias1.POPULATION) FROM STATE AS STATEalias1) SELECT extra columns ﬁxed Question what is the population of the state with the largest area Gold SELECT STATEalias0.POPULATION FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = (SELECT MAX(STATEalias1.AREA) FROM STATE AS STATEalias1) ; 0-shot pred SELECT state_name, population FROM state WHERE area = (SELECT MAX(area) FROM state) 10-shot pred SELECT STATEalias0.POPULATION FROM STATE AS STATEalias0 WHERE STATEalias0.AREA = (SELECT MAX(STATEalias1.AREA) FROM STATE AS STATEalias1) Figure 4: Cherry-picked examples of Codex improvements from 0-shot to 10-shot text-to-SQL on GeoQuery validation set. The style of the generated SQL changes a lot and is much closer to that of the gold SQL when few-shot examples are in the prompt. The few-shot examples were also useful to adapt the generated SQL to the conventions of the dataset, like the way argmax is done, or the selected columns."
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,51,caption,Figure 4: Cherry-picked examples of Codex improvements from 0-shot to 10-shot text-to-SQL on GeoQuery
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,10,page,"C Example Prompts What is Kyle’s id? | network_1 | highschooler : id, name ( Kyle ), grade | friend : student_id, friend_id | likes : student_id, liked_id Figure 5: Example input for baseline T5 models. -- Using valid SQLite, answer the following questions. -- What is Kyle’s id? SELECT Figure 6: Example prompt for Question. ### SQLite SQL tables, with their properties: # # Highschooler(ID, name, grade) # Friend(student_id, friend_id) # Likes(student_id, liked_id) # ### What is Kyle’s id? SELECT Figure 7: Example prompt for API Docs."
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,4,caption,Figure 5: Example input for baseline T5 models.
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,8,caption,Figure 6: Example prompt for Question.
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,17,caption,Figure 7: Example prompt for API Docs.
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,11,page,"/* 3 example rows from table Highschooler: SELECT * FROM Highschooler LIMIT 3; Table: Highschooler ID name grade 1510 Jordan 9 1689 Gabriel 9 1381 Tiffany 9 */ /* 3 example rows from table Friend: SELECT * FROM Friend LIMIT 3; Table: Friend student_id friend_id 1510 1381 1510 1689 1689 1709 */ /* 3 example rows from table Likes: SELECT * FROM Likes LIMIT 3; Table: Likes student_id liked_id 1689 1709 1709 1689 1782 1709 */ -- Using valid SQLite, answer the following questions for the tables provided above. -- What is Kyle’s id? SELECT Figure 8: Example prompt for Select 3. CREATE TABLE Highschooler( ID int primary key, name text, grade int) CREATE TABLE Friend( student_id int, friend_id int, primary key (student_id,friend_id), foreign key(student_id) references Highschooler(ID), foreign key (friend_id) references Highschooler(ID) ) CREATE TABLE Likes( student_id int, liked_id int, primary key (student_id, liked_id), foreign key (liked_id) references Highschooler(ID), foreign key (student_id) references Highschooler(ID) ) -- Using valid SQLite, answer the following questions for the tables provided above. -- What is Kyle’s id? SELECT Figure 9: Example prompt for Create Table."
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,31,caption,Figure 8: Example prompt for Select 3.
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,53,caption,Figure 9: Example prompt for Create Table.
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,12,page,"CREATE TABLE Highschooler( ID int primary key, name text, grade int) /* 3 example rows: SELECT * FROM Highschooler LIMIT 3; ID name grade 1510 Jordan 9 1689 Gabriel 9 1381 Tiffany 9 */ CREATE TABLE Friend( student_id int, friend_id int, primary key (student_id,friend_id), foreign key(student_id) references Highschooler(ID), foreign key (friend_id) references Highschooler(ID) ) /* 3 example rows: SELECT * FROM Friend LIMIT 3; student_id friend_id 1510 1381 1510 1689 1689 1709 */ CREATE TABLE Likes( student_id int, liked_id int, primary key (student_id, liked_id), foreign key (liked_id) references Highschooler(ID), foreign key (student_id) references Highschooler(ID) ) /* 3 example rows: SELECT * FROM Likes LIMIT 3; student_id liked_id 1689 1709 1709 1689 1782 1709 */ -- Using valid SQLite, answer the following questions for the tables provided above. -- What is Kyle’s id? SELECT Figure 10: Example prompt for Create Table + Select 3."
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,46,caption,Figure 10: Example prompt for Create Table + Select 3.
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,13,page,"CREATE TABLE ""border_info"" (""state_name"" text, ""border"" text) /* state_name border alabama tennessee alabama georgia alabama florida */ CREATE TABLE ""city"" (""city_name"" text, ""population"" int DEFAULT NULL, ""country_name"" varchar(3) NOT NULL DEFAULT ’’, "" state_name"" text) /* city_name population country_name state_name birmingham 284413 usa alabama mobile 200452 usa alabama montgomery 177857 usa alabama */ CREATE TABLE ""highlow"" (""state_name"" text, ""highest_elevation"" text, ""lowest_point"" text, ""highest_point"" text, "" lowest_elevation"" text) /* state_name highest_elevation lowest_point highest_point lowest_elevation alabama 734 gulf of mexico cheaha mountain 0 alaska 6194 pacific ocean mount mckinley 0 arizona 3851 colorado river humphreys peak 21 */ CREATE TABLE ""lake"" (""lake_name"" text, ""area"" double DEFAULT NULL, ""country_name"" varchar(3) NOT NULL DEFAULT ’’, ""state_name"" text) /* lake_name area country_name state_name iliamna 2675.0 usa alaska becharof 1186.0 usa alaska teshekpuk 816.0 usa alaska */ CREATE TABLE ""mountain"" (""mountain_name"" text, ""mountain_altitude"" int DEFAULT NULL, ""country_name"" varchar(3) NOT NULL DEFAULT ’’, ""state_name"" text) /* mountain_name mountain_altitude country_name state_name mckinley 6194 usa alaska st. elias 5489 usa alaska foraker 5304 usa alaska */ CREATE TABLE ""river"" (""river_name"" text, ""length"" int DEFAULT NULL, ""country_name"" varchar(3) NOT NULL DEFAULT ’’, ""traverse"" text) /* river_name length country_name traverse mississippi 3778 usa minnesota mississippi 3778 usa wisconsin mississippi 3778 usa iowa */ CREATE TABLE ""state"" (""state_name"" text, ""population"" int DEFAULT NULL, ""area"" double DEFAULT NULL, ""country_name"" varchar(3) NOT NULL DEFAULT ’’, ""capital"" text, ""density"" double DEFAULT NULL) /* state_name population area country_name capital density alabama 3894000 51700.0 usa montgomery 75.319149 alaska 401800 591000.0 usa juneau 0.679865 arizona 2718000 114000.0 usa phoenix 23.842105 */ -- Using valid SQLite, answer the following questions for the tables provided above. -- what is the population of austin SELECT CITYalias0.POPULATION FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = ""austin"" ; -- which state is kalamazoo in SELECT CITYalias0.STATE_NAME FROM CITY AS CITYalias0 WHERE CITYalias0.CITY_NAME = ""kalamazoo"" ; -- name all the rivers in colorado SELECT RIVERalias0.RIVER_NAME FROM RIVER AS RIVERalias0 WHERE RIVERalias0.TRAVERSE = ""colorado"" ; -- how many people live in new mexico SELECT STATEalias0.POPULATION FROM STATE AS STATEalias0 WHERE STATEalias0.STATE_NAME = ""new mexico"" ; -- what states border missouri SELECT BORDER_INFOalias0.BORDER FROM BORDER_INFO AS BORDER_INFOalias0 WHERE BORDER_INFOalias0.STATE_NAME = ""missouri"" ; -- what is the biggest city in arizona SELECT Figure 11: Example prompt for 5-shot. It starts with the schema and 3 rows per database (exactly as in Figure 10), followed by 5 few-shot examples, and ﬁnally the target question."
Rajkumar et al. - 2022 - Evaluating the Text-to-SQL Capabilities of Large L.pdf,69,caption,"Figure 11: Example prompt for 5-shot. It starts with the schema and 3 rows per database (exactly as in Figure 10),"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,1,page,"Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation Bin Zhang1,2,3,∗ Yuxiao Ye1,4,∗ Guoqing Du1,∗ Xiaoru Hu1,∗ Zhishuai Li1 Sun Yang1 Chi Harold Liu4 Rui Zhao1 Ziyue Li1 Hangyu Mao1,B 1Sensetime Research 2Institute of Automation, Chinese Academy of Sciences 3School of Artificial Intelligence, University of Chinese Academy of Sciences 4School of Computer Science and Technology, Beijing Institute of Technology Abstract Large Language Models (LLMs) have emerged as a powerful tool in advancing the Text-to-SQL task, significantly outperforming traditional methods. Nevertheless, as a nascent research field, there is still no consensus on the optimal prompt templates and design frameworks. Additionally, existing benchmarks inadequately explore the performance of LLMs across the various sub-tasks of the Text-to-SQL process, which hinders the assessment of LLMs’ cognitive capabilities and the optimization of LLM-based solutions. To address the aforementioned issues, we firstly construct a new dataset designed to mitigate the risk of overfitting in LLMs. Then we formulate five evaluation tasks to comprehensively assess the performance of diverse methods across various LLMs throughout the Text-to-SQL process. Our study highlights the performance disparities among LLMs and proposes optimal in-context learning solutions tailored to each task. These findings offer valuable insights for facilitating the development of LLM-based Text-to-SQL systems. 1 Introduction Text-to-SQL, which involves the automatic transformation of natural language (NL) questions into structured SQL statements, is a pivotal component in facilitating seamless user interaction with databases [35]. Previous approaches to this task primarily focus on pattern matching between natural language and SQL statements, utilizing machine learning models to acquire the mapping between the two [60, 21]. However, the introduction and rapid advancement of Large Language Models (LLMs) have brought about a substantial transformation in this field [9, 33]. LLMs have emerged as powerful tools [38, 18, 43], showcasing tremendous potential in comprehending complex NL questions and generating accurate SQL statements. By combining advanced reasoning techniques and in-context learning capabilities, LLMs have significantly pushed the boundaries of the state-of-the-art in this domain, outperforming traditional methods by a considerable margin. Despite the continuous improvement of LLM-based methods in various benchmarks such as Spi- der [ 56] and BIRD [ 24], there remains a critical gap in the systematic benchmarking of these solutions [17, 35, 19]. The absence of a comprehensive framework for evaluating LLMs in Text-to- SQL complicates the design and assessment of effective systems. The risk of overfitting, particularly for LLMs trained on coding tasks and open-source datasets, poses a significant challenge to the reliability of benchmark evaluations [37]. Additionally, the optimal prompt engineering strategies B Corresponding author: {hy.mao@pku.edu.cn, maohangyu@sensetime.com} ∗These authors contributed equally. Preprint. Under review. arXiv:2403.02951v2 [cs.CL] 6 Mar 2024"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,2,page,"User Database User question 𝓠 Database scheme 𝓢 Schema Linking Zero Shot Few Shot PreSQL Text-to-SQLSQL-to-Text LLM 𝓜 SQL Debugging SQL Optimization Correct SQL Wrong SQL Self-Debugging Multi-Round Debugging General Debugging System Error Result Error Error Information Analyze what problem this SQL statement is solving. 𝓨:SELECT xx from xx WHERE xx Output 𝓠: What xxx? LLM 𝓜 Answer the question by sqlite SQL query. Database and Question: $ {𝓠,𝓢′} Output 𝓨: SELECT xx from xx WHERE xx LLM 𝓜 Rewrite and optimize the given SQL query. Database and Question: $ {𝓠,𝓢} Output 𝓨: SELECT xx from xx WHERE xx Dataset Construction Question Database DDL SQL Query Difficulty BIRD Bigtable Figure 1: Benchmarking tasks in Text-to-SQL pipeline. that play a crucial role in guiding LLMs to generate accurate SQL queries are yet to be determined. Although various design schemes are explored in different methods [13], there is no consensus on the most effective prompt template. Furthermore, the current benchmarks, while comprehensive in their assessment of end-to-end Text-to-SQL tasks, have not yet provided a detailed exploration of the models’ performance across the various sub-tasks and components of the Text-to-SQL pro- cess [32, 27]. A detailed exploration of these sub-tasks is crucial for a thorough evaluation of LLMs’ cognitive capabilities and their role in facilitating the Text-to-SQL process. Therefore, it is necessary to develop a more granular benchmarking approach that can accurately reflect the multifaceted nature of Text-to-SQL and inform the creation of more effective LLM-based solutions. To address the aforementioned challenges and fill the gap in the systematic benchmarking of LLMs in Text-to-SQL, we construct a comprehensive testing benchmark that provides a holistic assessment of LLM capabilities in this domain. Our approach begins with the construction of a Text-to-SQL dataset, designed to mitigate the risk of overfitting by considering question complexities, database sizes, and prerequisite knowledge. Formally, we devise five distinct tasks—Text-to-SQL, SQL Debugging, SQL Optimization, Schema Linking and SQL-to-Text—to comprehensively evaluate the capabilities of LLMs across the full spectrum of the Text-to-SQL process (see Figure 1). Subsequently, we perform an extensive analysis of various techniques that are essential for improving the in-context learning abilities of LLMs and their precision in generating SQL queries. Specifically, our evaluations are summarized as follows: • To determine the optimal prompt template, we partition the prompt text into distinct compo- nents and perform thorough testing of LLMs’ performance on end-to-end Text-to-SQL tasks across all possible combination patterns. • Our benchmarking approach encompasses a range of LLMs, including both general-purpose and coding-specific models with varying parameter sizes. We determine the performance boundaries of these models and identify their performance disparities (see Figure 2). • For each task, we systematically assess the impact of information granularity on model performance and identify the optimal context learning strategies, such as zero-shot and few-shot, to maximize the performance of the models. 2 Related Work 2.1 Traditional Learning-based Text-to-SQL Methods Numerous traditional learning-based Text-to-SQL methods existed before the emergence of LLMs [60, 52, 29]. These methods can be essentially divided into non-seq2seq and seq2seq methods according to their network architecture. In non-seq2seq methods, representative works [46, 2, 1, 15] typically employ a relation-aware self-attention mechanism as encoder to learn representations of questions and schemas, and then use a grammar-based decoder to generate the SQL as an abstract syntax tree [54], or utilize a sketch-based decoder to obtain the SQL via slot-filling [ 8, 16, 5]. These methods can further benefit from leveraging pre-trained language models like BERT and its extensions [7, 28, 55] for input embedding initialization. As another line of research, seq2seq methods [36, 26, 34, 23] directly translate NL questions into SQL queries through transformer-based [45] architectures in an end-to-end manner. These methods obtain competitive performance by fine-tuning rather than training 2"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,47,caption,Figure 1: Benchmarking tasks in Text-to-SQL pipeline.
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,3,page,"Figure 2: Overall performance of different LLMs in various sub-tasks. from scratch, with less effort. They typically use smarter decoding techniques (e.g., constraining the predictions of the decoder [39], or schema-aware denoising [50]) to prevent the production of invalid SQLs. RESDSQL [ 21] further decouples the intertwined process of schema linking (determining the schema items like tables and columns in a SQL) and skeleton parsing (determining the SQL keywords) , which alleviates the difficulty of Text-to-SQL. Apart from network architectures, the paradigms employed by these methods vary greatly in terms of input encoding, output decoding, neural training, output refinement, making Text-to-SQL a flourishing research area [17]. However, the highest accuracy on the Spider leaderboard achieved by traditional learning-based methods is 79.9%, which is still far from being a reliable Text-to-SQL parser. 2.2 LLM-based Text-to-SQL Methods With the advancements in LLMs, researchers are increasingly interest in creating a natural language interface for relational databases through the powerful linguistic and coding capabilities of LLMs, forge a new trend of LLM-based Text-to-SQL. Given the powerful zero-shot reasoning and domain generalization abilities of LLMs, these methods successively refresh the record on the cross-domain Spider leaderboard. C3 [9], a zero-shot Text-to-SQL method built upon ChatGPT, provides treatment in terms of model input, bias and output, reaching an 82.3% execution accuracy on Spider leaderboard. DIN-SQL [33] proposes decomposing the Text-to-SQL task into smaller sub-tasks effectively and achieves an accuracy of 85.3% on Spider. DAIL-SQL [ 13] refreshes the accuracy of Spider with 86.6% through both supervised fine-tuning and a systematic study of in-context learning. It explores how to select the most helpful example and organize them properly in prompts in a few-shot scenario. Similarly, other studies investigate the selection of few-shot demonstrations by synthesising in- domain examples [3] and retrieving question skeletons [14]. MAC-SQL [ 47] utilizes multi-agent collaboration for Text-to-SQL tasks and reach an accuracy of 59.6% on more challenging BIRD. Several other studies focus on special yet non-trivial scenarios, aiming to expand the scope of the Text-to-SQL task. DBCopilot [48] considers a more realistic problem, wherein it deals with large- scale schemas, characterized by massive databases and a large number of tables. It proposes the use of a lightweight seq2seq copilot model for schema-routing, increasing scalability in comparison to traditional schema-linking. ACT-SQL [58] designs a chain-of-thought (CoT) [49] prompt to improve the reasoning ability when generating SQLs, and extended to the multi-turn Text-to-SQL task [22]. In summary, these methods primarily focus on improving the performance of the overall Text-to-SQL task through several aspects of the sub-tasks of interest in this paper. However, these methods 1) do not individually and systematically study the performance of sub-tasks, and 2) typically rely on experiments using only OpenAI LLMs (ChatGPT, GPT4, etc.), without verifying the robustness of their approaches when apply to other open-source LLMs. 2.3 Datasets and Evaluation Metrics WikiSQL [60] is considered the first large-scale dataset enabling the training and evaluation of learning-based Text-to-SQL methods, as well as offering a standardized benchmark for straightfor- ward comparison across various methods. It is also known as a cross-domain dataset, featuring over 3"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,1,caption,Figure 2: Overall performance of different LLMs in various sub-tasks.
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,4,page,"25,000 tables and 80,000 question-SQL pairs comprising various domains derived from Wikipedia. However, the SQL queries in WikiSQL exhibit low complexity. Subsequently, most of the recent Text-to-SQL works are done onSpider [56] because it is widely acknowledged as the most challenging cross-domain benchmark. It comprises 10,181 queries covering 138 different domains, involves multi-table queries (embodied by JOIN), complex SQL clauses (ORDER BY , GROUP BY , and HA VING, etc.) and nested SQLs. Several variants of Spider have been designed to evaluate the adaptability of Text-to-SQL methods [40, 12]. Spider-Realistic [6] removes the explicit mention of column names in the NL questions while keeping the SQL queries unchanged, which is more aligned with the use-case of Text-to-SQL in practice. Spider-Syn [ 10] replaces some schema words in the NL question with their synonyms that reflect real-world question paraphrases. Eliminating such explicit correspondence between NL questions and table schemas poses a greater challenge of Text-to-SQL methods. Spider-DK [ 11] integrates artificial domain knowledge, aiming to investigate the robustness of text-to-SQL models when the questions require rarely observed domain knowledge. Given that Spider is specifically designed for benchmarking Text-to-SQL methods while diverging from practical scenarios, KaggleDBQA [ 20] constructs a small-scale cross-domain dataset from Kaggle, highlighting the diversity of actual web-sourced databases. It comprises knowledge such as documentation and metadata of databases, raising an question of how this extra information could be used to improve the performance. The newest seminal benchmark is BIRD [24], involving 12,751 Text-to-SQL pairs and 95 databases with a size of 33.4 GB. It incorporates the advantages of previous datasets, such as Spider (which is cross-domain with complex SQLs) and KaggleDBQA (which requires the ability to use external knowledge evidence). It is the first to be curated for evaluating SQL execution efficiency in large-scale databases, thereby further bridging the gap between academic setups and real-world applications. In this paper, we construct a novel dataset built upon BIRD and use it in the evaluation (see Section 3.3). Two primary evaluation metrics for assessing the accuracy of SQLs on Spider are Exact Matching(EM) and Execution Accuracy(EX). EM measures whether the predicted query as a whole is equivalent to the gold query. It is possible to encounter false negative evaluations since a question might be solvable by multiple syntactically different but semantically identical SQL statements. EX is a more widely used metric, measures whether the result of executing the predicted query matches the gold value. We use EX to evaluate the accuracy of SQLs in this paper. Additionally, BIRD further proposes Valid Efficiency Score (VES), an integrated metric assessing both accuracy of execution results (i.e., EX) and the execution efficiency of SQL queries. To increase the VES, methods are required to enhance both execution accuracy and efficiency of SQL queries. We use this metric to assess the SQL optimization in Section 4.3. Refer to Appendix C.1 for detailed definitions of evaluation metrics. 3 Settings 3.1 Task Formulation In an LLM-based Text-to-SQL system, LLMs are employed to facilitate the transformation of natural language questions into executable SQL queries. Specifically, Let Q be a natural language question and S be the database schema. S is defined by a tuple S = (T , C, K), where T = {t1, ..., tm} represents multiple tables, C = {c1, ..., cn} represents columns, and K represents foreign key relationships. The goal is to produce a SQL query Y such that Y is executable and accurately represents the intent of Q. Given the prompt template P(Q, S), the generation process of the SQL query Y by an LLM M can be formally defined as a conditional probability distribution: PM(Y|P(Q, S)) = |Y|Y i=1 PM(Yi|P(Q, S), Y1:i−1). (1) Here, LLM autoregressively generates each token, Yi denotes the i-th token of the SQL query Y, and |Y| denotes the length of the query Y. 4"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,5,page,"3.2 Evaluation Models Our benchmarking study evaluates the performance of two distinct categories of LLMs with varying parameter sizes: general-purpose and coding-specific. General-purpose LLMs are designed for versatile text generation and comprehension across diverse domains, trained on extensive internet text datasets. Specifically, ChatGPT (gpt-35-turbo-16k) 2, LLaMa2-Chat-70B [44], InternLM-70B 3and InternLM2-20B are selected as the main baseline models. Coding-specific LLMs are fine-tuned and optimized for programming scenarios, excelling in code generation and technical language understanding. In this paper, the performance analysis of Codellama-34B [37] and SQLCoder-34B 4 are provided. 3.3 Dataset Construction Table 1: EX (%) of different LLMs on open source datasets. SQLCoder-34B InternLM-70B Codellama-34B LLama2-Chat-70B Spider Dev 65.00 67.40 71.60 54.70 BIRD Dev 32.07 29.60 28.29 20.60 We conduct a preliminary assessment of the performance of various LLMs on multiple open-source datasets. As depicted in Table 1, the performance of LLMs varies inconsistently across differ- ent datasets. Specifically, on the Spider dataset, Codellama-34B outperforms InternLM-70B and SQLCoder-34B, while on the Bird dataset, SQLCoder-34B surpasses InternLM-70B and Codellama- 34B. On the one hand, there may be differences in the problem types that different LLMs excel at handling. On the other hand, considering that LLMs learn and train from large corpora, these findings suggest that the performance discrepancies observed could be attributed to the potential utilization of open source datasets during the fine-tuning process for coding-specific LLMs. This poses challenges in ensuring the reliability of evaluation results obtained on these datasets. To address the potential overfitting of LLMs, particularly those specialized in coding tasks, and to ensure a reliable and accurate assessment of their capabilities, we construct a novel dataset, termed “BigTable-0.2k”. This dataset is an extension and augmentation of the BIRD dataset, which is a recently released and widely acknowledged benchmark for evaluating Text-to-SQL parsing. Table 2: The data distribution of “BigTable-0.2k” includes the average number of ground truth (GT) tables and columns involved in the instances. No. of GT Tables No. of Instances Avg. No. of Columns 1 50 23.30 2 50 56.94 3 50 31.73 >3 50 29.04 Specifically, our construction process involves a systematic analysis of the original BIRD dataset, identifying queries of varying difficulty levels and involving different numbers of tables (1, 2, 3, and more than 3). We modify and expand these queries by altering table and column names, as well as filtering conditions, to create a more diverse set of challenges. In cases where the original dataset lacks sufficient examples with four or more tables (there are only 20 instances in BIRD-Dev dataset), queries that involved three tables are expanded to four. As shown in Table 2, this process generates 50 new instances for each category, resulting in the “BigTable-0.2k” dataset. Moreover, each item in the dataset underwent mutual verification by at least two individuals to ensure the accuracy. Although the prevalence of queries involving more than three tables may not align with common real- world applications, this approach allows for a more nuanced evaluation of LLMs’ cognitive abilities across a spectrum of sub-tasks. Additionally, the “BigTable-0.2k” dataset retains the original BIRD dataset’s attributes, such as its large-scale and complexity, diversity of data sources, cross-domain applicability, and the requirement for external knowledge reasoning. Moreover, we standardize the 2https://chat.openai.com 3https://internlm.intern-ai.org.cn 4https://github.com/defog-ai/sqlcoder 5"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,11,caption,Table 1: EX (%) of different LLMs on open source datasets.
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,28,caption,Table 2: The data distribution of “BigTable-0.2k” includes the average number of ground truth (GT)
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,6,page,"dataset format to align with other benchmarks, by combining user NL questions Q with external knowledge E to form new, contextually rich questions. 4 Evaluation In this section, we formally evaluate the different sub-tasks within the Text-to-SQL process to determine the performance differences among various LLMs and provide recommendations for addressing specific task requirements. 1 DDL Format : 2 CREATE TABLE stadium ( stadium_id NUMBER PRIMARY KEY , location TEXT ,↰name TEXT , capacity NUMBER , highest NUMBER , lowest NUMBER , average↰NUMBER ); 3 SimpleDDL Format : 4 stadium ( Stadium_ID , Location ,Name , Capacity , Highest , Lowest , Average ); Listing 1: Examples of “DDL” and “SimpleDDL” formats. 1 MD Format : 2 ### How many singers do we have ? 3 HTML Format : 4 <Question > How many singers do we have ? </ Question > 5 Coding Format : 6 /* How many singers do we have ? */ Listing 2: Examples of “MD”, “HTML” and “Coding” formats. 1 Complete Format : 2 Question : How many singers do we have ? 3 SQL : SELECT 4 Chat Format : 5 Question : How many singers do we have ? 6 SQL : Listing 3: Examples of “Complete” and “Chat” formats. 4.1 Text2SQL 4.1.1 Zero-shot Prompting Optimization. Unlike previous learning-based studies, the primary challenge in LLM-based Text-to-SQL is the design of an effective prompt template P (as introduced in Section 3.1) for LLMs to generate accurate SQL queries, known as prompt engineering. Researchers have evaluated a variety of prompt templates [13]. However, these representations lack uniformity in their structure, making it difficult to find out how a specific feature within a prompt template impact performance. To address this issue, we investigate a more unified series of prompt templates. As shown in Listing 1- 3, these templates differ across three features: • DDL/SimpleDDL prefix affects the representation of the database schema S. – “DDL” (Data Definition Language) encompasses the standardized language that in- cludes commands for defining the structure and properties of a database, providing detailed information necessary for database creation, including column types and primary/foreign keys. – “SimpleDDL” is simplified by only supplying table and column names. • MD/HTML/Coding infix wraps the entire prompt template with Markdown syntax, HTML snippets and code comment blocks. • Complete/Chat postfix indicates the task of either completing SQL statements based on the ""SELECT"" clause or directly answering questions. 6"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,7,page,"1 ### Answer the question by sqlite SQL query only and with no↰explanation 2 ### Sqlite SQL tables , with their properties : 3 # 4 # stadium ( Stadium_ID , Location ,Name , Capacity , Highest , Lowest , Average ); 5 # singer ( Singer_ID ,Name , Country , Song_Name , Song_release_year ,Age ,↰Is_male ); 6 # concert ( concert_ID , concert_Name , Theme , Stadium_ID , Year ); 7 # singer_in_concert ( concert_ID , Singer_ID ). 8 # 9 ### How many singers do we have ? 10 ### SQL : Listing 4: Proposed prompt template “SimpleDDL-MD-Chat”. Table 3: EX (%) of 8 zero-shot prompt templates on Spider dev set. prompt template SQLCoder-34B Codellama-34B InternLM-70B Llama2-Chat-70B DDL-HTML-Chat 57.8 63.7 65.0 49.6 DDL-HTML-Complete 61.8 65.2 53.8 50.2 DDL-MD-Chat 63.2 68.4 66.3 48.7 DDL-MD-Complete 62.4 69.8 64.1 46.8 DDL-Coding-Chat 60.3 67.1 66.1 48.4 DDL-Coding-Complete 59.7 66.9 62.9 53.4 SimpleDDL-MD-Chat 65.0 71.6 67.4 54.7 SimpleDDL-MD-Complete 63.3 66.0 61.7 50.2 These features are combined to form a complete prompt template P, and more details of these representations can be found in Appendix A.1. We test these templates on Spider dev set. As shown in Table 3, “SimpleDDL-MD-Chat” (see Listing 4) consistently outperforms all other prompts when applied to all 5 backbone LLMs. To this end, we consistently utilize the prompt template “SimpleDDL-MD-Chat” throughout the subsequent evaluations in this paper. Core Conclusion 1. The prompt template “SimpleDDL-MD-Chat” achieves optimal performance in the Text-to-SQL task. Table 4: EX (%) of different LLMs on “BigTable-0.2k”. Prompt template is “SimpleDDL-MD-Chat"". No. of GT Tables ChatGPT SQLCoder-34B Codellama-34B InternLM-70B Llama2-Chat-70B InternLM2-20B 1 60.00 44.00 32.00 44.00 22.00 40.00 2 20.00 8.00 10.00 10.00 10.00 6.00 3 38.00 24.00 22.00 20.00 8.00 18.00 >3 30.00 20.00 12.00 10.00 8.00 20.00 Total 37.00 24.00 19.00 21.00 12.00 21.00 4.1.2 End-to-End Text2SQL evaluation. This research conducts an end-to-end evaluation of the Text-to-SQL capabilities of various LLMs using the ""SimpleDDL-MD-Chat"" prompt template on the ""BigTable-0.2k"" dataset, with results depicted in Table 4. Comparison of Performance Across Different Models.The results demonstrate a clear performance hierarchy among the models, with SQLCoder, CodeLlama, InternLM, and InternLM2 consistently outperforming Llama2-Chat. This finding highlights the effectiveness of coding-specific models, such as SQLCoder and CodeLlama, in the Text-to-SQL domain. Additionally, certain general-purpose models, like InternLM and InternLM2, can achieve performance levels comparable to specialized models, even without fine-tuning for coding tasks. Difficulty Comparison Across Different Numbers of GT Tables. We examine the query difficulty based on the number of GT tables involved. The results reveal a decrease in EX as the number of GT tables increases. Notably, the EX for models on queries with two GT tables is unexpectedly lower 7"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,12,caption,Table 3: EX (%) of 8 zero-shot prompt templates on Spider dev set.
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,29,caption,"Table 4: EX (%) of different LLMs on “BigTable-0.2k”. Prompt template is “SimpleDDL-MD-Chat""."
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,8,page,"(a) SQLCoder (b) Codellama (c) LLama2 (d) InternLM (e) InternLM2 Figure 3: Word cloud representation of error information for incorrect SQL queries generated by LLMs. Top: System Error and Result Error. Bottom: Detailed classification of Result Error. compared to those with three or more GT tables. This observation can be attributed to the fact that queries involving two GT tables have the highest average number of columns (see Table 2). Core Conclusion 2. As the number of tables and columns involved in user queries increases, the Text-to-SQL challenge for LLMs significantly escalates. 4.2 SQL Debugging In recent studies, researchers have demonstrated that LLMs possess self-reflection and self-correction capabilities similar to those of humans [53, 41, 57]. Additionally, previous studies also investigate the potential of LLMs to debug the code they generate [4, 33]. In this section, we provide a comprehensive analysis of the performance of numerous SQL debugging methods across LLMs. 4.2.1 Debugging Dataset We collect incorrect SQL queries generated by various LLMs in Section 4.1.2 and visualize the distribution of their error information, as shown in Figure 3. The error information can be divided into two categories: • System Error refers to syntax errors in the SQL statement, and the detailed system error information is generated by the Database Management System (DBMS), e.g., “syntax error” and “no such column”. • Result Error indicates that the syntax of the SQL statement is correct, but the execution result does not match the ground truth. The word cloud distribution reveals that “no such column” and Result Error are the primary areas of error concentration for all models. Additionally, more advanced models exhibit a greater proportion of the Result Error. This aligns with expectations, as powerful models are less prone to low-level System Errors. However, the concise nature of the error information in the Result Error category significantly hampers the debugging performance. Therefore, we propose a further detailed classification method. Specifically, in addressing the Result Error category, we categorize these errors based on the logical construction of SQL statements. This classification is prioritized according to the logical structure within the SQL query. It is delineated in order into the following five subcategories: (1) Table Query Errorpertains to issues related to the selection of tables in the SQL query. It is further subdivided into three types: Excessive/Missing/Incorrect Tables, which respectively address scenarios where unnecessary tables are included, required tables are omitted, or the wrong tables are referenced. (2) Column Selection Error focuses on the appropriateness of column selection. Similar to the Table Query Error, it is broken down into Excessive/Missing/Incorrect Columns. (3) Join Columns Error examines the errors associated with JOIN operations. (4) Condition Filter Error encompasses errors that occur in the conditions used to filter the data, including incorrect comparisons or misapplied filters. 8"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,2,caption,Figure 3: Word cloud representation of error information for incorrect SQL queries generated by
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,9,page,"Figure 4: EX (%) improvement brought by self debug. Figure 5: Left: EX (%) improvement brought by multi-round self debug. Right: Numbers of wrong SQL queries of detailed error type during the process of multi-round self debug. (5) Data Processing Error pertains to errors in the data processing stage, which includes aggregations, calculations, etc. applied to the data within the SQL query. During the categorization process, we employ rules to determine the first three error types and utilize LLMs to perform binary classification for the last two error types (see Appendix A.2). The distribution of different subcategories within Result Error is shown in the bottom section of Figure 3. 4.2.2 Debug Evaluation To assess the impact of different levels of information granularity on performance, we propose 5 distinct strategies for self-debugging, progressively incorporating more detailed information: • Regenerate. Simply regenerate the SQL query with the same prompt in Section 4.1. This setting acts as a baseline to eliminate the impact of model randomness. • w/ Wrong SQL. Let LLMs generate a new SQL query based on the wrong SQL statement. • w/ Wrong SQL + System_error_info. Provide the wrong SQL statement, the corresponding System Error information and the rough Result Error information. • w/ Wrong SQL + All_error_info. Add detailed Result Error information for those SQL queries that are syntactically correct but semantically wrong. • w/ Wrong SQL + All_error_info + Comment . Add manual annotations for all error information. See Appendix A.2 for a detailed prompt template. What is the most powerful information organization of self debug? As shown in Figure 4, it is evident that the self-debugging performance of LLMs exhibits an upward trend with the introduction of more granular error information. In the absence of additional information, LLM does not possess the capability to regenerate correct answers. However, all models are able to comprehend fine-grained error information, whether it includes comments or not, and rectify their own mistakes. Core Conclusion 3. Detailed error information and corresponding annotations greatly enhance the capabilities of LLMs, enabling them to effectively correct errors. Can LLMs benefit from multi-round self debug? As shown in Figure 5, substantial improvements in EX are achieved in the initial rounds of debugging, yet the performance gain become marginal later. This indicates that conducting 1-2 debugging rounds might strikes a favorable balance between 9"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,1,caption,Figure 4: EX (%) improvement brought by self debug.
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,2,caption,Figure 5: Left: EX (%) improvement brought by multi-round self debug. Right: Numbers of wrong
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,10,page,"Table 5: EX (%) results of general debug. Each column demonstrates the debugging results of SQLCoder or InternLM for the wrong SQL statements generated by the corresponding model. Dataset−→ SQLCoder-34B Codellama-34B InternLM-70B Llama2-Chat-70B SQLCoder Debugs All Errors Regenerate 0.00 12.42 13.21 18.75 w/ Wrong SQL 0.66 2.48 6.92 7.95 w/ Wrong SQL + System_error_info 4.64 11.18 9.43 11.36 w/ Wrong SQL + All_error_info 4.64 11.80 8.81 10.80 InternLM Debugs All Errors Regenerate 9.27 11.80 0.63 15.34 w/ Wrong SQL 7.95 10.56 5.66 9.66 w/ Wrong SQL + System_error_info10.60 10.56 8.81 11.93 w/ Wrong SQL + All_error_info 11.92 11.80 8.81 13.07 performance improvement and economic efficiency. In addition, we analyze the distribution of detailed error types during the multi-round self-debugging process. As the debugging rounds advance, we observe a reduction in System Error and a slight rise in Result Error. This suggests that although syntax errors are fixed, the regenerated SQL statements may still have semantic errors. With each round of debugging, the generated statements tend to transition from System Error to Result Error before eventually converging towards correct SQL statements. Core Conclusion 4. Multi-round self-debugging aids in error correction for LLMs, but there exists a performance boundary, with 1-2 rounds of debugging being the optimal choice. Can an LLM debug the error incurred by other LLMs (general debugging)? As depicted in Table 5, we select two representative LLMs, SQLCoder and InternLM, to assess their general debugging capabilities. When debugging erroneous SQL statements generated by other LLMs, additional error information can potentially impair the performance. Conversely, the most naive approach of simply regenerating the SQL statements often yields the best results. This highlights the differences between different LLMs. The debugger LLMs may not encounter errors caused by other LLMs, and these error type informations might confuse them. Consequently, it is unlikely to achieve performance improvement through general debugging. However, the integration of results generated by different LLMs holds promise as a future research direction. Core Conclusion 5. The performance of cross-LLM SQL debugging is inferior to the direct regener- ation. A multi-agent approach that integrates outputs from different models shows great potential. 4.3 SQL Optimization Execution efficiency of SQL queries is a critical aspect, particularly in real-time systems that utilize large-scale databases. In this section, we further explore whether LLMs are able to enhance the execution efficiency of correct SQL queries. Formally, the SQL optimization process [31] involves transforming the initial SQL queries Y into an optimized form, denoted as Yo, with the goal of improving efficiency while maintaining identical results: Yo = fM(Po(Y, I)), (2) where Po and I are the corresponding prompt template and the additional information used for SQL optimization, fM(·) represents the mapping function of the LLM M. VES is commonly employed to evaluate the efficiency of SQL query execution. However, in practice, LLMs can sometimes rewrite a correct SQL query into an incorrect one, making it challenging to figure out if the main reason for the decline in VES is due to these incorrect rewrites or a decrease in the SQL execution efficiency. To this end, we suggest adopting a complementary metric C-VES (Correct-VES): C-VES = P n∈Nc R(Yo n, ˆYn) |Nc| , (3) where Nc .= {n|1 (Vn, ˆVn) = 1}. (4) 10"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,1,caption,Table 5: EX (%) results of general debug. Each column demonstrates the debugging results of
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,11,page,"Here, Nc represent the set of accurate SQLs (see Appendix C.1 for detailed notations). C-VES is designed exclusively to validate the capability of LLMs to generate more efficient SQL queries, regardless of the potential drawback of rewriting correct SQLs into erroneous ones. Do LLMs have the capability for SQL self-optimization? To the best of our knowledge, we are the first to consider utilizing LLMs for SQL optimization. Specifically, we devise an extensive suite of prompts Po curated to SQL optimization: • with Y: In this basic form, only original SQL statements are provided. • w/ Y + S + Q: Further incorporates the database schema S and the user question Q. • w/ demo: Introduce few-shot demonstrations without explanations. Demonstrations are intu- itively designed, incorporating common optimization rules, such as substituting“COUNT(*)” with “COUNT(<column_name>)”. • w/ demo + comments : Add an explanation for the few-shot demonstrations. See Ap- pendix A.3 for a detailed prompt template. • SimpleDDL-MD-Chat-Efficiency: To avoid the accumulation of errors caused by multiple generations, this prompt template require LLMs to directly generate the most efficient SQL query statement based on user query. Table 6: VES and C-VES results of different SQL optimization methods. Methods Prompt Template Metrics ChatGPT SQLCoder-34B Codellama-34B InternLM-70B InternLM2-20B Baseline SimpleDDL-MD-Chat VES 36.90 24.28 18.94 22.63 19.81C-VES102.50 101.17 99.68 110.39 94.33 Two-Stage Generation zero-shot withY VES 30.73 16.86 17.34 15.33 20.28C-VES102.43 102.19 102.08 102.17 101.42 w/Y+S+Q VES 32.21 18.90 19.28 18.44 20.77C-VES102.24 102.14 101.47 102.43 101.30 few-shotw/ demo VES 32.19 18.84 18.34 17.97 20.86C-VES102.18 101.84 101.88 102.68 101.75 w/ demo + commentsVES 32.65 18.28 18.52 17.45 20.92C-VES102.03 101.54 102.86 102.66 102.06 Direct Generation SimpleDDL-MD-Chat-EfficiencyVES 39.26 27.77 20.75 25.23 25.93C-VES 103.31 102.84 103.75 102.98 101.70 The effectiveness of these SQL optimization methods are demonstrated in Table 6. Almost all two- stage methods experience a significant decrease in VES. It can be attributed to the possibility of LLMs optimizing the correct SQL statements into incorrect ones, thereby resulting in a further decrease in accuracy. Even when considering only the correct results, the performance improvement in terms of execution efficiency brought by the optimized SQL statements is almost negligible. Furthermore, it is intriguing to note that directly instructing the LLM to generate efficient SQL statements appears to achieve improved accuracy. This suggests that placing higher demands on the LLM could yield surprisingly positive outcomes. Core Conclusion 6. In-context learning methods present challenges in achieving effective SQL optimization with LLMs. 4.4 SQL-to-Text The goal of SQL-to-Text is to transform the SQL query back into its original natural language question [51, 30, 42]. While it seems that SQL-to-Text cannot serve as a sub-task within the Text- to-SQL pipeline to improve the performance of End-to-End Text-to-SQL systems, employing this conversion as a supplementary step within the pipeline can indeed provide valuable insights. By converting the generated SQL statements back into text and juxtaposing these with the semantics of the original user questions, we can assess the accuracy of the SQL statements produced. In addition, it can assist researchers in evaluating the semantic comprehension capabilities of different LLMs, thus facilitating the development of more effective Text-to-SQL methodologies. To this end, we assess the performance of SQL-to-Text across different LLMs (See Appendix A.4 for prompt templates). The selected metrics for evaluation encompass the F1 values of Rouge-1/2/L and BertScore, along with the application of LLM to assess the semantic coherence between the two texts. The evaluation results are depicted in Table 7. ChatGPT and InternLM2 demonstrate the highest performance, followed by InternLM, while Codellama and SQLCoder exhibit comparatively lower 11"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,17,caption,Table 6: VES and C-VES results of different SQL optimization methods.
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,12,page,"Table 7: SQL-to-Text performance of different LLMs, including the F1 scores of Rouge and BertScore, as well as the accuracy rate assessed by LLM. ChatGPT SQLCoder-34B Codellama-34B InternLM-70B InternLM2-20B Llama2-Chat-70B Rouge-1 0.428 0.118 0.359 0.408 0.410 0.368Rouge-2 0.384 0.039 0.167 0.198 0.294 0.159Rouge-L 0.545 0.108 0.320 0.372 0.358 0.325BertScore 0.887 0.779 0.856 0.886 0.887 0.885ChatGPT Evaluator70.5% 8.5% 47.0% 63.5% 66.5% 50.0%InternLM2 Evaluator62% 22.5% 49.8% 65.8% 71.3% 60.0% performance. This highlights that even in regards to semantic description of code, general-purpose models exhibit significantly stronger descriptive capabilities compared to coding-specific models. Core Conclusion 7. Utilizing a general-purpose model for semantic description of SQL statements is a better choice. 4.5 Schema Linking Schema linking is recognized as a crucial prerequisite of generating correct SQL queries. It involves aligning entity references in the question with the corresponding schema tables or columns, requiring the model to understand both structure and value of the database, as well as the the semantics of user questions. In LLM-based Text-to-SQL, prior studies [ 9, 33] design prompt instructions with in-context learning examples to enable LLMs to retrieve linked tables and columns, which are then used for the downstream Text-to-SQL task. However, none of these methods individually evaluate the performance of schema linking, and explicit evaluation metrics have yet to be established. Moreover, despite considerable advancements in semantic comprehension and generalization brought by LLMs, the performance of schema linking is still far from promising. In this section, we aim to bridge these gaps by: (1) introducing a elaborately designed metrics to assess schema linking methods, (2) presenting a novel schema linking method “PreSQL”, which demonstrates superior performance, (3) conducting a comprehensive evaluation of a range of schema linking methods across various LLMs. 4.5.1 Evaluation Metric for Schema Linking: RES What schema linking method is considered good? A straightforward goal of schema linking is that the GT tables should be retrieved as much as possible. However, due to the ambiguity inherent in natural language and the potential semantic similarities between candidate tables, retrieving more GT tables typically comes at the cost of a higher redundancy, known as the precision-recall trade-off. As discussed in [33], excessive table retrieval can introduce redundant joins between tables, potentially impairing the EX of Text-to-SQL generation. Therefore, the objective of schema linking is to retrieve all GT tables while avoiding the retrieval of excessive tables (with minimal redundancy). To evaluate this, we design a comprehensive metric called Retrieval Efficiency Score (RES), defined as: RES = PN n=1 1 (Tn, ˆTn) · R(Tn, ˆTn) N , (5) where 1 (Tn, ˆTn) = ( 1, if Tn ⊆ ˆTn 0, if Tn ⊈ ˆTn , R (Tn, ˆTn) = s |Tn| | ˆTn| , (6) where Tn and ˆTn denote the set of GT tables and retrieved tables for the n-th instance, respectively, | · |refers to the scale of a set. We emphasize that the RES serves as a more appropriate metric for evaluating schema linking than the F1-score. This is because it aligns the principle that recalling all GT tables is more important than increasing the precision of retrieval, as the former constitutes a prerequisite for generating correct SQL queries. 4.5.2 Schema Linking Evaluation The methods we evaluated are as follows (see Appendix A.5 for the full prompt of these methods): 12"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,1,caption,"Table 7: SQL-to-Text performance of different LLMs, including the F1 scores of Rouge and BertScore,"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,13,page,"• Zero Shot: A schema linking prompt proposed in C3 [9], which instructs the LLM to rank all the tables from relevant to irrelevant in a zero-shot manner. • Few Shot: Instructing the LLM to retrieve only the most important tables with few-shot demonstration, presented in DIN-SQL [33]. • PreSQL: First, we employ the zero-shot Text-to-SQL described in Section 4.1 to generate a preliminary SQL query. From this preliminary SQL, table and column entities are parsed to serve as the retrieval results for schema linking. • Few Shot + PreSQL: This approach takes the union of the retrieval results from both the Few Shot and PreSQL methods, aiming to leverage the strengths of each. Note that we organize the information of the database schema in a way similar to ""SimpleDDL-"" in the prompt of all the methods mentioned above, which ignores the information about foreign keys. However, as argued in [46], foreign keys embody features of known schema relations and provide importance clues for understanding the database structure. To this end, we conduct experiments under both settings, w/ and w/o foreign keys, to investigate how incorporating foreign keys in the prompt influences the performance of schema linking. Results are demonstrated in Table 8 (refer to Appendix B for results on detailed metrics like Exact Match & Subset Match). Table 8: RES results of different Schema Linking methods. ChatGPT SQLCoder-34B InternLM-70B Codellama-34B InternLM2-20B Llama2-Chat-70B w/o fk w/ fk w/o fk w/ fk w/o fk w/ fk w/o fk w/ fk w/o fk w/ fk w/o fk w/ fk Zero Shot 0.6384 0.6399 0.2278 0.4686 0.5745 0.5652 0.3675 0.4835 0.5687 0.5811 0.4759 0.5566Few Shot 0.6222 0.6402 0.3657 0.3919 0.5302 0.4961 0.4401 0.4745 0.464 0.4829 0.4123 0.5375PreSQL 0.6888 0.66100.56610.64170.4632 0.4881 0.50850.597 0.5649 0.6267 0.4478 0.5273Few Shot + PreSQL0.73400.74690.5354 0.63360.59990.60540.55760.56460.65940.70160.6105 0.6418 Which method achieved the best performance in schema linking? It can be seen that code-specific models excel in performance when utilizing the PreSQL approach, whereas general-purpose models yield optimal results through the Few-Shot + PreSQL method. This aligns with our expectations, as these two types of models excel in coding tasks (see Section 4.1) and semantic understanding tasks (see Section 4.4), respectively. Can foreign key information facilitate schema linking? The introduction of foreign key information yield improved performance across all methods and all LLMs. This is evident since a valid JOIN operation in SQL queries is typically based on foreign keys. The foreign key information helps the model retrieve more ground truth tables by indicating all potential table pairs involved in a JOIN operation. Core Conclusion 8. Foreign key information is capable of advance the performance of schema linking. PreSQL yields the highest performance on coding-specific models, and integrating the results from Few Shot can further enhance performance on general-purpose models. 5 Conclusion In this study, we conduct a systematic benchmarking of the various sub-tasks within the Text-to- SQL pipeline, encompassing Text-to-SQL, SQL-Debugging, SQL-Optimization, SQL-to-Text, and Schema-Linking. Our comprehensive evaluation involves six distinct LLMs, spanning both general- purpose and coding-specific models. We focus on determining the optimal prompt templates for each task, assessing performance variations among different approaches, and identifying the distinct capabilities and limitations of each LLM. The results of the study demonstrate notable performance variations across the LLMs, underscoring the significance of careful model selection and prompt engineering in attaining optimal outcomes in text-to-SQL tasks. Our benchmarking provides a meticulous perspective on the pipeline, equipping the research community with strategies to improve the semantic understanding and computational performance of LLMs. This advancement contributes to the development of more reliable Text-to-SQL systems. References [1] Ruichu Cai, Jinjie Yuan, Boyan Xu, and Zhifeng Hao. Sadga: Structure-aware dual graph aggregation network for text-to-sql. Advances in Neural Information Processing Systems, 34:7664–7676, 2021. 13"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,17,caption,Table 8: RES results of different Schema Linking methods.
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,14,page,"[2] Ruisheng Cao, Lu Chen, Zhi Chen, Yanbin Zhao, Su Zhu, and Kai Yu. Lgesql: line graph enhanced text-to-sql model with mixed local and non-local relations. arXiv preprint arXiv:2106.01093, 2021. [3] Shuaichen Chang and Eric Fosler-Lussier. Selective demonstrations for cross-domain text-to-sql. arXiv preprint arXiv:2310.06302, 2023. [4] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023. [5] DongHyun Choi, Myeong Cheol Shin, EungGyun Kim, and Dong Ryeol Shin. Ryansql: Recursively applying sketch-based slot fillings for complex text-to-sql in cross-domain databases. Computational Linguistics, 47(2):309–332, 2021. [6] Xiang Deng, Ahmed Hassan Awadallah, Christopher Meek, Oleksandr Polozov, Huan Sun, and Matthew Richardson. Structure-grounded pretraining for text-to-sql. arXiv preprint arXiv:2010.12773, 2020. [7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec- tional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [8] Li Dong and Mirella Lapata. Coarse-to-fine decoding for neural semantic parsing. arXiv preprint arXiv:1805.04793, 2018. [9] Xuemei Dong, Chao Zhang, Yuhang Ge, Yuren Mao, Yunjun Gao, Jinshu Lin, Dongfang Lou, et al. C3: Zero-shot text-to-sql with chatgpt. arXiv preprint arXiv:2307.07306, 2023. [10] Yujian Gan, Xinyun Chen, Qiuping Huang, Matthew Purver, John R Woodward, Jinxia Xie, and Peng- sheng Huang. Towards robustness of text-to-sql models against synonym substitution. arXiv preprint arXiv:2106.01065, 2021. [11] Yujian Gan, Xinyun Chen, and Matthew Purver. Exploring underexplored limitations of cross-domain text-to-sql generalization. arXiv preprint arXiv:2109.05157, 2021. [12] Yujian Gan, Xinyun Chen, Qiuping Huang, and Matthew Purver. Measuring and improving compositional generalization in text-to-sql via component alignment. arXiv preprint arXiv:2205.02054, 2022. [13] Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou. Text-to-sql empowered by large language models: A benchmark evaluation. arXiv preprint arXiv:2308.15363, 2023. [14] Chunxi Guo, Zhiliang Tian, Jintao Tang, Pancheng Wang, Zhihua Wen, Kang Yang, and Ting Wang. Prompting gpt-3.5 for text-to-sql with de-semanticization and skeleton retrieval. InPacific Rim International Conference on Artificial Intelligence, pages 262–274. Springer, 2023. [15] Binyuan Hui, Ruiying Geng, Lihan Wang, Bowen Qin, Bowen Li, Jian Sun, and Yongbin Li. S 2sql: Injecting syntax to question-schema interaction graph encoder for text-to-sql parsers. arXiv preprint arXiv:2203.06958, 2022. [16] Wonseok Hwang, Jinyeong Yim, Seunghyun Park, and Minjoon Seo. A comprehensive exploration on wikisql with table-aware word contextualization. arXiv preprint arXiv:1902.01069, 2019. [17] George Katsogiannis-Meimarakis and Georgia Koutrika. A survey on deep learning approaches for text-to-sql. The VLDB Journal, pages 1–32, 2023. [18] Yilun Kong, Jingqing Ruan, Yihong Chen, Bin Zhang, Tianpeng Bao, Shiwei Shi, Guoqing Du, Xiaoru Hu, Hangyu Mao, Ziyue Li, et al. Tptu-v2: Boosting task planning and tool usage of large language model-based agents in real-world systems. arXiv preprint arXiv:2311.11315, 2023. [19] Ayush Kumar, Parth Nagarkar, Prabhav Nalhe, and Sanjeev Vijayakumar. Deep learning driven natural languages text to sql query conversion: A survey. arXiv preprint arXiv:2208.04415, 2022. [20] Chia-Hsuan Lee, Oleksandr Polozov, and Matthew Richardson. Kaggledbqa: Realistic evaluation of text-to-sql parsers. arXiv preprint arXiv:2106.11455, 2021. [21] Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 13067–13075, 2023. [22] Jieyu Li, Zhi Chen, Lu Chen, Zichen Zhu, Hanqi Li, Ruisheng Cao, and Kai Yu. Dir: A large-scale dialogue rewrite dataset for cross-domain conversational text-to-sql. Applied Sciences, 13(4):2262, 2023. 14"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,15,page,"[23] Jinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei Huang, Wenyu Du, Luo Si, and Yongbin Li. Graphix-t5: Mixing pre-trained transformers with graph-aware layers for text-to-sql parsing. arXiv preprint arXiv:2301.07507, 2023. [24] Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, et al. Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls. Advances in Neural Information Processing Systems, 36, 2024. [25] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74–81, 2004. [26] Xi Victoria Lin, Richard Socher, and Caiming Xiong. Bridging textual and tabular data for cross-domain text-to-sql semantic parsing. arXiv preprint arXiv:2012.12627, 2020. [27] Aiwei Liu, Xuming Hu, Lijie Wen, and Philip S Yu. A comprehensive evaluation of chatgpt’s zero-shot text-to-sql capability. arXiv preprint arXiv:2303.13547, 2023. [28] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. [29] Qin Lyu, Kaushik Chakrabarti, Shobhit Hathi, Souvik Kundu, Jianwen Zhang, and Zheng Chen. Hybrid ranking network for text-to-sql. arXiv preprint arXiv:2008.04759, 2020. [30] Da Ma, Xingyu Chen, Ruisheng Cao, Zhi Chen, Lu Chen, and Kai Yu. Relation-aware graph transformer for sql-to-text generation. Applied Sciences, 12(1):369, 2021. [31] Hamid Pirahesh, Joseph M Hellerstein, and Waqar Hasan. Extensible/rule based query rewrite optimization in starburst. ACM Sigmod Record, 21(2):39–48, 1992. [32] Mohammadreza Pourreza and Davood Rafiei. Evaluating cross-domain text-to-sql models and benchmarks. arXiv preprint arXiv:2310.18538, 2023. [33] Mohammadreza Pourreza and Davood Rafiei. Din-sql: Decomposed in-context learning of text-to-sql with self-correction. Advances in Neural Information Processing Systems, 36, 2024. [34] Jiexing Qi, Jingyao Tang, Ziwei He, Xiangpeng Wan, Yu Cheng, Chenghu Zhou, Xinbing Wang, Quanshi Zhang, and Zhouhan Lin. Rasat: Integrating relational structures into pretrained seq2seq model for text-to-sql. arXiv preprint arXiv:2205.06983, 2022. [35] Bowen Qin, Binyuan Hui, Lihan Wang, Min Yang, Jinyang Li, Binhua Li, Ruiying Geng, Rongyu Cao, Jian Sun, Luo Si, et al. A survey on text-to-sql parsing: Concepts, methods, and future directions. arXiv preprint arXiv:2208.13629, 2022. [36] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020. [37] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. [38] Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, and Rui Zhao. Tptu: Task planning and tool usage of large language model-based ai agents. arXiv preprint arXiv:2308.03427, 2023. [39] Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. Picard: Parsing incrementally for constrained auto-regressive decoding from language models. arXiv preprint arXiv:2109.05093, 2021. [40] Peter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova. Compositional generaliza- tion and natural language variation: Can a semantic parsing approach handle both? arXiv preprint arXiv:2010.12725, 2020. [41] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv e-prints, pages arXiv–2303, 2023. [42] Chang Shu, Yusen Zhang, Xiangyu Dong, Peng Shi, Tao Yu, and Rui Zhang. Logic-consistency text generation from semantic parses. arXiv preprint arXiv:2108.00577, 2021. 15"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,16,page,"[43] Guanghu Sui, Zhishuai Li, Ziyue Li, Sun Yang, Jingqing Ruan, Hangyu Mao, and Rui Zhao. Reboost large language model-based text-to-sql, text-to-python, and text-to-function–with real applications in traffic domain. arXiv preprint arXiv:2310.18752, 2023. [44] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [46] Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew Richardson. Rat-sql: Relation- aware schema encoding and linking for text-to-sql parsers. arXiv preprint arXiv:1911.04942, 2019. [47] Bing Wang, Changyu Ren, Jian Yang, Xinnian Liang, Jiaqi Bai, Qian-Wen Zhang, Zhao Yan, and Zhoujun Li. Mac-sql: Multi-agent collaboration for text-to-sql. arXiv preprint arXiv:2312.11242, 2023. [48] Tianshu Wang, Hongyu Lin, Xianpei Han, Le Sun, Xiaoyang Chen, Hao Wang, and Zhenyu Zeng. Dbcopilot: Scaling natural language querying to massive databases. arXiv preprint arXiv:2312.03463, 2023. [49] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022. [50] Kuan Xu, Yongbo Wang, Yongliang Wang, Zujie Wen, and Yang Dong. Sead: End-to-end text-to-sql generation with schema-aware denoising. arXiv preprint arXiv:2105.07911, 2021. [51] Kun Xu, Lingfei Wu, Zhiguo Wang, Yansong Feng, and Vadim Sheinin. Sql-to-text generation with graph-to-sequence model. arXiv preprint arXiv:1809.05255, 2018. [52] Xiaojun Xu, Chang Liu, and Dawn Song. Sqlnet: Generating structured queries from natural language without reinforcement learning. arXiv preprint arXiv:1711.04436, 2017. [53] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. [54] Pengcheng Yin and Graham Neubig. A syntactic neural model for general-purpose code generation. arXiv preprint arXiv:1704.01696, 2017. [55] Pengcheng Yin, Graham Neubig, Wen-tau Yih, and Sebastian Riedel. Tabert: Pretraining for joint understanding of textual and tabular data. arXiv preprint arXiv:2005.08314, 2020. [56] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. arXiv preprint arXiv:1809.08887, 2018. [57] Bin Zhang, Hangyu Mao, Jingqing Ruan, Ying Wen, Yang Li, Shao Zhang, Zhiwei Xu, Dapeng Li, Ziyue Li, Rui Zhao, et al. Controlling large language model-based agents for large-scale decision-making: An actor-critic approach. arXiv preprint arXiv:2311.13884, 2023. [58] Hanchong Zhang, Ruisheng Cao, Lu Chen, Hongshen Xu, and Kai Yu. Act-sql: In-context learning for text-to-sql with automatically-generated chain-of-thought. arXiv preprint arXiv:2310.17342, 2023. [59] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019. [60] Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. arXiv preprint arXiv:1709.00103, 2017. 16"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,17,page,"A Prompt Template A.1 Text-to-SQL Listing 5- 10 demonstrate eight prompt templates evaluated in the end-to-end Text-TO-SQL task, where “SimpleDDL-MD-Chat” obtains the best evaluation performance. 1 Figure out corresponding SQLite SQL Query of Question according to↰database . 2 <Database > 3 CREATE TABLE stadium ( stadium_id NUMBER PRIMARY KEY , ...) ; 4 CREATE TABLE singer ( singer_id NUMBER PRIMARY KEY , name TEXT , ...) ; 5 </ Database > 6 <Question > How many singers do we have ? </ Question > 7 (<SQL > SELECT ) Listing 5: Prompt template “DDL-HTML-Chat/Complete”. The content in parentheses is unique to“Complete”. 1 Figure out corresponding SQLite SQL Query of Question according to↰database . 2 <Database > 3 stadium ( Stadium_ID , Location ,Name , Capacity , Highest , Lowest , Average ); 4 singer ( Singer_ID ,Name , Country , Song_Name ,...) ; 5 </ Database > 6 <Question > How many singers do we have ? </ Question > 7 (<SQL > SELECT ) Listing 6: Prompt template “SimpleDDL-HTML-Chat/Complete”. The content in parentheses is unique to“Complete”. 1 ### Answer the question by sqlite SQL query only and with no↰explanation 2 ### Sqlite SQL tables , with their properties : 3 CREATE TABLE stadium ( stadium_id NUMBER PRIMARY KEY ,...) ; 4 CREATE TABLE singer ( singer_id NUMBER PRIMARY KEY , name TEXT , ...) ; 5 ### Question : How many singers do we have ? 6 ### SQL : ( SELECT ) Listing 7: Prompt template “DDL-MD-Chat/Complete”. The content in parentheses is unique to“Complete”. A.2 SQL Debugging Listing 11 is used for error classification. In the detailed categorization process of Result Error, Table Query Error, Column Selection Error, and Join Column Error can be determined by comparing SQL Query with the ground truth using predefined rules. For Data processing Error and Condition Filter Error, we employ InternLM2-20B for binary classification tasks. Listing 12 is the prompt template used for SQL debugging tasks. A.3 SQL Optimization Listing 13 is the prompt template “w/ demo + comments” used for SQL optimization tasks. Other variations of the template correspond to the removal of relevant content. 17"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,18,page,"1 ### Answer the question by sqlite SQL query only and with no↰explanation 2 ### Sqlite SQL tables , with their properties : 3 # 4 # stadium ( Stadium_ID , Location ,Name , Capacity , Highest , Lowest , Average ); 5 # singer ( Singer_ID ,Name , Country , Song_Name ,...) ; 6 # 7 ### How many singers do we have ? 8 ### SQL : ( SELECT ) Listing 8: Prompt template “SimpleDDL-MD-Chat/Complete”. The content in parentheses is unique to“Complete”. 1 /* Given the following database schema : */ 2 CREATE TABLE stadium ( stadium_id NUMBER PRIMARY KEY , ...) ; 3 4 CREATE TABLE singer ( singer_id NUMBER PRIMARY KEY , name TEXT , ...) ; 5 6 /* Answer the following by SQLite SQL Query according to database :↰Show name , country , age for all singers ordered by age from the↰oldest to the youngest . */ 7 /* SQL Query here */ 8 ( SELECT ) Listing 9: Prompt template “DDL-Coding-Chat/Complete”. The content in parentheses is unique to“Complete”. 1 /* Given the following database schema : */ 2 stadium ( Stadium_ID , Location ,Name , Capacity , Highest , Lowest , Average ); 3 singer ( Singer_ID ,Name , Country , Song_Name ,...) ; 4 5 /* Answer the following by SQLite SQL Query according to database :↰Show name , country , age for all singers ordered by age from the↰oldest to the youngest . */ 6 /* SQL Query here */ 7 ( SELECT ) Listing 10: Prompt template “SimpleDDL-Coding-Chat/Complete”. The content in parentheses is unique to“Complete”. 1 You are an expert in SQL queries . Please provide the error categories↰for incorrect SQL queries based on the Question and the correct SQL↰query . 2 Please think step by step and check for the following errors in order : 3 1. Condition Filter Error : Incorrect filtering of conditions . 4 2. Data Processing Error : The condition is filtered correctly , but the↰data processing is wrong . Note that the premise of this error is↰that the conditional filtering is correct . 5 6 Question : What is the name and capacity for the stadium with highest↰average attendance ? 7 Correct SQL Query : SELECT name , capacity FROM stadium ORDER BY↰average DESC LIMIT 1 8 Wrong SQL Query : SELECT Name , Capacity FROM stadium WHERE Average = (↰SELECT MAX ( Average ) FROM stadium ) ORDER BY Highest DESC 9 10 Give your Thought and Answer based on the information above . Listing 11: Prompt template for conducting the “Condition Filter Error” and “Data Processing Error” classification tasks. 18"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,19,page,"1 ### Write the correct SQLite SQL Query corresponding to the Question↰based on the database , the Wrong SQL Query and the cause of the↰error . 2 ### Sqlite SQL tables , with their properties : 3 # 4 #{ SimpleDDL } 5 # 6 ### Question : Under whose administration does the school with the↰highest number of test takers whose total SAT Scores are greater or↰equal to 1500 belong to? Indicate his or her full name . 7 ### Wrong SQL Query : 8 SELECT T1. AdmFName1 , T1. AdmLName1 FROM schools AS T1 JOIN satscores↰AS T2 ON T1. CDSCode = T2. cds WHERE T2. NumTstTakr = ( SELECT↰NumTstTakr FROM satscores GROUP BY cds HAVING NumGE1500 >= 1500↰ORDER BY NumTstTakr DESC LIMIT 1 ) 9 ### Error Information : 10 Executed correctly , but with the wrong result . 11 You have found the correct tables . But you select wrong columns , you↰need to select more Columns . 12 ### Correct SQL : Listing 12: Prompt template “w/ Wrong SQL + All_error_info + Comment” for SQL Debugging. 1 ### Rewrite and optimize the given SQL query to improve SQL query↰efficiency and minimize SQL execution time while ensuring↰correctness . Only output sql query , do not output any other content .↰Only output sql query , do not output any other content . 2 ### Here are some reference cases : 3 # 4 # Question : List out the age of users who located in Vienna , Austria↰obtained the badge ? 5 # SQL Query : SELECT Age FROM users WHERE Location = ’Vienna , Austria ’↰AND Id IN ( SELECT UserId FROM badges ) 6 # New SQL Query : SELECT u. Age FROM users AS u INNER JOIN badges AS b↰ON u.Id = b. UserId WHERE u. Location = ’Vienna , Austria ’ 7 # Explanation : By applying a JOIN operation instead of a subquery with↰ IN can improve efficiency , as the database may execute the JOIN and↰ filtering processes concurrently in just one operation without the↰need to store the intermediate results to filter primary query . 8 #... 9 # 10 ### Sqlite SQL tables , with their properties : 11 # 12 #{ SimpleDDLdd } 13 # 14 ### Question : What is the short name and fifa ID for Queens Park↰Rangers Football Team ?... 15 ### SQL Query : SELECT team_short_name , team_fifa_api_id FROM Team↰WHERE team_long_name = "" Queens Park Rangers "" 16 ### New SQL Query : Listing 13: Prompt template “w/ demo + comments” for SQL Optimization. 19"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,20,page,"A.4 SQL-to-Text Listing 14 is the prompt template used for SQL-to-Text task. Listing 15 is utilized to assess the consistency between the transformed text description and the original user query semantics. To mitigate the influence of the order of the two text inputs, we employ ChatGPT and InternLM2 to conduct two sequential tests and calculate the average of the results. 1 <Instruction > 2 You are an expert in database analysis and processing of SQL↰statements . 3 I will provide an SQL statement and relevant evidence . You need to↰help me analyze what problem this SQL statement is solving . 4 Here are some reference cases : 5 SQL : SELECT list_id FROM lists_users WHERE user_id = 85981819 ORDER BY↰ list_creation_date_utc ASC LIMIT 1 6 question : What is the list ID that was first created by user 85981819? 7 SQL : SELECT COUNT (T2. user_id ) FROM movies AS T1 INNER JOIN ratings AS↰T2 ON T1. movie_id = T2. movie_id WHERE T1. movie_title = ’Pavee↰Lackeen : The Traveller Girl ’ AND T2. rating_score = 4 8 question : How many users gave \"" Pavee Lackeen : The Traveller Girl \""↰movie a rating score of 4? 9 Please answer strictly in the following format and do not change the↰format arbitrarily : 10 question : This is a problem description . 11 </ Instruction > 12 <SQL > SELECT ... </SQL > Listing 14: Prompt template for SQL-to-Text. 1 <Instruction > Determine whether the following two sentences ask the↰same question and whether their corresponding answers are the same↰. </ Instruction > 2 <sentence1 > How many singers do we have ? </ sentence1 > 3 <sentence2 > How many singers are there in total ? </ sentence2 > 4 <Question > Just output True or False , do not output anything else </↰Question > Listing 15: Prompt template for semantic consistency checking. A.5 Schema Linking Listing 16 is a schema linking prompt template proposed in C3 [9]. Listing 17 is used for few-shot learning proposed in DIN-SQL [ 33]. In addition, we conduct tests on the PreSQL method. This method generates an SQL query statement based on the user query and database schema, and extracts the corresponding tables from that statement. 20"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,21,page,"1 Given the database schema and question , perform the following actions : 2 1 - Rank all the tables based on the possibility of being used in the↰SQL according to the question from the most relevant to the least↰relevant , Table or its column that matches more with the question↰words is highly relevant and must be placed ahead . 3 2 - Check whether you consider all the tables . 4 3 - Output a list object in the order of step 2, Your output should↰contain all the tables . The format should be like : 5 [ 6 "" table_1 "", "" table_2 "", ... 7 ] 8 9 Database schemas with their properties : 10 { SimpleDDL } 11 12 Question : What is the short name and fifa ID for Queens Park Rangers↰Football Team ?In the database , short name of the football team↰refers to team_short_name ; Queens Park Rangers refers to↰team_long_name = ’ Queens Park Rangers ’; fifa ID refers to↰team_fifa_api_id . 13 Answer ( Only output the list object containing all tables , do not↰output other content ): Listing 16: Prompt template for zero-shot schema linking. 21"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,22,page,"1 Given the database schema and question , perform the following actions : 2 1 - Evaluate the importance of each table ** in relation to the SQL↰query ** , prioritizing tables and columns that closely match the↰question words . Rank the tables from the most crucial to the least↰crucial . 3 2 - Focus on identifying and listing only the most important tables↰based on the evaluation in step 1. 4 3 - Output a list object representing the order determined in step 2.↰The output should include ** the most important tables ** and follow↰this format : 5 [ 6 "" most_important_table_1 "", "" most_important_table_2 "", ... 7 ] 8 9 Schema : 10 # department ( Department_ID ,Name , Creation , ...) 11 # head ( head_ID ,name , born_state , age ) 12 # management ( department_ID , head_ID , temporary_acting ) 13 Foreign key : 14 management ( department_ID ) REFERENCES department ( Department_ID )\↰nmanagement ( head_ID ) REFERENCES head ( head_ID ) 15 Question : what are the distinct creation years of the departments↰managed by a secretary born in state ’Alabama ’? 16 Answer : ["" department "" ,"" management "" ,"" head ""] 17 18 Schema : 19 # Country (id , name ) 20 # League (id , country_id , name ) 21 # Player (id , player_api_id , player_name , player_fifa_api_id , birthday ,↰height , weight ) 22 # Player_Attributes (id , player_fifa_api_id , ...) 23 # Team (id , team_api_id , team_fifa_api_id , team_long_name , team_short_name ) 24 # Team_Attributes (id , team_fifa_api_id , ...) 25 # sqlite_sequence (name , seq ) 26 Foreign key : 27 Player_Attributes ( player_api_id ) REFERENCES Player ( player_api_id )\↰nLeague ( country_id ) REFERENCES country (id)\ nTeam_Attributes (↰team_api_id ) REFERENCES Team ( team_api_id )\ nMatch ( away_player_11 )↰REFERENCES Player ( player_api_id ) 28 Question : List the names of all left - footed players who have overall↰rating between 85 and 90. 29 Answer : ["" Player "" ,"" Player_Attributes ""] 30 31 Database schemas with their properties : 32 { SimpleDDL } 33 34 Question : What is the short name and fifa ID for Queens Park Rangers↰Football Team ? In the database , short name of the football team↰refers to team_short_name ; Queens Park Rangers refers to↰team_long_name = ’ Queens Park Rangers ’; fifa ID refers to↰team_fifa_api_id . 35 Answer : Listing 17: Prompt template for few-shot schema linking. 22"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,23,page,"Figure 6: EX (%) improvement brought by self debug on BIRD dev set. B Supplemental Evaluation Results B.1 Additional Experimental Results on “BigTable-0.2k” For a more detailed analysis of schema linking, we introduce two supplemental metrics, Subset Match and Exact Match. These metrics measure the proportion of instances in which the GT tables are a subset of, or exactly match, retrieved tables, respectively. We observe from Table 9 that the Few Shot + PreSQL and PreSQL method achieves the highest Subset Match and the Exact Match, respectively, which explains the higher RES achieved by these methods. Table 9: The results of table linking include the metrics of exact match and subset match. SQLCoder-34B InternLM-70B Codellama-34B InternLM2-20B Llama2-Chat-70B w/o fk w/ fk w/o fk w/ fk w/o fk w/ fk w/o fk w/ fk w/o fk w/ fk Subset Match0.28 0.55 0.66 0.645 0.495 0.655 0.605 0.67 0.655 0.76Zero Shot Exact Match0.135 0.295 0.355 0.34 0.19 0.27 0.235 0.415 0.12 0.135 Subset Match0.5 0.475 0.595 0.555 0.645 0.7 0.53 0.545 0.57 0.72Few Shot Exact Match0.155 0.22 0.355 0.34 0.19 0.155 0.33 0.355 0.125 0.18 Subset Match0.6 0.675 0.475 0.505 0.525 0.62 0.6 0.67 0.51 0.65PreSQL Exact Match0.465 0.55 0.43 0.435 0.45 0.525 0.5 0.525 0.3 0.23 Subset Match0.715 0.77 0.67 0.675 0.755 0.785 0.74 0.78 0.815 0.86Few Shot + PreSQLExact Match0.195 0.345 0.395 0.405 0.2 0.185 0.42 0.46 0.19 0.165 B.2 Additional Experimental Results on BIRD To further justify the effectiveness and robustness of the core conclusions of this paper, we conduct identical experiments on the BIRD development dataset. The BIRD dev set contains 1,534 instances, which exhibit a larger scale compared to “BigTable-0.2k”. In the experiments, we only choose representative LLMs and methods that are influential in deriving core conclusions. Results for five sub-tasks are shown in Table 10, Figure 6, Tables 11–13, respectively. These experimental results are largely consistent with the core conclusions obtained on “BigTable-0.2k”, supporting the validity of the conclusions. Table 10: EX (%) of different LLMs on BIRD dev set. prompt template is “SimpleDDL-MD-Chat"". No. of GT Tables SQLCoder-34B InternLM-70B Codellama-34B InternLM2-20B 1 40.33 43.37 40.88 40.33 2 28.42 25.24 27.57 28.31 3 21.05 15.79 16.75 21.05 >3 30.00 5.00 15.00 30.00 Total 30.25 27.97 29.07 30.18 23"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,1,caption,Figure 6: EX (%) improvement brought by self debug on BIRD dev set.
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,9,caption,Table 9: The results of table linking include the metrics of exact match and subset match.
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,24,caption,"Table 10: EX (%) of different LLMs on BIRD dev set. prompt template is “SimpleDDL-MD-Chat""."
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,24,page,"Table 11: VES results of SQL Optimization on BIRD dev set. Prompt Template SQLCoder-34B InternLM-70B Codellama-34B with Y 99.68 99.99 99.04 w/ Y + S 102.65 100.10 101.48 w/ Y + S + Q 102.92 100.69 101.78 SimpleDDL-MD-Chat-Efficiency 101.26 100.51 100.34 Table 12: SQL-to-Text performance on BIRD dev set, including the F1 scores of Rouge and BertScore, as well as the accuracy rate assessed by LLM. Codellama-34B InternLM-70B Llama2-Chat-70B Rouge-1 0.423 0.495 0.454 Rouge-2 0.231 0.273 0.230 Rouge-L 0.423 0.449 0.408 BertScore 0.908 0.924 0.919 LLM Evaluator 64.0% 80.8% 75.3% Table 13: RES results of Schema Linking on BIRD dev set. SQLCoder 34B InternLM-70B Codellama-34B w/o fk w/ fk w/o fk w/ fk w/o fk w/ fk Zero Shot 0.7034 0.7138 0.7505 0.7739 0.7130 0.6564 Few Shot 0.4343 0.4078 0.7365 0.7574 0.5712 0.5822 PreSQL 0.7274 0.7715 0.5563 0.5958 0.6389 0.6615 Few Shot + PreSQL 0.6621 0.7296 0.7686 0.7936 0.6551 0.6417 24"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,1,caption,Table 11: VES results of SQL Optimization on BIRD dev set.
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,7,caption,"Table 12: SQL-to-Text performance on BIRD dev set, including the F1 scores of Rouge and BertScore,"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,15,caption,Table 13: RES results of Schema Linking on BIRD dev set.
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,25,page,"C Details for Sub-tasks C.1 Details for Evaluation Metrics In this section, we present the detailed definition of evaluation metrics in this paper. EX is defined as: EX = PN n=1 1 (Vn, ˆVn) N , (7) where 1 (Vn, ˆVn) = ( 1, if Vn = ˆVn 0, if Vn ̸= ˆVn , (8) where Vn and ˆVn denote execution results of the predicted SQL and the GT SQL for then-th instance in the evaluation set, respectively. The evaluation metric VES used for assessing the performance of SQL optimization is defined as: VES = PN n=1 1 (Vn, ˆVn) · R(Yo n, ˆYn) N , (9) where R(Yo n, ˆYn) = s E(Yon) E( ˆYn) , (10) where Yo n and ˆYn denote the optimized predicted SQL and GT SQL for then-th instance, respectively. E(·) serves as a function that quantifies the absolute execution efficiency (e.g., the reciprocal of execution time) of each SQL within a specified environment. C.2 Details for SQL Debugging Detailed comments for all Result Error are shown in Table 14. Table 14: Comments for different categories of Result Error used in SQL debugging. Error Type Subcategory comments prompt Table Query Error Excessive Tables The tables you inquired about is incorrect, you query too much tables. Missing Tables The tables you inquired about is incorrect, you need to query more tables. Incorrect Tables The tables you inquired about is incorrect. Column Selection Error Excessive Columns You have found the correct tables. But you select wrong columns,you select too much Columns. Missing Columns You have found the correct tables. But you select wrong columns,you need to select more Columns. Incorrect Columns You have found the correct tables.But you select wrong columns. Join Columns Error You have found the correct tables. You have selected the correct Columns. But you combine wrong rows when JOIN two tables. Condition Filter Error You have found the correct tables.You have selected the correct Columns. You have combined (JOIN) the correct tables. But an error occurred in the conditional filter. Data Processing Error You have found the correct tables. You have selected the correct Columns. You have combined (JOIN) the correct tables. You have used the correct conditional filtering. But there was an error in your processing of the data. C.3 Details for SQL Optimization We suspect the reasons LLMs are challenging to achieve effective SQL optimization are as follows: 25"
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,33,caption,Table 14: Comments for different categories of Result Error used in SQL debugging.
Zhang et al. - 2024 - Benchmarking the Text-to-SQL Capability of Large L.pdf,26,page,"• Lack of in-context learning guidance. For self-debug, models can benefit from the error messages given by DBMS (see the last two rows of Figure 4). However, SQL optimization tasks lack explicit feedback from the system and guidance through In-context learning, despite the comprehensively designed prompt templates. • Lack of pre-training data. During the pre-training phase, models can access massive correct SQL statements as training data, yet they may lack information on what kind of SQL statements are efficient. • System-level optimization is inherently challenging. The efficiency of a SQL query can only be assessed by executing it within a database system. This is a system-level optimization beyond the scope of syntactic-level and semantic-level language modeling areas in which LLMs excel. It is challenging for models to capture the mapping between SQLs and their execution efficiency. C.4 Details for SQL-to-Text Detailed description of evaluation metrics used for SQL-to-Text task are as follows: • Rouge [25] focuses on token-level syntactical similarity by breaking down the predicted question and the GT question into N-grams and calculating the recall rate. We use ROUGE-1, ROUGE-2, and ROUGE-L, respectively. • BertScore [59] focuses on semantic similarity between predicted and GT questions. Based on the BERT pre-trained model, BertScore uses contextual embeddings to describe sentences and calculates the cosine similarity between them. • Using LLMs as evaluator is an intuitive and recently prevalent evaluation approach. This involves presenting two questions to an LLM to determine whether they are semantically identical, with the accuracy rate being recorded. We use InternLM2 as evaluator, which achieves the best performance in the first two metrics. In practice, to mitigate the potential bias introduced by positional preference, we alter the order of predicted and GT questions in the input of InternLM2 and report the average result of two experiments. 26"
